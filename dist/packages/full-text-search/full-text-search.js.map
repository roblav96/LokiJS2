{"version":3,"sources":["webpack:///webpack/universalModuleDefinition","webpack:///webpack/bootstrap 98e76107b00e1234f39d","webpack:///./packages/full-text-search/src/tokenizer.js","webpack:///./packages/full-text-search/src/inverted_index.js","webpack:///./packages/full-text-search/src/queries.js","webpack:///./packages/full-text-search/src/scorer.js","webpack:///./packages/full-text-search/src/full_text_search.js","webpack:///./packages/full-text-search/src/language/de.js","webpack:///./packages/full-text-search/src/index.js","webpack:///./packages/full-text-search/src/index_searcher.js","webpack:///./packages/full-text-search/src/language/support.js","webpack:///./packages/full-text-search/src/utils.js"],"names":[],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD,O;ACVA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;;AAGA;AACA;;AAEA;AACA;;AAEA;AACA,mDAA2C,cAAc;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAK;AACL;AACA;;AAEA;AACA;AACA;AACA,mCAA2B,0BAA0B,EAAE;AACvD,yCAAiC,eAAe;AAChD;AACA;AACA;;AAEA;AACA,8DAAsD,+DAA+D;;AAErH;AACA;;AAEA;AACA;;;;;;;;;AChEA;;AAEA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,SAAS;AACtB;AACA;AACA;AACA;AACA,iBAAiB,mBAAmB;AACpC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,2CAA2C,4BAA4B;AACvE,yCAAyC,oBAAoB,GAAG,0BAA0B;AAC1F,MAAM,yBAAyB;AAC/B,+CAA+C,uBAAuB;AACtE,kCAAkC,sBAAsB;AACxD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,YAAY,OAAO;AACnB,YAAY,SAAS;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,yBAAyB;AACtC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,gBAAgB;AAC5B,cAAc,QAAQ;AACtB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,YAAY,gBAAgB;AAC5B,aAAa,yBAAyB;AACtC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,YAAY,OAAO;AACnB,YAAY,SAAS;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,YAAY,gBAAgB;AAC5B,YAAY,OAAO;AACnB,YAAY,SAAS;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,YAAY,gBAAgB;AAC5B,YAAY,OAAO;AACnB,YAAY,SAAS;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,gBAAgB;AAC5B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,aAAa,SAAS;AACtB;AACA;AACA;AACA;AACA,mBAAmB,wBAAwB;AAC3C;AACA,qBAAqB,mBAAmB;AACxC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe,yCAAyC;AACxD;AACA;AACA;AACA,sBAAsB;AACtB;AACA;AACA;AACA,mBAAmB,wBAAwB;AAC3C;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,wCAAwC;AACrD,YAAY,oCAAoC;AAChD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,qBAAqB,kCAAkC;AACvD;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA,qBAAqB,kCAAkC;AACvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,gBAAgB;AAC5B,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,qBAAqB,wBAAwB;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,YAAY,SAAS;AACrB,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;;;;;;;;AC3QkB;;AAElB;AACA;AACA;AACA,WAAW,QAAQ;AACnB;AACA;AACA;AACA,aAAa,QAAQ;AACrB,aAAa,QAAQ;AACrB,aAAa,UAAU;AACvB;AACA,eAAe,oHAAgE,KAAK;AACpF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,6BAA6B;AAC7B;AACA;AACA,mBAAmB;AACnB,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qBAAqB,wBAAwB;AAC7C;AACA;AACA;AACA;;AAEA;AACA;AACA,qBAAqB,iBAAiB;AACtC;AACA;AACA;AACA;AACA;AACA,mBAAmB;AACnB,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,qBAAqB,qBAAqB;AAC1C;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,2BAA2B,iBAAiB;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,uBAAuB,iBAAiB;AACxC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,cAAc,OAAO;AACrB;AACA;AACA;AACA;AACA;AACA,uBAAuB,iBAAiB;AACxC;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,cAAc,MAAM;AACpB;AACA;AACA;AACA;AACA,mBAAmB,iBAAiB;AACpC;AACA,0BAA0B,oCAAoC;AAC9D;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,0BAA0B,6BAA6B;AACvD;;AAEA;AACA,qBAAqB,iBAAiB;AACtC;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA,gBAAgB;AAChB;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,cAAc,mCAAmC;AACjD,aAAa,oCAAoC;AACjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,eAAe;AACf,SAAS;AACT;;AAEA;AACA;AACA,qBAAqB,iBAAiB;AACtC;AACA;AACA;AACA;AACA,yBAAyB,mBAAmB;AAC5C;AACA;AACA;AACA;AACA,2BAA2B;AAC3B,eAAe;AACf;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AAAA;AAAA;;;;;;;;ACzVA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA,6BAA6B;AAC7B;AACA;AACA;;AAEA;AACA;AACA;AACA,kCAAkC;AAClC,+BAA+B,qFAAqF;AACpH;AACA,aAAa,OAAO;AACpB,cAAc,UAAU;AACxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,cAAc,OAAO;AACrB;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA,+BAA+B;AAC/B,iCAAiC,gGAAgG;AACjI;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA,oCAAoC;AACpC;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA,oCAAoC;AACpC,kCAAkC,iGAAiG;AACnI;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB;AACA;AACA,qCAAqC;AACrC;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kDAAkD,kCAAkC;AACpF;AACA,mCAAmC;AACnC,qCAAqC,oGAAoG;AACzI;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA,wCAAwC;AACxC;AACA;AACA;AACA;;AAEA;AACA,iEAAiE,gBAAgB;AACjF,aAAa,QAAQ;AACrB,cAAc;AACd;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA,mCAAmC,yEAAyE;AAC5G;AACA;AACA;AACA;AACA,oDAAoD,2BAA2B;AAC/E,8EAA8E,8BAA8B;AAC5G;AACA,gCAAgC;AAChC,kCAAkC,iGAAiG;AACnI;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA,qCAAqC;AACrC;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA,cAAc,WAAW;AACzB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,cAAc,WAAW;AACzB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA,kDAAkD,kCAAkC;AACpF;AACA,iCAAiC;AACjC,kCAAkC;AAClC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA,sCAAsC;AACtC;AACA;AACA;AACA;;AAEA;AACA,iEAAiE,gBAAgB;AACjF,aAAa,QAAQ;AACrB,cAAc;AACd;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA,wCAAwC,kGAAkG;AAC1I;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA,8BAA8B;AAC9B;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,uIAAuI,oCAAoC;AAC3K;AACA;AACA,6BAA6B,iBAAiB,sBAAsB,2BAA2B,MAAM,8BAA8B;AACnI;AACA,uBAAuB;AACvB,kCAAkC,iGAAiG;AACnI;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA,qCAAqC;AACrC;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,cAAc,WAAW;AACzB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,cAAc,WAAW;AACzB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA,cAAc,WAAW;AACzB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,cAAc,WAAW;AACzB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA,6CAA6C,gBAAgB;AAC7D;AACA;AACA,uCAAuC;AACvC,qCAAqC,qGAAqG;AAC1I;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uBAAuB;AACvB;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA;AACA;AACA,+CAA+C;AAC/C;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,kCAAkC;AAClC,0CAA0C,0GAA0G;AACpJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uBAAuB;AACvB;AACA;;AAEA;AACA;AACA,cAAc,WAAW;AACzB;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uIAAuI,mCAAmC;AAC1K;AACA;AACA;AACA;AACA,kCAAkC;AAClC,iCAAiC,gGAAgG;AACjI;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uBAAuB;AACvB;AACA;;AAEA;AACA;AACA,cAAc,WAAW;AACzB;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,cAAc,WAAW;AACzB;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,cAAc,WAAW;AACzB;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,cAAc,WAAW;AACzB;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,cAAc,UAAU;AACxB;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI,gBAAgB,GAAG,iBAAiB,GAAG,iBAAiB,GAAG,oBAAoB;AACnF,IAAI,iBAAiB,GAAG,oBAAoB,GAAG,kBAAkB,IAAI,gBAAgB;AACrF,IAAI,yBAAyB,GAAG;AAChC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kBAAkB;AAClB;AACA;;AAEA;AACA;AACA,aAAa,QAAQ;AACrB,cAAc;AACd;AACA;AACA;AACA;AACA;;AAEA;AACA,sBAAsB,+CAA+C;AACrE;AACA,yCAAyC;AACzC,8BAA8B,kGAAkG;AAChI;AACA,aAAa,OAAO;AACpB;AACA,aAAa,OAAO;AACpB;AACA,cAAc;AACd;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;;;;;;;AC1uBA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,+DAA+D;AAC/D;AACA;AACA;;AAEA;AACA;AACA,mBAAmB,mBAAmB;AACtC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,OAAO;AACP;AACA;AACA,SAAS;AACT;AACA;;AAEA;AACA;;AAEA,6CAA6C;AAC7C;AACA;AACA;AACA,4BAA4B,kCAAkC;AAC9D;AACA;;AAEA,mCAAmC;AACnC;AACA;AACA;;AAEA;AACA,0BAA0B,kCAAkC;AAC5D;AACA,qBAAqB,8BAA8B;AACnD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yCAAyC;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,mBAAmB;AACtC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,gCAAgC,QAAQ;AACxC;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,YAAY,OAAO;AACnB,cAAc,OAAO;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAAA;AAAA;;;;;;;;;;;ACnJsB;AACA;AACJ;;AAElB;AACA;AACA;AACA,aAAa,SAAS;AACtB,aAAa,OAAO;AACpB,aAAa,aAAa;AAC1B;AACA,aAAa,aAAa;AAC1B;AACA,aAAa,oBAAoB;AACjC,aAAa,aAAa;AAC1B;AACA,uBAAuB,aAAa,KAAK;AACzC;AACA;AACA;;AAEA;AACA;AACA,mBAAmB,mBAAmB;AACtC;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,8BAA8B,kDAAkD;AAChF;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,mBAAmB,uBAAuB;AAC1C;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,8BAA8B,kDAAkD;AAChF;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,8BAA8B,kDAAkD;AAChF;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAAA;AAAA;;;;;;;;;;AC9FA;AAAA;AACA;AACA;AACA;AACwE;AACtD;;AAElB;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,iBAAiB,mBAAmB;AACpC;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;;AAEA;AACA;;AAEQ;;;;;;;;;;;;;;;;;;;;;AC9Ue;AACD;AACD;AACN;AACG;AACP;;AASX;;;;;;;;;;;ACde;AACO;AACD;;AAErB;AACA;AACA;AACA,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA,gCAAgC,kCAAkC;AAClE;AACA;AACA;AACA,eAAe;AACf;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gCAAgC,kCAAkC;AAClE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uBAAuB,wBAAwB;AAC/C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uBAAuB,cAAc;AACrC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uBAAuB,cAAc;AACrC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uBAAuB,iBAAiB;AACxC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yBAAyB,oBAAoB;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yBAAyB,iBAAiB;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,yBAAyB,kBAAkB;AAC3C;AACA;AACA,SAAS;AACT;AACA,yBAAyB,kBAAkB;AAC3C;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,mBAAmB,mBAAmB;AACtC;AACA;AACA;AACA;AACA;;AAEA;AACA,4BAA4B,kCAAkC;AAC9D;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,mBAAmB,mBAAmB;AACtC;AACA;AACA,4BAA4B,kCAAkC;AAC9D;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe,eAAe;AAC9B;AACA;;AAEA;AACA,eAAe,eAAe;AAC9B;AACA,iBAAiB,eAAe;AAChC,oCAAoC;AACpC;AACA,SAAS;AACT;AACA;AACA,oBAAoB;;AAEpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,YAAY,OAAO;AACnB,cAAc,MAAM;AACpB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe,8CAA8C;AAC7D;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,8BAA8B,yBAAyB;AACvD;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,uBAAuB,iBAAiB;AACxC;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,cAAc,MAAM;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,2BAA2B,kBAAkB;AAC7C;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA,qBAAqB,mBAAmB;AACxC;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,uBAAuB,gBAAgB;AACvC;AACA;AACA;AACA;AACA;AACA;AACA,oBAAoB,sBAAsB;AAC1C;AACA;AACA;AACA,uBAAuB,mBAAmB;AAC1C;AACA,sBAAsB,yDAAyD;AAC/E;AACA,OAAO;AACP,KAAK;AACL;AACA;AACA;AACA;;;;;;;;;ACjcA;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,qBAAqB,aAAa;AAClC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,mBAAmB,YAAY;AAC/B;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,mBAAmB,YAAY;AAC/B;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,2BAA2B,eAAe;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,0CAA0C,SAAS;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAAA;AAAA;;;;;;;;;;;;AC9SA;AAAA;AACA;AACA,WAAW,EAAE;AACb,YAAY,QAAQ;AACpB;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW,EAAE;AACb,YAAY,QAAQ;AACpB;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW,EAAE;AACb,YAAY,QAAQ;AACpB;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW,EAAE;AACb,YAAY,QAAQ;AACpB;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW,EAAE;AACb,YAAY,QAAQ;AACpB;AACA;AACA;AACA","file":"full-text-search.js","sourcesContent":["(function webpackUniversalModuleDefinition(root, factory) {\n\tif(typeof exports === 'object' && typeof module === 'object')\n\t\tmodule.exports = factory();\n\telse if(typeof define === 'function' && define.amd)\n\t\tdefine(\"full-text-search\", [], factory);\n\telse if(typeof exports === 'object')\n\t\texports[\"full-text-search\"] = factory();\n\telse\n\t\troot[\"full-text-search\"] = factory();\n})(this, function() {\nreturn \n\n\n// WEBPACK FOOTER //\n// webpack/universalModuleDefinition"," \t// The module cache\n \tvar installedModules = {};\n\n \t// The require function\n \tfunction __webpack_require__(moduleId) {\n\n \t\t// Check if module is in cache\n \t\tif(installedModules[moduleId]) {\n \t\t\treturn installedModules[moduleId].exports;\n \t\t}\n \t\t// Create a new module (and put it into the cache)\n \t\tvar module = installedModules[moduleId] = {\n \t\t\ti: moduleId,\n \t\t\tl: false,\n \t\t\texports: {}\n \t\t};\n\n \t\t// Execute the module function\n \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n \t\t// Flag the module as loaded\n \t\tmodule.l = true;\n\n \t\t// Return the exports of the module\n \t\treturn module.exports;\n \t}\n\n\n \t// expose the modules object (__webpack_modules__)\n \t__webpack_require__.m = modules;\n\n \t// expose the module cache\n \t__webpack_require__.c = installedModules;\n\n \t// identity function for calling harmony imports with the correct context\n \t__webpack_require__.i = function(value) { return value; };\n\n \t// define getter function for harmony exports\n \t__webpack_require__.d = function(exports, name, getter) {\n \t\tif(!__webpack_require__.o(exports, name)) {\n \t\t\tObject.defineProperty(exports, name, {\n \t\t\t\tconfigurable: false,\n \t\t\t\tenumerable: true,\n \t\t\t\tget: getter\n \t\t\t});\n \t\t}\n \t};\n\n \t// getDefaultExport function for compatibility with non-harmony modules\n \t__webpack_require__.n = function(module) {\n \t\tvar getter = module && module.__esModule ?\n \t\t\tfunction getDefault() { return module['default']; } :\n \t\t\tfunction getModuleExports() { return module; };\n \t\t__webpack_require__.d(getter, 'a', getter);\n \t\treturn getter;\n \t};\n\n \t// Object.prototype.hasOwnProperty.call\n \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n \t// __webpack_public_path__\n \t__webpack_require__.p = \"\";\n\n \t// Load entry module and return exports\n \treturn __webpack_require__(__webpack_require__.s = 6);\n\n\n\n// WEBPACK FOOTER //\n// webpack/bootstrap 98e76107b00e1234f39d","import * as Utils from './utils.js';\n\n/**\n * Splits a string at non-alphanumeric characters into lower case tokens.\n * @param {string} str - the string\n * @returns {string[]} - the tokens\n * @private\n */\nfunction defaultSplitter(str) {\n  let tokens = str.split(/[^\\w]+/);\n  for (let i = 0; i < tokens.length; i++) {\n    tokens[i] = tokens[i].toLowerCase();\n  }\n  return tokens;\n}\n\n/**\n * The tokenizer is used to prepare the string content of a document field for the inverted index.\n * Firstly the string gets split into tokens.\n * After that the tokens will be trimmed/stemmed with defined functions from the queue.\n *\n * * To change the splitter function, use {@link Tokenizer#setSplitter}.\n * * To add functions to the queue, use {@link Tokenizer#add}, {@link Tokenizer#addBefore} and\n *   {@link Tokenizer#addAfter}.\n * * To remove a function from the queue, use {@link Tokenizer#remove}.\n * * To reset the tokenizer, use {@link Tokenizer#reset}.\n */\nexport class Tokenizer {\n\t/**\n\t * Initializes the tokenizer with a splitter, which splits a string at non-alphanumeric characters.\n\t * The queue is empty.\n\t */\n  constructor() {\n    this._splitter = null;\n    this._queue = [];\n    this._symbol = Symbol('label');\n    this.reset();\n  }\n\n\t/**\n\t * Sets a function with defined label as the splitter function.\n\t * The function must take a string as argument and return an array of tokens.\n\t *\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t */\n  setSplitter(label, func) {\n    if (label === \"\") {\n      throw Error(\"Label cannot be empty.\");\n    }\n    func[this._symbol] = label;\n    this._splitter = func;\n  }\n\n\t/**\n\t * Gets the splitter.\n\t * @return {Array.<string, function>} - tuple with label and function\n\t */\n  getSplitter() {\n    return [this._splitter[this._symbol], this._splitter];\n  }\n\n\t/**\n\t * Resets the splitter to default.\n\t */\n  resetSplitter() {\n    this._splitter = defaultSplitter;\n  }\n\n\t/**\n\t * Checks if a function is inside the queue.\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @returns {boolean} true if exists, otherwise false\n\t */\n  has(labelFunc) {\n    return this._getPosition(labelFunc) !== -1;\n  }\n\n\t/**\n\t * Gets a function from the queue.\n\t * Only the first found function gets returned if a label or a function is multiple used.\n\t *\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @return {Array.<string, function>} - tuple with label and function\n\t */\n  get(labelFunc) {\n    let pos = this._getPosition(labelFunc);\n    if (pos === -1) {\n      throw Error('Cannot find existing function.');\n    }\n    return [this._queue[pos][this._symbol], this._queue[pos]];\n  }\n\n\t/**\n\t * Adds a function with defined label to the end of the queue.\n\t * The function must take a token string as argument and return a token.\n\t *\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t */\n  add(label, func) {\n    this._addFunction(label, func, this._queue.length);\n  }\n\n\t/**\n\t * Adds a function with defined label before an existing function to the queue.\n\t * The function must take a token string as argument and return a token.\n\t *\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t */\n  addBefore(labelFunc, label, func) {\n    let pos = this._getPosition(labelFunc);\n    if (pos === -1) {\n      throw Error('Cannot find existing function.');\n    }\n    this._addFunction(label, func, pos);\n  }\n\n\t/**\n\t * Adds a function with defined label after an existing function to the queue.\n\t * The function must take a token string as argument and return a token.\n\t *\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t */\n  addAfter(labelFunc, label, func) {\n    let pos = this._getPosition(labelFunc);\n    if (pos === -1) {\n      throw Error('Cannot find existing function.');\n    }\n    this._addFunction(label, func, pos + 1);\n  }\n\n\t/**\n\t * Removes a function from the queue.\n\t * @param {string|function} labelFunc - an existing label or function\n\t */\n  remove(labelFunc) {\n    let pos = this._getPosition(labelFunc);\n    if (pos === -1) {\n      throw Error('Cannot find existing function.');\n    }\n    this._queue.splice(pos, 1);\n  }\n\n\t/**\n\t * Resets the splitter and tokenize queue to default.\n\t */\n  reset() {\n    this._splitter = defaultSplitter;\n    this._queue = [];\n  }\n\n\t/**\n\t * Tokenizes a string into tokens.\n\t * @param {string} str - the string\n\t * @return {string[]} the tokens\n\t */\n  tokenize(str) {\n    let tokens = this._splitter(str);\n\t\t// Apply each token over the queue functions.\n    for (let i = 0; i < this._queue.length; i++) {\n      let newTokens = [];\n      for (let j = 0; j < tokens.length; j++) {\n        let token = this._queue[i](tokens[j]);\n        if (token) {\n          newTokens.push(token);\n        }\n      }\n      tokens = newTokens;\n    }\n    return tokens;\n  }\n\n\t/**\n\t * Serializes the tokenizer by returning the labels of the used functions.\n\t * @returns {{splitter: string?, tokenizers: string[]}} - the serialization\n\t * @private\n\t */\n  toJSON() {\n    let serialized = {tokenizers: []};\n    if (this._splitter !== defaultSplitter) {\n      serialized.splitter = this._splitter[this._symbol];\n    }\n    for (let i = 0; i < this._queue.length; i++) {\n      serialized.tokenizers.push(this._queue[i][this._symbol]);\n    }\n    return serialized;\n  }\n\n\t/**\n\t * Deserializes the tokenizer by reassign the correct function to each label.\n\t * @param {{splitter: string, tokenizers: string[]}} serialized - the serialized labels\n\t * @param {Object.<string, function>|Tokenizer} funcTok - the depending functions with labels\n\t * \tor an equivalent tokenizer\n\t */\n  static fromJSONObject(serialized, funcTok) {\n    let tkz = new Tokenizer();\n    if (funcTok !== undefined && funcTok instanceof Tokenizer) {\n      if (serialized.splitter !== undefined) {\n        let splitter = funcTok.getSplitter();\n        if (serialized.splitter !== splitter[0]) {\n          throw Error(\"Splitter function not found.\");\n        }\n        tkz.setSplitter(splitter[0], splitter[1]);\n      }\n\n      for (let i = 0; i < serialized.tokenizers.length; i++) {\n        if (!funcTok.has(serialized.tokenizers[i])) {\n          throw Error(\"Tokenizer function not found.\");\n        }\n        let labelFunc = funcTok.get(serialized.tokenizers[i]);\n        tkz.add(labelFunc[0], labelFunc[1]);\n      }\n    } else {\n      if (serialized.splitter !== undefined) {\n        if (funcTok.splitters[serialized.splitter] === undefined) {\n          throw Error(\"Splitter function not found.\");\n        }\n        tkz.setSplitter(serialized.splitter, funcTok.splitters[serialized.splitter]);\n      }\n      for (let i = 0; i < serialized.tokenizers.length; i++) {\n        if (funcTok.tokenizers[serialized.tokenizers[i]] === undefined) {\n          throw Error(\"Tokenizer function not found.\");\n        }\n        tkz.add(serialized.tokenizers[i], funcTok.tokenizers[serialized.tokenizers[i]]);\n      }\n    }\n    return tkz;\n  }\n\n\t/**\n\t * Returns the position of a function inside the queue.\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @return {number} the position\n\t * @private\n\t */\n  _getPosition(labelFunc) {\n    if (Utils.isFunction(labelFunc)) {\n      return this._queue.indexOf(labelFunc);\n    } else {\n      for (let i = 0; i < this._queue.length; i++) {\n        if (this._queue[i][this._symbol] === labelFunc) {\n          return i;\n        }\n      }\n    }\n    return -1;\n  }\n\n\t/**\n\t * Adds a function with defined label at a specific position to the queue.\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t * @param {number} pos - the position\n\t * @private\n\t */\n  _addFunction(label, func, pos) {\n    if (label === \"\") {\n      throw Error(\"Label cannot be empty.\");\n    }\n    func[this._symbol] = label;\n    this._queue.splice(pos, 0, func);\n  }\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./packages/full-text-search/src/tokenizer.js\n// module id = 0\n// module chunks = 0","import {Tokenizer} from './tokenizer';\n\n/**\n * Inverted index class handles featured text search for specific document fields.\n * @constructor InvertedIndex\n * @param {boolean} [options.store=true] - inverted index will be stored at serialization rather than rebuilt on load.\n */\nexport class InvertedIndex {\n  /**\n   * @param {boolean} store\n   * @param {boolean} optimizeChanges\n   * @param {Tokenizer} tokenizer\n   */\n  constructor({store = true, optimizeChanges = true, tokenizer = new Tokenizer} = {}) {\n    this._store = store;\n    this._optimizeChanges = optimizeChanges;\n    this._tokenizer = tokenizer;\n    this._docCount = 0;\n    this._docStore = {};\n    this._totalFieldLength = 0;\n    this._root = {};\n  }\n\n  get store() {\n    return this._store;\n  }\n\n  get tokenizer() {\n    return this._tokenizer;\n  }\n\n  get documentCount() {\n    return this._docCount;\n  }\n\n  get documentStore() {\n    return this._docStore;\n  }\n\n  get totalFieldLength() {\n    return this._totalFieldLength;\n  }\n\n  get root() {\n    return this._root;\n  }\n\n  /**\n   * Adds defined fields of a document to the inverted index.\n   * @param {object} field - the field to add\n   * @param {number} docId - the doc id of the field\n   */\n  insert(field, docId) {\n    if (this._docStore[docId] !== undefined) {\n      throw Error('Field already added.');\n    }\n\n    this._docCount += 1;\n    this._docStore[docId] = {};\n\n    // Tokenize document field.\n    let fieldTokens = this._tokenizer.tokenize(field);\n    this._totalFieldLength += fieldTokens.length;\n\n    let termRefs = [];\n    this._docStore[docId] = {fieldLength: fieldTokens.length};\n    if (this._optimizeChanges) {\n      Object.defineProperties(this._docStore[docId], {\n        termRefs: {enumerable: false, configurable: true, writable: true, value: termRefs}\n      });\n    }\n\n    // Iterate over all unique field terms.\n    for (let term of new Set(fieldTokens)) {\n      if (term === '') {\n        continue;\n      }\n      // Calculate term frequency.\n      let tf = 0;\n      for (let j = 0; j < fieldTokens.length; j++) {\n        if (fieldTokens[j] === term) {\n          tf++;\n        }\n      }\n\n      // Add term to index tree.\n      let branch = this._root;\n      for (let i = 0; i < term.length; i++) {\n        let c = term[i];\n        if (branch[c] === undefined) {\n          let child = {};\n          if (this._optimizeChanges) {\n            Object.defineProperties(child, {\n              pa: {enumerable: false, configurable: true, writable: true, value: branch}\n            });\n          }\n          branch[c] = child;\n        }\n        branch = branch[c];\n      }\n      // Add term info to index leaf.\n      if (branch.dc === undefined) {\n        branch.dc = {};\n        branch.df = 0;\n      }\n      branch.dc[docId] = tf;\n      branch.df += 1;\n\n      // Store index leaf for deletion.\n      termRefs.push(branch);\n    }\n  }\n\n  /**\n   * Removes all relevant terms of a document from the inverted index.\n   * @param {number} docId - the document.\n   */\n  remove(docId) {\n    if (this._docStore[docId] === undefined) {\n      return;\n    }\n    let docStore = this._docStore[docId];\n    // Remove document.\n    delete this._docStore[docId];\n    this._docCount -= 1;\n\n    // Reduce total field length.\n    this._totalFieldLength -= docStore.fieldLength;\n\n    if (this._optimizeChanges) {\n      // Iterate over all term references.\n      // Remove docId from docs and decrement document frequency.\n      let termRefs = docStore.termRefs;\n      for (let j = 0; j < termRefs.length; j++) {\n        let index = termRefs[j];\n        index.df -= 1;\n        delete index.dc[docId];\n\n        // Check if no document is left for current tree.\n        if (index.df === 0) {\n          // Delete unused meta data of branch.\n          delete index.df;\n          delete index.dc;\n\n          // Check for sub branches.\n          if (Object.keys(index).length !== 0) {\n            continue;\n          }\n\n          // Delete term branch if not used anymore.\n          let keys = [];\n          do {\n            // Go tree upwards.\n            let parent = index.pa;\n            // Delete parent reference for preventing memory leak (cycle reference).\n            delete index.pa;\n\n            // Iterate over all children.\n            keys = Object.keys(parent);\n            for (let k = 0; k < keys.length; k++) {\n              let key = keys[k];\n              if (key.length !== 1) {\n                continue;\n              }\n              // Remove previous child form parent.\n              if (parent[key] === index) {\n                delete parent[key];\n                break;\n              }\n            }\n            index = parent;\n          } while (index.pa !== undefined && keys.length === 1);\n        }\n      }\n    } else {\n      // Iterate over the whole inverted index and remove the document.\n      // Delete branch if not needed anymore.\n      let recursive = (root) => {\n        let keys = Object.keys(root);\n        for (let i = 0; i < keys.length; i++) {\n          let key = keys[i];\n          if (key.length === 1) {\n            // Checkout branch.\n            if (recursive(root[key])) {\n              delete root[key];\n            }\n          }\n        }\n        // Remove docId from docs and decrement document frequency.\n        if (root.df !== undefined) {\n          if (root.dc[docId] !== undefined) {\n            root.df -= 1;\n            delete root.dc[docId];\n\n            // Delete unused meta data of branch.\n            if (root.df === 0) {\n              delete root.df;\n              delete root.dc;\n            }\n          }\n        }\n        return Object.keys(root).length === 0;\n      };\n      recursive(this._root);\n    }\n  }\n\n  /**\n   * Gets the term index of a term.\n   * @param {string} term - the term.\n   * @param {object} root - the term index to start from\n   * @param {number} start - the position of the term string to start from\n   * @return {object} - The term index or null if the term is not in the term tree.\n   */\n  static getTermIndex(term, root, start = 0) {\n    if (start >= term.length) {\n      return null;\n    }\n    for (let i = start; i < term.length; i++) {\n      if (root[term[i]] === undefined) {\n        return null;\n      }\n      root = root[term[i]];\n    }\n    return root;\n  }\n\n  /**\n   * Extends a term index for the one branch.\n   * @param {object} root - the term index to start from\n   * @return {Array} - array with term indices and extension\n   */\n  static getNextTermIndex(root) {\n    let termIndices = [];\n    let keys = Object.keys(root);\n    for (let i = 0; i < keys.length; i++) {\n      if (keys[i].length === 1) {\n        termIndices.push({index: root[keys[i]], term: keys[i]});\n      }\n    }\n    return termIndices;\n  }\n\n  /**\n   * Extends a term index to all available term leafs.\n   * @param {object} root - the term index to start from\n   * @returns {Array} - Array with term indices and extension\n   */\n  static extendTermIndex(root) {\n    let termIndices = [];\n    let stack = [root];\n    let treeStack = [''];\n    do {\n      let root = stack.pop();\n      let treeTermn = treeStack.pop();\n\n      if (root.df !== undefined) {\n        termIndices.push({index: root, term: treeTermn});\n      }\n\n      let keys = Object.keys(root);\n      for (let i = 0; i < keys.length; i++) {\n        if (keys[i].length === 1) {\n          stack.push(root[keys[i]]);\n          treeStack.push(treeTermn + keys[i]);\n        }\n      }\n    } while (stack.length !== 0);\n\n    return termIndices;\n  }\n\n  /**\n   * Serialize the inverted index.\n   * @returns {{docStore: *, _fields: *, index: *}}\n   */\n  toJSON() {\n    if (this._store) {\n      return this;\n    } else {\n      return {\n        _store: false,\n        _optimizeChanges: this._optimizeChanges,\n        _tokenizer: this._tokenizer\n      };\n    }\n  }\n\n  /**\n   * Deserialize the inverted index.\n   * @param {{docStore: *, _fields: *, index: *}} serialized - The serialized inverted index.\n   * @param {Object.<string, function>|Tokenizer} funcTok[undefined] - the depending functions with labels\n   *  or an equivalent tokenizer\n   */\n  static fromJSONObject(serialized, funcTok = undefined) {\n    let dbObject = serialized;\n    let invIdx = new InvertedIndex({\n      store: dbObject._store,\n      optimizeChanges: dbObject._optimizeChanges,\n      tokenizer: Tokenizer.fromJSONObject(dbObject._tokenizer, funcTok)\n    });\n    invIdx._docCount = dbObject._docCount;\n    invIdx._docStore = dbObject._docStore;\n    invIdx._totalFieldLength = dbObject._totalFieldLength;\n    invIdx._root = dbObject._root;\n\n    let regenerate = (index, parent) => {\n      // Set parent.\n      if (parent !== null) {\n        Object.defineProperties(index, {\n          pa: {enumerable: false, configurable: true, writable: false, value: parent}\n        });\n      }\n\n      // Iterate over all keys.\n      let keys = Object.keys(index);\n      for (let i = 0; i < keys.length; i++) {\n        // Found term, save in document store.\n        if (keys[i] === 'dc') {\n          // Get documents of term.\n          let docIds = Object.keys(index.dc);\n          for (let j = 0; j < docIds.length; j++) {\n            // Get document store at specific document/field.\n            let ref = invIdx._docStore[docIds[j]];\n            if (ref.termRefs === undefined) {\n              Object.defineProperties(ref, {\n                termRefs: {enumerable: false, configurable: true, writable: true, value: []}\n              });\n            }\n            // Set reference to term index.\n            ref.termRefs.push(index);\n          }\n        } else if (keys[i].length === 1) {\n          // Iterate over subtree.\n          regenerate(index[keys[i]], index);\n        }\n      }\n    };\n\n    if (invIdx._optimizeChanges) {\n      regenerate(invIdx._root, null);\n    }\n\n    return invIdx;\n  }\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./packages/full-text-search/src/inverted_index.js\n// module id = 1\n// module chunks = 0","/**\n * Query builder\n */\n//import * as Utils from './utils.js';\n\n/**\n * The base query class to enable boost to a query type.\n */\nexport class BaseQuery {\n  /**\n   * @param {string} type - the type name of the query\n   * @param data\n   */\n  constructor(type, data = {}) {\n    this._data = data;\n    this._data.type = type;\n  }\n\n  /**\n   * Boosts the query result.\n   *\n   * See also [Lucene#BoostQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/BoostQuery.html}\n   * and [Elasticsearch#boost]{@link https://www.elastic.co/guide/en/elasticsearch/reference/5.2/mapping-boost.html}.\n   *\n   * @param {number} value - the positive boost\n   * @return {BaseQuery} object itself for cascading\n   */\n  boost(value) {\n    if (value < 0) {\n      throw TypeError(\"Boost must be a positive number.\");\n    }\n    this._data.boost = value;\n    return this;\n  }\n\n  /**\n   * Build the final query.\n   * @return {Object} - the final query\n   */\n  build() {\n    return this._data;\n  }\n}\n\n/**\n * A query which finds documents where a document field contains a term.\n *\n * See also [Lucene#TermQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/TermQuery.html}\n * and [Elasticsearch#TermQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-term-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .term(\"name\", \"infinity\"])\n * .build();\n * // The resulting documents:\n * // contains the term infinity\n *\n * @extends BaseQuery\n */\nexport class TermQuery extends BaseQuery {\n  /**\n   * @param {string} field - the field name of the document\n   * @param {string} term - the term\n   * @param data\n   */\n  constructor(field, term, data = {}) {\n    super(\"term\", data);\n    this._data.field = field;\n    this._data.value = term;\n  }\n}\n\n/**\n * A query which finds documents where a document field contains any of the terms.\n *\n * See also [Lucene#TermRangeQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/TermRangeQuery.html}\n * and [Elasticsearch#TermsQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-terms-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .terms(\"quotes\", [\"infinity\", \"atom\", \"energy\"])\n * .build();\n * // The resulting documents:\n * // contains the terms infinity, atom or energy\n *\n * @extends BaseQuery\n */\nexport class TermsQuery extends BaseQuery {\n  /**\n   * @param {string} field - the field name of the document\n   * @param {string[]} terms - the terms\n   * @param data\n   */\n  constructor(field, terms, data = {}) {\n    super(\"terms\", data);\n    this._data.field = field;\n    this._data.value = terms;\n  }\n}\n\n/**\n * A query which finds documents where the wildcard term can be applied at an existing document field term.\n *\n * Wildcard | Description\n * -------- | ------------\n * ? (question mark) | Skips a single character.\n *\n * To escape a wildcard character, use _\\_ (backslash), e.g. \\?.\n *\n * * To enable scoring for wildcard queries, use {@link WildcardQuery#enableScoring}.\n *\n * See also [Lucene#WildcardQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/WildcardQuery.html}\n * and [Elasticsearch#WildcardQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-wildcard-query.html}.\n *\n * _TODO: Implement wildcard * (asterisk) to skip zero or more characters._\n * @todo Implement wildcard * (asterisk) to skip zero or more characters.\n *\n * @example\n * new QueryBuilder()\n *   .wildcard(\"question\", \"e?nste?n\\?\")\n * .build();\n * // The resulting documents:\n * // contains the wildcard surname e?nste?n\\? (like Einstein? or Eynsteyn? but not Einsteine or Ensten?)\n *\n * @extends BaseQuery\n */\nexport class WildcardQuery extends BaseQuery {\n  /**\n   * @param {string} field - the field name of the document\n   * @param {string} wildcard - the wildcard term\n   * @param data\n   */\n  constructor(field, wildcard, data = {}) {\n    super(\"wildcard\", data);\n    this._data.field = field;\n    this._data.value = wildcard;\n  }\n\n  /**\n   * This flag enables scoring for wildcard results, similar to {@link TermQuery}.\n   * @param {boolean} enable - flag to enable or disable scoring\n   * @return {WildcardQuery}\n   */\n  enableScoring(enable) {\n    this._data.enable_scoring = enable;\n    return this;\n  }\n}\n\n/**\n * A query which finds documents where the fuzzy term can be transformed into an existing document field term within a\n * given edit distance\n * ([Damerauâ€“Levenshtein distance]{@link https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance}).\n *\n * The edit distance is the minimum number of an insertion, deletion or substitution of a single character\n * or a transposition of two adjacent characters.\n *\n * * To set the maximal allowed edit distance, use {@link FuzzyQuery#fuzziness} (default is AUTO).\n * * To set the initial word length, which should ignored for fuzziness, use {@link FuzzyQuery#prefixLength}.\n *\n * See also [Lucene#FuzzyQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/FuzzyQuery.html}\n * and [Elasticsearch#FuzzyQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-fuzzy-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .fuzzy(\"surname\", \"einsten\")\n *     .fuzziness(3)\n *     .prefixLength(3)\n * .build();\n * // The resulting documents:\n * // contains the fuzzy surname einstn (like Einstein or Einst but not Eisstein or Insten)\n *\n * @extends BaseQuery\n */\nexport class FuzzyQuery extends BaseQuery {\n  /**\n   * @param {string} field - the field name of the document\n   * @param {string} fuzzy - the fuzzy term\n   * @param data\n   */\n  constructor(field, fuzzy, data = {}) {\n    super(\"fuzzy\", data);\n    this._data.field = field;\n    this._data.value = fuzzy;\n  }\n\n  /**\n   * Sets the maximal allowed fuzziness.\n   * @param {number|string} fuzziness - the edit distance as number or AUTO\n   *\n   * AUTO generates an edit distance based on the length of the term:\n   * * 0..2 -> must match exactly\n   * * 3..5 -> one edit allowed\n   * * >5 two edits allowed\n   *\n   * @return {FuzzyQuery} - object itself for cascading\n   */\n  fuzziness(fuzziness) {\n    if (fuzziness !== \"AUTO\" && fuzziness < 0) {\n      throw TypeError(\"Fuzziness must be a positive number or AUTO.\");\n    }\n    this._data.fuzziness = fuzziness;\n    return this;\n  }\n\n  /**\n   * Sets the initial word length.\n   * @param {number} prefixLength - the positive prefix length\n   * @return {FuzzyQuery}  object itself for cascading\n   */\n  prefixLength(prefixLength) {\n    if (prefixLength < 0) {\n      throw TypeError(\"Prefix length must be a positive number.\");\n    }\n    this._data.prefix_length = prefixLength;\n    return this;\n  }\n}\n\n/**\n * A query which matches documents containing the prefix of a term inside a field.\n *\n * * To enable scoring for wildcard queries, use {@link WildcardQuery#enableScoring}.\n *\n * See also [Lucene#PrefixQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/PrefixQuery.html}\n * and [Elasticsearch#MatchQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-prefix-query.html}\n *\n * @example\n * new QueryBuilder()\n *   .prefix(\"surname\", \"alb\")\n * .build()\n * // The resulting documents:\n * // contains the term prefix alb as surname\n *\n * @extends BaseQuery\n */\nexport class PrefixQuery extends BaseQuery {\n  /**\n   * @param {string} field - the field name of the document\n   * @param {string} prefix - the prefix of a term\n   * @param data\n   */\n  constructor(field, prefix, data = {}) {\n    super(\"prefix\", data);\n    this._data.field = field;\n    this._data.value = prefix;\n  }\n\n  /**\n   * This flag enables scoring for wildcard results, similar to {@link TermQuery}.\n   * @param {boolean} enable - flag to enable or disable scoring\n   * @return {PrefixQuery}\n   */\n  enableScoring(enable) {\n    this._data.enable_scoring = enable;\n    return this;\n  }\n}\n\n/**\n * A query which matches all documents with a given field.\n *\n * See also [Elasticsearch#ExistsQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-exists-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .exists(\"name\")\n * .build()\n * // The resulting documents:\n * // has the field \"name\"\n *\n * @extends BaseQuery\n */\nexport class ExistsQuery extends BaseQuery {\n  /**\n   * @param {string} field - the field name of the document\n   * @param data\n   */\n  constructor(field, data = {}) {\n    super(\"exists\", data);\n    this._data.field = field;\n  }\n}\n\n/**\n * A query which tokenizes the given query text, performs a query foreach token and combines the results using a boolean\n * operator.\n *\n * Operator      | Description\n * ------------- | -------------\n * or (default) | Finds documents which matches some tokens. The minimum amount of matches can be controlled with [minimumShouldMatch]{@link MatchQuery#minimumShouldMatch} (default is 1).\n * and | Finds documents which matches all tokens.\n *\n * To enable a [fuzzy query]{@link FuzzyQuery} for the tokens, use {@link MatchQuery#fuzziness} and {@link MatchQuery#prefixLength}.\n *\n * See also [Lucene#?]{@link ?}\n * and [Elasticsearch#MatchQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .match(\"name\", \"albrt einsten\")\n *     .boost(2.5)\n *     .operator(\"and\")\n *     .fuzziness(2)\n *     .prefixLength(3)\n * .build();\n * // The resulting documents:\n * // contains the fuzzy name albrt einsten (like Albert Einstein) with a boost of 2.5\n *\n * @extends BaseQuery\n */\nexport class MatchQuery extends BaseQuery {\n  /**\n   * @param {string} field - the field name of the document\n   * @param {string} query - the query text\n   * @param data\n   */\n  constructor(field, query, data = {}) {\n    super(\"match\", data);\n    this._data.field = field;\n    this._data.value = query;\n  }\n\n  /**\n   * Controls the amount of minimum matching sub queries before a document will be considered.\n   * @param {number} minShouldMatch - number of minimum matching sub queries\n   *   minShouldMatch >= 1: Indicates a fixed value regardless of the number of sub queries.\n   *   minShouldMatch <= -1: Indicates that the number of sub queries, minus this number should be mandatory.\n   *   minShouldMatch < 0: Indicates that this percent of the total number of sub queries can be missing.\n   *     The number computed from the percentage is rounded down, before being subtracted from the total to determine\n   *     the minimum.\n   *   minShouldMatch < 1: Indicates that this percent of the total number of sub queries are necessary.\n   *     The number computed from the percentage is rounded down and used as the minimum.\n   * @return {MatchQuery} object itself for cascading\n   */\n  minimumShouldMatch(minShouldMatch) {\n    if (this._data.operator !== undefined && this._data.operator === \"and\") {\n      throw SyntaxError(\"Match query with \\\"and\\\" operator does not support minimum should match.\");\n    }\n    this._data.minimum_should_match = minShouldMatch;\n    return this;\n  }\n\n  /**\n   * Sets the boolean operator.\n   * @param {string} op - the operator (_or_/_and_)\n   * @return {MatchQuery} object itself for cascading\n   */\n  operator(op) {\n    if (op !== 'and' && op !== 'or') {\n      throw SyntaxError(\"Unknown operator.\");\n    }\n    this._data.operator = op;\n    if (this._data.minimum_should_match !== undefined && this._data.operator === \"and\") {\n      throw SyntaxError(\"Match query with \\\"and\\\" operator does not support minimum should match.\");\n    }\n    return this;\n  }\n\n  /**\n   * Sets the maximal allowed fuzziness.\n   * @param {number|string} fuzziness - the edit distance as number or AUTO\n   *\n   * AUTO generates an edit distance based on the length of the term:\n   * * 0..2 -> must match exactly\n   * * 3..5 -> one edit allowed\n   * * >5 two edits allowed\n   *\n   * @return {MatchQuery} - object itself for cascading\n   */\n  fuzziness(fuzziness) {\n    if (fuzziness !== \"AUTO\" && fuzziness < 0) {\n      throw TypeError(\"Fuzziness must be a positive number or AUTO.\");\n    }\n    this._data.fuzziness = fuzziness;\n    return this;\n  }\n\n  /**\n   * Sets the starting word length which should not be considered for fuzziness.\n   * @param {number} prefixLength - the positive prefix length\n   * @return {MatchQuery} - object itself for cascading\n   */\n  prefixLength(prefixLength) {\n    if (prefixLength < 0) {\n      throw TypeError(\"Prefix length must be a positive number.\");\n    }\n    this._data.prefix_length = prefixLength;\n    return this;\n  }\n}\n\n/**\n * A query that matches all documents and giving them a constant score equal to the query boost.\n *\n * Typically used inside a must clause of a {@link BoolQuery} to subsequently reject non matching documents with the not\n * clause.\n *\n * See also [Lucene#MatchAllDocsQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/MatchAllDocsQuery.html}\n * and [Elasticsearch#MatchAllQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-all-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .matchAll()\n *     .boost(2.5)\n * .build()\n * // The resulting documents:\n * // all documents and giving a score of 2.5\n *\n * @extends BaseQuery\n */\nexport class MatchAllQuery extends BaseQuery {\n  constructor(data = {}) {\n    super(\"match_all\", data);\n  }\n}\n\n/**\n * A query which holds all sub queries like an array.\n * @private\n */\nclass ArrayQuery extends BaseQuery {\n  constructor(callbackName, callback, data = {}) {\n    super(\"array\", data);\n    this._data.values = [];\n    this._callbackName = callbackName;\n    this[callbackName] = callback;\n\n    this._prepare = (queryType, ...args) => {\n      let data = {};\n      let query = new queryType(...args, data);\n      this._data.values.push(data);\n      query.bool = this.bool;\n      query.constantScore = this.constantScore;\n      query.term = this.term;\n      query.terms = this.terms;\n      query.wildcard = this.wildcard;\n      query.fuzzy = this.fuzzy;\n      query.match = this.match;\n      query.matchAll = this.matchAll;\n      query.prefix = this.prefix;\n      query.exists = this.exists;\n      query._prepare = this._prepare;\n      query[this._callbackName] = this[this._callbackName];\n      return query;\n    };\n  }\n\n  bool() {\n    return this._prepare(BoolQuery);\n  }\n\n  constantScore() {\n    return this._prepare(ConstantScoreQuery);\n  }\n\n  term(field, term) {\n    return this._prepare(TermQuery, field, term);\n  }\n\n  terms(field, terms) {\n    return this._prepare(TermsQuery, field, terms);\n  }\n\n  wildcard(field, wildcard) {\n    return this._prepare(WildcardQuery, field, wildcard);\n  }\n\n  fuzzy(field, fuzzy) {\n    return this._prepare(FuzzyQuery, field, fuzzy);\n  }\n\n  match(field, query) {\n    return this._prepare(MatchQuery, field, query);\n  }\n\n  matchAll() {\n    return this._prepare(MatchAllQuery);\n  }\n\n  prefix(field, prefix) {\n    return this._prepare(PrefixQuery, field, prefix);\n  }\n\n  exists(field) {\n    return this._prepare(ExistsQuery, field);\n  }\n}\n\n/**\n * A query that wraps sub queries and returns a constant score equal to the query boost for every document in the filter.\n *\n * See also [Lucene#BooleanQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/ConstantScoreQuery.html}\n * and [Elasticsearch#ConstantScoreQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-constant-score-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .constantScore()\n *     .boost(1.5)\n *     .beginFilter()\n *       .term(\"first_name\", \"albert\")\n *       .term(\"surname\", \"einstein\")\n *     .endFilter()\n * .build()\n * // The resulting documents:\n * // * contains albert as first name, einstein as surname and the document score is 42.\n *\n * @extends BaseQuery\n */\nexport class ConstantScoreQuery extends BaseQuery {\n  constructor(data = {}) {\n    super(\"constant_score\", data);\n  }\n\n  /**\n   * Starts an array of queries. Use endFilter() to finish the array.\n   * @return {ArrayQuery} array query for holding sub queries\n   */\n  beginFilter() {\n    this._data.filter = {};\n    return new ArrayQuery(\"endFilter\", () => {\n      return this;\n    }, this._data.filter);\n  }\n}\n\n/**\n * A query that matches documents matching boolean combinations of sub queries.\n *\n * This query consists of one or more boolean clauses with different behavior but interrelated to each other.\n *\n * Occur         | Description\n * ------------- | -------------\n * must  | Finds documents which matches all sub queries.\n * filter  | Finds documents which matches all sub queries but these documents do not contribute to the score.\n * should  | Finds documents which matches some sub queries. The minimum amount of matches can be controlled with [minimumShouldMatch]{@link BoolQuery#minimumShouldMatch} (default is 1).\n * not  | Documents which match any sub query will be ignored.\n *\n * A sub query can be any other query type and also the bool query itself.\n *\n * See also [Lucene#BooleanQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/BooleanQuery.html}\n * and [Elasticsearch#BoolQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .bool()\n *     .beginMust().boost(2)\n *       .term(\"first_name\", \"albert\")\n *     .endMust()\n *     .beginFilter()\n *       .term(\"birthplace\", \"ulm\")\n *     .endFilter()\n *     .beginShould().minimumShouldMatch(2)\n *       .fuzzy(\"surname\", \"einstin\")\n *       .wildcard(\"name\", \"geni?s\")\n *       .term(\"quotes\", \"infinity\")\n *     .endShould()\n *     .beginNot()\n *       .terms(\"research_field\", [\"biology\", \"geography\"])\n *     .endNot()\n * .build();\n * // The resulting documents:\n * // contains the name albert (must: contribute to the score with a boost of 2)\n * // contains the birthplace ulm (filter: not contribute to the score)\n * // contains a minimum of two matches from the fuzzy, wildcard and/or term query (should: contribute to the score)\n * // do not contains biology or geography as research field (not: not contribute to the score)\n *\n * @extends BaseQuery\n */\nexport class BoolQuery extends BaseQuery {\n  constructor(data = {}) {\n    super(\"bool\", data);\n  }\n\n  /**\n   * Starts an array of queries for must clause. Use endMust() to finish the array.\n   * @return {ArrayQuery} array query for holding sub queries\n   */\n  beginMust() {\n    this._data.must = {};\n    return new ArrayQuery(\"endMust\", () => {\n      return this;\n    }, this._data.must);\n  }\n\n  /**\n   * Starts an array of queries for filter clause. Use endFilter() to finish the array.\n   * @return {ArrayQuery} array query for holding sub queries\n   */\n  beginFilter() {\n    this._data.filter = {};\n    return new ArrayQuery(\"endFilter\", () => {\n      return this;\n    }, this._data.filter);\n  }\n\n  /**\n   * Starts an array of queries for should clause. Use endShould() to finish the array.\n   * @return {ArrayQuery} array query for holding sub queries\n   */\n  beginShould() {\n    this._data.should = {};\n    return new ArrayQuery(\"endShould\", () => {\n      return this;\n    }, this._data.should);\n  }\n\n  /**\n   * Starts an array of queries for not clause. Use endNot() to finish the array.\n   * @return {ArrayQuery} array query for holding sub queries\n   */\n  beginNot() {\n    this._data.not = {};\n    return new ArrayQuery(\"endNot\", () => {\n      return this;\n    }, this._data.not);\n  }\n\n  /**\n   * Controls the amount of minimum matching sub queries before a document will be considered.\n   * @param {number} minShouldMatch - number of minimum matching sub queries\n   *   minShouldMatch >= 1: Indicates a fixed value regardless of the number of sub queries.\n   *   minShouldMatch <= -1: Indicates that the number of sub queries, minus this number should be mandatory.\n   *   minShouldMatch < 0: Indicates that this percent of the total number of sub queries can be missing.\n   *     The number computed from the percentage is rounded down, before being subtracted from the total to determine\n   *     the minimum.\n   *   minShouldMatch < 1: Indicates that this percent of the total number of sub queries are necessary.\n   *     The number computed from the percentage is rounded down and used as the minimum.\n   * @return {BoolQuery} object itself for cascading\n   */\n  minimumShouldMatch(minShouldMatch) {\n    this._data.minimum_should_match = minShouldMatch;\n    return this;\n  }\n}\n\n/**\n * This query builder is the root of each query search.\n * The query contains a sub query and parameters for setup scoring and search options.\n *\n * Possible sub query types are:\n * {@link TermQuery}, {@link TermsQuery}, {@link FuzzyQuery}, {@link WildcardQuery},\n * {@link MatchQuery}, {@link MatchAllQuery}, {@link PrefixQuery},  {@link BoolQuery},\n * {@link ConstantScoreQuery}, {@link ExistsQuery}\n *\n * @example\n * new QueryBuilder()\n *   .finalScoring(true)\n *   .useBM25(1.5, 0.5)\n *   .term(\"first_name\", \"albert\")\n * .build();\n * // The resulting documents:\n * // contains the first name albert\n * // are scored and ranked using BM25 with k1=1.5 and b=0.5\n */\nexport class QueryBuilder {\n  constructor() {\n    this._data = {query: {}};\n    this.useBM25();\n  }\n\n  /**\n   * The query performs a final scoring over all scored sub queries and rank documents by there relevant.\n   * @param {boolean} enable - flag to enable or disable final scoring\n   * @return {QueryBuilder}\n   */\n  enableFinalScoring(enable) {\n    this._data.final_scoring = enable;\n    return this;\n  }\n\n  /**\n   * Use [Okapi BM25]{@link https://en.wikipedia.org/wiki/Okapi_BM25} as scoring model (default).\n   *\n   * See also [Lucene#MatchAllDocsQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/similarities/BM25Similarity.html}\n   * and [Elasticsearch#BM25]{@link https://www.elastic.co/guide/en/elasticsearch/guide/current/pluggable-similarites.html#bm25}.\n   *\n   * @param {number} [k1=1.2] - controls how quickly an increase in term frequency results in term-frequency saturation.\n   *                            Lower values result in quicker saturation, and higher values in slower saturation.\n   * @param {number} [b=0.75] - controls how much effect field-length normalization should have.\n   *                            A value of 0.0 disables normalization completely, and a value of 1.0 normalizes fully.\n   * @return {QueryBuilder}\n   */\n  useBM25(k1 = 1.2, b = 0.75) {\n    if (k1 < 0) {\n      throw TypeError(\"BM25s k1 must be a positive number.\");\n    }\n    if (b < 0 || b > 1) {\n      throw TypeError(\"BM25s b must be a number between 0 and 1 inclusive.\");\n    }\n\n    this._data.scoring = {\n      type: \"BM25\",\n      k1,\n      b\n    };\n    return this;\n  }\n\n  bool() {\n    return this._prepare(BoolQuery);\n  }\n\n  constantScore() {\n    return this._prepare(ConstantScoreQuery);\n  }\n\n  term(field, term) {\n    return this._prepare(TermQuery, field, term);\n  }\n\n  terms(field, terms) {\n    return this._prepare(TermsQuery, field, terms);\n  }\n\n  wildcard(field, wildcard) {\n    return this._prepare(WildcardQuery, field, wildcard);\n  }\n\n  fuzzy(field, fuzzy) {\n    return this._prepare(FuzzyQuery, field, fuzzy);\n  }\n\n  match(field, query) {\n    return this._prepare(MatchQuery, field, query);\n  }\n\n  matchAll() {\n    return this._prepare(MatchAllQuery);\n  }\n\n  prefix(field, prefix) {\n    return this._prepare(PrefixQuery, field, prefix);\n  }\n\n  exists(field) {\n    return this._prepare(ExistsQuery, field);\n  }\n\n  _prepare(queryType, ...args) {\n    this._child = new queryType(...args, this._data.query);\n    this._child.build = () => {\n      return this._data;\n    };\n    return this._child;\n  }\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./packages/full-text-search/src/queries.js\n// module id = 2\n// module chunks = 0","export class Scorer {\n  constructor(invIdxs) {\n    this._invIdxs = invIdxs;\n    this._cache = {};\n  }\n\n  setDirty() {\n    this._cache = {};\n  }\n\n  prepare(fieldName, boost, termIdx, doScoring, docResults = {}, term = null) {\n    if (termIdx === null || termIdx.dc === undefined) {\n      return null;\n    }\n\n    let idf = this._idf(fieldName, termIdx.df);\n    let docIds = Object.keys(termIdx.dc);\n    for (let j = 0; j < docIds.length; j++) {\n      let docId = docIds[j];\n      if (docResults[docId] === undefined) {\n        docResults[docId] = [];\n      }\n\n      if (doScoring) {\n        let tf = termIdx.dc[docId];\n        docResults[docId].push({\n          type: 'BM25',\n          tf,\n          idf,\n          boost,\n          fieldName,\n          term\n        });\n      } else {\n        docResults[docId] = [{\n          type: \"constant\", value: 1, boost, fieldName\n        }];\n      }\n    }\n\n    return docResults;\n  }\n\n  scoreConstant(boost, docId, docResults = {}) {\n    if (docResults[docId] === undefined) {\n      docResults[docId] = [];\n    }\n    docResults[docId].push({type: \"constant\", value: 1, boost});\n    return docResults;\n  }\n\n  finalScore(query, docResults = {}) {\n    let result = {};\n    let k1 = query.scoring.k1;\n    let b = query.scoring.b;\n\n    let docs = Object.keys(docResults);\n    for (let i = 0, docId; i < docs.length, docId = docs[i]; i++) {\n      let docScore = 0;\n      for (let j = 0; j < docResults[docId].length; j++) {\n        let docResult = docResults[docId][j];\n\n        let res = 0;\n        switch (docResult.type) {\n          case 'BM25': {\n            let tf = docResult.tf;\n            let fieldLength = Scorer._calculateFieldLength(this._invIdxs[docResult.fieldName].documentStore[docId]\n\t\t\t\t\t\t\t.fieldLength);\n            let avgFieldLength = this._avgFieldLength(docResult.fieldName);\n\t\t\t\t\t\t// tfNorm, computed as (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * fieldLength / avgFieldLength)) from\n            let tfNorm = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (fieldLength / avgFieldLength)));\n            res = docResult.idf * tfNorm * docResult.boost;\n\t\t\t\t\t\t// console.log(\n\t\t\t\t\t\t// \tdocId + \":\" + docResult.fieldName + \":\" + docResult.term + \" = \" + res,\n\t\t\t\t\t\t// \t\"\\n\\ttype: BM25\",\n\t\t\t\t\t\t// \t\"\\n\\tboost: \" + docResult.boost,\n\t\t\t\t\t\t// \t\"\\n\\tidf : \" + docResult.idf,\n\t\t\t\t\t\t// \t\"\\n\\ttfNorm : \" + tfNorm,\n\t\t\t\t\t\t// \t\"\\n\\ttf : \" + tf,\n\t\t\t\t\t\t// \t\"\\n\\tavg : \" + avgFieldLength,\n\t\t\t\t\t\t// \t\"\\n\\tfl : \" + fieldLength);\n            break;\n          }\n          case 'constant':\n            res = docResult.value * docResult.boost;\n\t\t\t\t\t\t/*console.log(\n\t\t\t\t\t\t \"Constant: \" + res,\n\t\t\t\t\t\t \"\\n\\tboost: \" + docResult.boost,\n\t\t\t\t\t\t \"\\n\\tvalue : \" + docResult.value);*/\n            break;\n        }\n        docScore += res;\n      }\n\t\t\t//console.log(docId, \" === \", docScore);\n      result[docId] = docScore;\n    }\n    return result;\n  }\n\n  static _calculateFieldLength(fieldLength) {\n\t\t// Lucene uses a SmallFloat (size of 1 byte) to store the field length in scoring.\n\t\t// This is useless in javascript, because every number is represented as a double (8 byte).\n\t\t// To align the scoring result with lucene, this calculation is still needed.\n\t\t// Lucene also includes the field boost, but field boost is deprecated and not supported by Loki.\n\n\t\t// Find closest value in array.\n    const lockUp = [1, 1.30612242, 1.77777779, 2.55999994, 4, 5.22448969, 7.11111116, 10.2399998, 16, 20.8979588,\n      28.4444447, 40.9599991, 64, 83.591835, 113.777779, 163.839996, 256, 334.36734, 455.111115, 655.359985, 1024,\n      1337.46936, 1820.44446, 2621.43994, 4096, 5349.87744, 7281.77783, 10485.7598, 16384, 21399.5098, 29127.1113,\n      41943.0391, 65536, 85598.0391, 116508.445, 167772.156, 262144, 342392.156, 466033.781, 671088.625, 1048576,\n      1369568.62, 1864135.12, 2684354.5, 4194304, 5478274.5, 7456540.5, 10737418, 16777216, 21913098, 29826162,\n      42949672, 67108864, 87652392, 119304648, 171798688, 268435456, 350609568, 477218592, 687194752];\n\n    for (let i = 0; i < lockUp.length; i++) {\n      if (lockUp[i] >= fieldLength) {\n        return lockUp[i];\n      }\n    }\n    throw RangeError(\"Unsupported field length.\");\n  }\n\n  _getCache(fieldName) {\n    if (this._cache[fieldName] === undefined) {\n      let avgFieldLength = this._invIdxs[fieldName].totalFieldLength / this._invIdxs[fieldName].documentCount;\n      this._cache[fieldName] = {idfs: {}, avgFieldLength};\n    }\n    return this._cache[fieldName];\n  }\n\n\t/**\n\t * Returns the idf by either calculate it or use a cached one.\n\t * @param {string} fieldName - the name of the field\n\t * @param {number} docFreq - the doc frequency of the term\n\t * @returns {number} the idf\n\t * @private\n\t */\n  _idf(fieldName, docFreq) {\n    let cache = this._getCache(fieldName);\n    if (cache.idfs[docFreq] !== undefined) {\n      return cache.idfs[docFreq];\n    }\n    return cache.idfs[docFreq] = Math.log(1 + (this._invIdxs[fieldName].documentCount - docFreq + 0.5) / (docFreq + 0.5));\n  }\n\n  _avgFieldLength(fieldName) {\n    return this._getCache(fieldName).avgFieldLength;\n  }\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./packages/full-text-search/src/scorer.js\n// module id = 3\n// module chunks = 0","import {InvertedIndex} from './inverted_index';\nimport {IndexSearcher} from './index_searcher';\nimport {Tokenizer} from './tokenizer';\n\nexport class FullTextSearch {\n  /**\n   * Initialize the full text search for the given fields.\n   * @param {object[]} fields - the field options\n   * @param {string} fields.name - the name of the field\n   * @param {boolean=true} fields.store - flag to indicate if the full text search should be stored on serialization or\n   *  rebuild on deserialization\n   * @param {boolean=true} fields.optimizeChanges - flag to indicate if deleting/updating a document should be optimized\n   *  (requires more memory but performs better)\n   * @param {Tokenizer=Tokenizer} fields.tokenizer - the tokenizer of the field\n   * @param {string=$loki} id - the property name of the document index\n   */\n  constructor(fields, {id = \"$loki\"} = {}) {\n    if (fields === undefined) {\n      throw new SyntaxError('Fields needs to be defined!');\n    }\n\n    this._invIdxs = {};\n    // Create inverted indices for each field.\n    for (let i = 0; i < fields.length; i++) {\n      let field = fields[i];\n      this._invIdxs[field.name] = new InvertedIndex(field);\n    }\n    this._id = id;\n    this._docs = new Set();\n    this._idxSearcher = new IndexSearcher(this._invIdxs, this._docs);\n  }\n\n  addDocument(doc) {\n    if (doc[this._id] === undefined) {\n      throw new Error('Document is not stored in the collection.');\n    }\n\n    let fieldNames = Object.keys(doc);\n    for (let i = 0, fieldName; i < fieldNames.length, fieldName = fieldNames[i]; i++) {\n      if (this._invIdxs[fieldName] !== undefined) {\n        this._invIdxs[fieldName].insert(doc[fieldName], doc[this._id]);\n      }\n    }\n\n    this._docs.add(doc[this._id]);\n    this.setDirty();\n  }\n\n  removeDocument(doc) {\n    if (doc[this._id] === undefined) {\n      throw new Error('Document is not stored in the collection.');\n    }\n\n    let fieldNames = Object.keys(this._invIdxs);\n    for (let i = 0; i < fieldNames.length; i++) {\n      this._invIdxs[fieldNames[i]].remove(doc[this._id]);\n    }\n\n    this._docs.delete(doc[this._id]);\n    this.setDirty();\n  }\n\n  updateDocument(doc) {\n    this.removeDocument(doc);\n    this.addDocument(doc);\n  }\n\n  search(query) {\n    return this._idxSearcher.search(query);\n  }\n\n  toJSON() {\n    let serialized = {};\n    let fieldNames = Object.keys(this._invIdxs);\n    for (let i = 0, fieldName; i < fieldNames.length, fieldName = fieldNames[i]; i++) {\n      serialized[fieldName] = this._invIdxs[fieldName].toJSON();\n    }\n    return serialized;\n  }\n\n  static fromJSONObject(serialized, tokenizers) {\n    let db = JSON.parse(serialized);\n    let fts = new FullTextSearch();\n    let fieldNames = Object.keys(db);\n    for (let i = 0, fieldName; i < fieldNames.length, fieldName = fieldNames[i]; i++) {\n      fts._invIdxs[fieldName] = new InvertedIndex();\n      fts._invIdxs[fieldName].loadJSON(db[fieldName], tokenizers[fieldName]);\n    }\n    return fts;\n  }\n\n  setDirty() {\n    this._idxSearcher.setDirty();\n  }\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./packages/full-text-search/src/full_text_search.js\n// module id = 4\n// module chunks = 0","/*\n * From MihaiValentin/lunr-languages.\n * Last update from 04/16/2017 - 19af41fb9bd644d9081ad274f96f700b21464290\n */\nimport {generateTrimmer, generateStopWordFilter, Among, SnowballProgram} from './support.js';\nimport {Tokenizer} from '../tokenizer';\n\nlet wordCharacters = \"A-Za-z\\xAA\\xBA\\xC0-\\xD6\\xD8-\\xF6\\xF8-\\u02B8\\u02E0-\\u02E4\\u1D00-\\u1D25\\u1D2C-\\u1D5C\\u1D62-\\u1D65\\u1D6B-\\u1D77\\u1D79-\\u1DBE\\u1E00-\\u1EFF\\u2071\\u207F\\u2090-\\u209C\\u212A\\u212B\\u2132\\u214E\\u2160-\\u2188\\u2C60-\\u2C7F\\uA722-\\uA787\\uA78B-\\uA7AD\\uA7B0-\\uA7B7\\uA7F7-\\uA7FF\\uAB30-\\uAB5A\\uAB5C-\\uAB64\\uFB00-\\uFB06\\uFF21-\\uFF3A\\uFF41-\\uFF5A\";\nlet trimmer = generateTrimmer(wordCharacters);\n\nlet tkz = new Tokenizer();\n\ntkz.add('trimmer-de', trimmer);\n\nlet stemmer = ((() => {\n\t/* create the wrapped stemmer object */\n  let st = new (function GermanStemmer() {\n    let a_0 = [new Among(\"\", -1, 6), new Among(\"U\", 0, 2),\n      new Among(\"Y\", 0, 1), new Among(\"\\u00E4\", 0, 3),\n      new Among(\"\\u00F6\", 0, 4), new Among(\"\\u00FC\", 0, 5)\n    ];\n\n    let a_1 = [\n      new Among(\"e\", -1, 2), new Among(\"em\", -1, 1),\n      new Among(\"en\", -1, 2), new Among(\"ern\", -1, 1),\n      new Among(\"er\", -1, 1), new Among(\"s\", -1, 3),\n      new Among(\"es\", 5, 2)\n    ];\n\n    let a_2 = [new Among(\"en\", -1, 1),\n      new Among(\"er\", -1, 1), new Among(\"st\", -1, 2),\n      new Among(\"est\", 2, 1)\n    ];\n\n    let a_3 = [new Among(\"ig\", -1, 1),\n      new Among(\"lich\", -1, 1)\n    ];\n\n    let a_4 = [new Among(\"end\", -1, 1),\n      new Among(\"ig\", -1, 2), new Among(\"ung\", -1, 1),\n      new Among(\"lich\", -1, 3), new Among(\"isch\", -1, 2),\n      new Among(\"ik\", -1, 2), new Among(\"heit\", -1, 3),\n      new Among(\"keit\", -1, 4)\n    ];\n\n    let g_v = [17, 65, 16, 1, 0, 0, 0, 0, 0, 0,\n      0, 0, 0, 0, 0, 0, 8, 0, 32, 8\n    ];\n\n    let g_s_ending = [117, 30, 5];\n\n    let g_st_ending = [\n      117, 30, 4\n    ];\n\n    let I_x;\n    let I_p2;\n    let I_p1;\n    let sbp = new SnowballProgram();\n    this.setCurrent = (word) => {\n      sbp.setCurrent(word);\n    };\n    this.getCurrent = () => sbp.getCurrent();\n\n    function habr1(c1, c2, v_1) {\n      if (sbp.eq_s(1, c1)) {\n        sbp.ket = sbp.cursor;\n        if (sbp.in_grouping(g_v, 97, 252)) {\n          sbp.slice_from(c2);\n          sbp.cursor = v_1;\n          return true;\n        }\n      }\n      return false;\n    }\n\n    function r_prelude() {\n      let v_1 = sbp.cursor;\n      let v_2;\n      let v_3;\n      let v_4;\n      let v_5;\n      while (true) {\n        v_2 = sbp.cursor;\n        sbp.bra = v_2;\n        if (sbp.eq_s(1, \"\\u00DF\")) {\n          sbp.ket = sbp.cursor;\n          sbp.slice_from(\"ss\");\n        } else {\n          if (v_2 >= sbp.limit)\n            break;\n          sbp.cursor = v_2 + 1;\n        }\n      }\n      sbp.cursor = v_1;\n      while (true) {\n        v_3 = sbp.cursor;\n        while (true) {\n          v_4 = sbp.cursor;\n          if (sbp.in_grouping(g_v, 97, 252)) {\n            v_5 = sbp.cursor;\n            sbp.bra = v_5;\n            if (habr1(\"u\", \"U\", v_4))\n              break;\n            sbp.cursor = v_5;\n            if (habr1(\"y\", \"Y\", v_4))\n              break;\n          }\n          if (v_4 >= sbp.limit) {\n            sbp.cursor = v_3;\n            return;\n          }\n          sbp.cursor = v_4 + 1;\n        }\n      }\n    }\n\n    function habr2() {\n      while (!sbp.in_grouping(g_v, 97, 252)) {\n        if (sbp.cursor >= sbp.limit)\n          return true;\n        sbp.cursor++;\n      }\n      while (!sbp.out_grouping(g_v, 97, 252)) {\n        if (sbp.cursor >= sbp.limit)\n          return true;\n        sbp.cursor++;\n      }\n      return false;\n    }\n\n    function r_mark_regions() {\n      I_p1 = sbp.limit;\n      I_p2 = I_p1;\n      let c = sbp.cursor + 3;\n      if (0 <= c && c <= sbp.limit) {\n        I_x = c;\n        if (!habr2()) {\n          I_p1 = sbp.cursor;\n          if (I_p1 < I_x)\n            I_p1 = I_x;\n          if (!habr2())\n            I_p2 = sbp.cursor;\n        }\n      }\n    }\n\n    function r_postlude() {\n      let among_var;\n      let v_1;\n      while (true) {\n        v_1 = sbp.cursor;\n        sbp.bra = v_1;\n        among_var = sbp.find_among(a_0, 6);\n        if (!among_var)\n          return;\n        sbp.ket = sbp.cursor;\n        switch (among_var) {\n          case 1:\n            sbp.slice_from(\"y\");\n            break;\n          case 2:\n          case 5:\n            sbp.slice_from(\"u\");\n            break;\n          case 3:\n            sbp.slice_from(\"a\");\n            break;\n          case 4:\n            sbp.slice_from(\"o\");\n            break;\n          case 6:\n            if (sbp.cursor >= sbp.limit)\n              return;\n            sbp.cursor++;\n            break;\n        }\n      }\n    }\n\n    function r_R1() {\n      return I_p1 <= sbp.cursor;\n    }\n\n    function r_R2() {\n      return I_p2 <= sbp.cursor;\n    }\n\n    function r_standard_suffix() {\n      let among_var;\n      let v_1 = sbp.limit - sbp.cursor;\n      let v_2;\n      let v_3;\n      let v_4;\n      sbp.ket = sbp.cursor;\n      among_var = sbp.find_among_b(a_1, 7);\n      if (among_var) {\n        sbp.bra = sbp.cursor;\n        if (r_R1()) {\n          switch (among_var) {\n            case 1:\n              sbp.slice_del();\n              break;\n            case 2:\n              sbp.slice_del();\n              sbp.ket = sbp.cursor;\n              if (sbp.eq_s_b(1, \"s\")) {\n                sbp.bra = sbp.cursor;\n                if (sbp.eq_s_b(3, \"nis\"))\n                  sbp.slice_del();\n              }\n              break;\n            case 3:\n              if (sbp.in_grouping_b(g_s_ending, 98, 116))\n                sbp.slice_del();\n              break;\n          }\n        }\n      }\n      sbp.cursor = sbp.limit - v_1;\n      sbp.ket = sbp.cursor;\n      among_var = sbp.find_among_b(a_2, 4);\n      if (among_var) {\n        sbp.bra = sbp.cursor;\n        if (r_R1()) {\n          switch (among_var) {\n            case 1:\n              sbp.slice_del();\n              break;\n            case 2:\n              if (sbp.in_grouping_b(g_st_ending, 98, 116)) {\n                let c = sbp.cursor - 3;\n                if (sbp.limit_backward <= c && c <= sbp.limit) {\n                  sbp.cursor = c;\n                  sbp.slice_del();\n                }\n              }\n              break;\n          }\n        }\n      }\n      sbp.cursor = sbp.limit - v_1;\n      sbp.ket = sbp.cursor;\n      among_var = sbp.find_among_b(a_4, 8);\n      if (among_var) {\n        sbp.bra = sbp.cursor;\n        if (r_R2()) {\n          switch (among_var) {\n            case 1:\n              sbp.slice_del();\n              sbp.ket = sbp.cursor;\n              if (sbp.eq_s_b(2, \"ig\")) {\n                sbp.bra = sbp.cursor;\n                v_2 = sbp.limit - sbp.cursor;\n                if (!sbp.eq_s_b(1, \"e\")) {\n                  sbp.cursor = sbp.limit - v_2;\n                  if (r_R2())\n                    sbp.slice_del();\n                }\n              }\n              break;\n            case 2:\n              v_3 = sbp.limit - sbp.cursor;\n              if (!sbp.eq_s_b(1, \"e\")) {\n                sbp.cursor = sbp.limit - v_3;\n                sbp.slice_del();\n              }\n              break;\n            case 3:\n              sbp.slice_del();\n              sbp.ket = sbp.cursor;\n              v_4 = sbp.limit - sbp.cursor;\n              if (!sbp.eq_s_b(2, \"er\")) {\n                sbp.cursor = sbp.limit - v_4;\n                if (!sbp.eq_s_b(2, \"en\"))\n                  break;\n              }\n              sbp.bra = sbp.cursor;\n              if (r_R1())\n                sbp.slice_del();\n              break;\n            case 4:\n              sbp.slice_del();\n              sbp.ket = sbp.cursor;\n              among_var = sbp.find_among_b(a_3, 2);\n              if (among_var) {\n                sbp.bra = sbp.cursor;\n                if (r_R2() && among_var === 1)\n                  sbp.slice_del();\n              }\n              break;\n          }\n        }\n      }\n    }\n\n    this.stem = () => {\n      let v_1 = sbp.cursor;\n      r_prelude();\n      sbp.cursor = v_1;\n      r_mark_regions();\n      sbp.limit_backward = v_1;\n      sbp.cursor = sbp.limit;\n      r_standard_suffix();\n      sbp.cursor = sbp.limit_backward;\n      r_postlude();\n      return true;\n    };\n  });\n\n\t/* and return a function that stems a word for the current locale */\n  return (token) => {\n    st.setCurrent(token);\n    st.stem();\n    return st.getCurrent();\n  };\n}))();\n\ntkz.setSplitter(\"whitespace-splitter\", function defaultSplitter(str) {\n  let trimmedTokens = [];\n  let tokens = str.split(/[\\s\\-]+/);\n  for (let i = 0; i < tokens.length; i++) {\n    if (tokens[i] !== '') {\n      trimmedTokens.push(tokens[i].toLowerCase());\n    }\n  }\n  return trimmedTokens;\n});\n\ntkz.add('stemmer-de', stemmer);\n\nlet stopWordFilter = generateStopWordFilter([\"aber\", \"alle\", \"allem\", \"allen\", \"aller\", \"alles\", \"als\", \"also\", \"am\", \"an\", \"ander\", \"andere\", \"anderem\", \"anderen\", \"anderer\", \"anderes\", \"anderm\", \"andern\", \"anderr\", \"anders\", \"auch\", \"auf\", \"aus\", \"bei\", \"bin\", \"bis\", \"bist\", \"da\", \"damit\", \"dann\", \"das\", \"dasselbe\", \"dazu\", \"daÃŸ\", \"dein\", \"deine\", \"deinem\", \"deinen\", \"deiner\", \"deines\", \"dem\", \"demselben\", \"den\", \"denn\", \"denselben\", \"der\", \"derer\", \"derselbe\", \"derselben\", \"des\", \"desselben\", \"dessen\", \"dich\", \"die\", \"dies\", \"diese\", \"dieselbe\", \"dieselben\", \"diesem\", \"diesen\", \"dieser\", \"dieses\", \"dir\", \"doch\", \"dort\", \"du\", \"durch\", \"ein\", \"eine\", \"einem\", \"einen\", \"einer\", \"eines\", \"einig\", \"einige\", \"einigem\", \"einigen\", \"einiger\", \"einiges\", \"einmal\", \"er\", \"es\", \"etwas\", \"euch\", \"euer\", \"eure\", \"eurem\", \"euren\", \"eurer\", \"eures\", \"fÃ¼r\", \"gegen\", \"gewesen\", \"hab\", \"habe\", \"haben\", \"hat\", \"hatte\", \"hatten\", \"hier\", \"hin\", \"hinter\", \"ich\", \"ihm\", \"ihn\", \"ihnen\", \"ihr\", \"ihre\", \"ihrem\", \"ihren\", \"ihrer\", \"ihres\", \"im\", \"in\", \"indem\", \"ins\", \"ist\", \"jede\", \"jedem\", \"jeden\", \"jeder\", \"jedes\", \"jene\", \"jenem\", \"jenen\", \"jener\", \"jenes\", \"jetzt\", \"kann\", \"kein\", \"keine\", \"keinem\", \"keinen\", \"keiner\", \"keines\", \"kÃ¶nnen\", \"kÃ¶nnte\", \"machen\", \"man\", \"manche\", \"manchem\", \"manchen\", \"mancher\", \"manches\", \"mein\", \"meine\", \"meinem\", \"meinen\", \"meiner\", \"meines\", \"mich\", \"mir\", \"mit\", \"muss\", \"musste\", \"nach\", \"nicht\", \"nichts\", \"noch\", \"nun\", \"nur\", \"ob\", \"oder\", \"ohne\", \"sehr\", \"sein\", \"seine\", \"seinem\", \"seinen\", \"seiner\", \"seines\", \"selbst\", \"sich\", \"sie\", \"sind\", \"so\", \"solche\", \"solchem\", \"solchen\", \"solcher\", \"solches\", \"soll\", \"sollte\", \"sondern\", \"sonst\", \"um\", \"und\", \"uns\", \"unse\", \"unsem\", \"unsen\", \"unser\", \"unses\", \"unter\", \"viel\", \"vom\", \"von\", \"vor\", \"war\", \"waren\", \"warst\", \"was\", \"weg\", \"weil\", \"weiter\", \"welche\", \"welchem\", \"welchen\", \"welcher\", \"welches\", \"wenn\", \"werde\", \"werden\", \"wie\", \"wieder\", \"will\", \"wir\", \"wird\", \"wirst\", \"wo\", \"wollen\", \"wollte\", \"wÃ¤hrend\", \"wÃ¼rde\", \"wÃ¼rden\", \"zu\", \"zum\", \"zur\", \"zwar\", \"zwischen\", \"Ã¼ber\"]);\ntkz.add('stopWordFilter-de', stopWordFilter);\n\nexport {tkz as DE};\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./packages/full-text-search/src/language/de.js\n// module id = 5\n// module chunks = 0","import {FullTextSearch} from './full_text_search';\nimport {InvertedIndex} from './inverted_index';\nimport {QueryBuilder} from './queries';\nimport {Scorer} from './scorer';\nimport {Tokenizer} from './tokenizer';\nimport {DE} from './language/de';\n\nexport {\n\tFullTextSearch,\n\tTokenizer,\n\tQueryBuilder,\n\tDE,\n\tInvertedIndex,\n\tScorer\n};\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./packages/full-text-search/src/index.js\n// module id = 6\n// module chunks = 0","import {Scorer} from './scorer';\nimport {InvertedIndex} from './inverted_index';\nimport {QueryBuilder} from './queries';\n\nexport class IndexSearcher {\n\t/**\n\t *\n\t * @param {object} invIdxs\n\t */\n  constructor(invIdxs, docs) {\n    this._invIdxs = invIdxs;\n    this._docs = docs;\n    this._scorer = new Scorer(this._invIdxs);\n  }\n\n  search(query) {\n    let docResults = this._recursive(query.query, true);\n\n\t\t// Final scoring.\n    let finalScoring = query.final_scoring !== undefined ? query.final_scoring : true;\n    if (finalScoring) {\n      return this._scorer.finalScore(query, docResults);\n    }\n    return docResults;\n  }\n\n  setDirty() {\n    this._scorer.setDirty();\n  }\n\n  _recursive(query, doScoring) {\n    let docResults = {};\n    let boost = query.boost !== undefined ? query.boost : 1;\n    let fieldName = query.field !== undefined ? query.field : null;\n    let enableScoring = query.enable_scoring !== undefined ? query.enable_scoring : false;\n\n    let root = null;\n    let tokenizer = null;\n    if (this._invIdxs[fieldName] !== undefined) {\n      root = this._invIdxs[fieldName].root;\n      tokenizer = this._invIdxs[fieldName].tokenizer;\n    }\n\n    switch (query.type) {\n      case \"bool\": {\n        docResults = null;\n        if (query.must !== undefined) {\n          docResults = this._getUnique(query.must.values, doScoring, docResults);\n        }\n        if (query.filter !== undefined) {\n          docResults = this._getUnique(query.filter.values, false, docResults);\n        }\n\n        if (query.should !== undefined) {\n          let shouldDocs = this._getAll(query.should.values, doScoring);\n\n          let empty = false;\n          if (docResults === null) {\n            docResults = {};\n            empty = true;\n          }\n\n          let msm = 1;\n\t\t\t\t\t// TODO: Enable percent and ranges.\n          if (query.minimum_should_match !== undefined) {\n            msm = query.minimum_should_match;\n            let shouldLength = query.should.values.length;\n            if (msm <= -1) {\n              msm = shouldLength + msm;\n            } else if (msm < 0) {\n              msm = shouldLength - Math.floor(shouldLength * -msm);\n            } else if (msm < 1) {\n              msm = Math.floor(shouldLength * msm);\n            }\n          }\n\t\t\t\t\t// Remove all docs with fewer matches.\n          let docs = Object.keys(shouldDocs);\n          for (let i = 0, docId; i < docs.length, docId = docs[i]; i++) {\n            if (shouldDocs[docId].length >= msm) {\n              if (docResults[docId] !== undefined) {\n                docResults[docId].push(...shouldDocs[docId]);\n              } else if (empty) {\n                docResults[docId] = shouldDocs[docId];\n              } else {\n                delete docResults[docId];\n              }\n            }\n          }\n        }\n        if (query.not !== undefined) {\n          let notDocs = this._getAll(query.not.values, false);\n\t\t\t\t\t// Remove all docs.\n          let docs = Object.keys(notDocs);\n          for (let i = 0, docId; i < docs.length, docId = docs[i]; i++) {\n            if (docResults[docId] !== undefined) {\n              delete docResults[docId];\n            }\n          }\n        }\n        break;\n      }\n      case \"term\": {\n        let termIdx = InvertedIndex.getTermIndex(query.value, root);\n        this._scorer.prepare(fieldName, boost, termIdx, doScoring, docResults, query.value);\n        break;\n      }\n      case \"terms\": {\n        for (let i = 0; i < query.value.length; i++) {\n          let termIdx = InvertedIndex.getTermIndex(query.value[i], root);\n          this._scorer.prepare(fieldName, boost, termIdx, doScoring, docResults, query.value[i]);\n        }\n        break;\n      }\n      case \"fuzzy\": {\n        let f = new FuzzySearch(query);\n        let b = f.search(root);\n        for (let i = 0; i < b.length; i++) {\n          this._scorer.prepare(fieldName, boost * b[i].boost, b[i].index, doScoring, docResults, b[i].term);\n        }\n        break;\n      }\n      case \"wildcard\": {\n        let w = new WildcardSearch(query);\n        let a = w.search(root);\n        for (let i = 0; i < a.length; i++) {\n          this._scorer.prepare(fieldName, boost, a[i].index, doScoring && enableScoring, docResults, a[i].term);\n        }\n        break;\n      }\n      case \"match_all\": {\n        for (let docId of this._docs) {\n          this._scorer.scoreConstant(boost, docId, docResults);\n        }\n        break;\n      }\n      case \"constant_score\": {\n        let tmpDocResults = this._getAll(query.filter.values, false);\n        let docs = Object.keys(tmpDocResults);\n\t\t\t\t// Add to each document a constant score.\n        for (let i = 0; i < docs.length; i++) {\n          this._scorer.scoreConstant(boost, docs[i], docResults);\n        }\n        break;\n      }\n      case \"prefix\": {\n        let termIdx = InvertedIndex.getTermIndex(query.value, root);\n        if (termIdx !== null) {\n          termIdx = InvertedIndex.extendTermIndex(termIdx);\n          for (let i = 0; i < termIdx.length; i++) {\n            this._scorer.prepare(fieldName, boost, termIdx[i].index, doScoring && enableScoring, docResults, query.value + termIdx[i].term);\n          }\n        }\n        break;\n      }\n      case \"exists\": {\n        if (root !== null) {\n          let docs = Object.keys(this._invIdxs[fieldName].documentStore);\n          for (let i = 0; i < docs.length; i++) {\n            this._scorer.scoreConstant(boost, docs[i], docResults);\n          }\n        }\n        break;\n      }\n      case \"match\": {\n        let terms = tokenizer.tokenize(query.value);\n        let operator = query.operator !== undefined ? query.operator : \"or\";\n\n        let tmpQuery = new QueryBuilder().bool();\n        if (operator === \"or\") {\n          if (query.minimum_should_match !== undefined) {\n            tmpQuery = tmpQuery.minimumShouldMatch(query.minimum_should_match);\n          }\n\t\t\t\t\t// Build a should query.\n          tmpQuery = tmpQuery.beginShould();\n        } else {\n\t\t\t\t\t// Build a must query.\n          tmpQuery = tmpQuery.beginMust();\n        }\n        tmpQuery = tmpQuery.boost(boost);\n\n        if (query.fuzziness !== undefined) {\n          let prefixLength = query.prefix_length !== undefined ? query.prefix_length : 2;\n\t\t\t\t\t// Add each fuzzy.\n          for (let i = 0; i < terms.length; i++) {\n            tmpQuery = tmpQuery.fuzzy(fieldName, terms[i]).fuzziness(query.fuzziness).prefixLength(prefixLength);\n          }\n        } else {\n\t\t\t\t\t// Add each term.\n          for (let i = 0; i < terms.length; i++) {\n            tmpQuery = tmpQuery.term(fieldName, terms[i]);\n          }\n        }\n        if (operator === \"or\") {\n          tmpQuery = tmpQuery.endShould();\n        } else {\n          tmpQuery = tmpQuery.endMust();\n        }\n        docResults = this._recursive(tmpQuery.build().query, doScoring);\n\n        break;\n      }\n      default:\n        break;\n    }\n    return docResults;\n  }\n\n  _getUnique(values, doScoring, docResults) {\n    if (values.length === 0) {\n      return docResults;\n    }\n\n    for (let i = 0; i < values.length; i++) {\n      let currDocs = this._recursive(values[i], doScoring);\n      if (docResults === null) {\n        docResults = this._recursive(values[0], doScoring);\n        continue;\n      }\n\n      let docs = Object.keys(docResults);\n      for (let j = 0, docId; j < docs.length, docId = docs[j]; j++) {\n        if (currDocs[docId] === undefined) {\n          delete docResults[docId];\n        } else {\n          docResults[docId].push(...currDocs[docId]);\n        }\n      }\n    }\n    return docResults;\n  }\n\n  _getAll(values, doScoring) {\n    let docResults = {};\n    for (let i = 0; i < values.length; i++) {\n      let currDocs = this._recursive(values[i], doScoring);\n      let docs = Object.keys(currDocs);\n      for (let j = 0, docId; j < docs.length, docId = docs[j]; j++) {\n        if (docResults[docId] === undefined) {\n          docResults[docId] = currDocs[docId];\n        } else {\n          docResults[docId].push(...currDocs[docId]);\n        }\n      }\n    }\n    return docResults;\n  }\n}\n\n\nclass FuzzySearch {\n  constructor(query) {\n    this._fuzzy = query.value;\n    this._fuzziness = query.fuzziness !== undefined ? query.fuzziness : \"AUTO\";\n    if (this._fuzziness === \"AUTO\") {\n      if (this._fuzzy.length <= 2) {\n        this._fuzziness = 0;\n      } else if (this._fuzzy.length <= 5) {\n        this._fuzziness = 1;\n      } else {\n        this._fuzziness = 2;\n      }\n    }\n    this._prefixLength = query.prefix_length !== undefined ? query.prefix_length : 2;\n  }\n\n\t/**\n\t * Copyright Kigiri: https://github.com/kigiri\n\t *                     Milot Mirdita: https://github.com/milot-mirdita\n\t *                     Toni Neubert:  https://github.com/Viatorus/\n\t */\n  levenshtein_distance(a, b) {\n    if (a.length === 0) return b.length;\n    if (b.length === 0) return a.length;\n    let tmp;\n    let i;\n    let j;\n    let prev;\n    let val;\n\t\t// swap to save some memory O(min(a,b)) instead of O(a)\n    if (a.length > b.length) {\n      tmp = a;\n      a = b;\n      b = tmp;\n    }\n\n    const row = Array(a.length + 1);\n\t\t// init the row\n    for (i = 0; i <= a.length; i++) {\n      row[i] = i;\n    }\n\n\t\t// fill in the rest\n    for (i = 1; i <= b.length; i++) {\n      prev = i;\n      for (j = 1; j <= a.length; j++) {\n        if (b[i - 1] === a[j - 1]) {\t// match\n          val = row[j - 1];\n        } else {\n          val = Math.min(row[j - 1] + 1, // substitution\n\t\t\t\t\t\tMath.min(prev + 1,         // insertion\n\t\t\t\t\t\t\trow[j] + 1));          // deletion\n\n\t\t\t\t\t// transposition.\n          if (i > 1 && j > 1 && b[i - 2] === a[j - 1] && a[j - 2] === b[i - 1]) {\n            val = Math.min(val, row[j - 1] - (a[j - 1] === b[i - 1] ? 1 : 0));\n          }\n        }\n        row[j - 1] = prev;\n        prev = val;\n      }\n      row[a.length] = prev;\n    }\n    return row[a.length];\n  }\n\n\t/**\n\t * Performs a fuzzy search for a given term.\n\t * @param {string} query - a fuzzy term to match.\n\t * @param {number} [maxDistance=2] - maximal edit distance between terms\n\t * @returns {Array} - array with all matching term indices.\n\t */\n  search(root) {\n\t\t// Todo: Include levenshtein to reduce similar iterations.\n\t\t// Tree tokens at same depth share same row until depth (should works if recursive).\n\t\t// Pregenerate tree token ?\n\t\t// var treeToken = Array(token.length + maxDistance);\n\n    let start = root;\n    let pre = this._fuzzy.slice(0, this._prefixLength);\n    let fuzzy = this._fuzzy;\n    if (this._prefixLength !== 0) {\n      start = InvertedIndex.getTermIndex(pre, start);\n      fuzzy = fuzzy.slice(this._prefixLength);\n    }\n    if (start === null) {\n      return [];\n    }\n    if (fuzzy.length === 0) {\n\t\t\t// Return if prefixLength == this._fuzzy length.\n      return [{term: this._fuzziness, index: start, boost: 1}];\n    }\n\n    let similarTokens = [];\n\n    let stack = [start];\n    let treeStack = [''];\n    do {\n      let root = stack.pop();\n      let treeTerms = treeStack.pop();\n\n\t\t\t// Compare tokens if they are in near distance.\n      if (root.df !== undefined && Math.abs(fuzzy.length - treeTerms.length) <= this._fuzziness) {\n        const distance = this.levenshtein_distance(fuzzy, treeTerms);\n        if (distance <= this._fuzziness) {\n          let term = pre + treeTerms;\n\t\t\t\t\t// Calculate boost.\n          let boost = 1 - distance / Math.min(term.length, this._fuzzy.length);\n          similarTokens.push({term, index: root, boost});\n        }\n      }\n\n\t\t\t// Iterate over all subtrees.\n\t\t\t// If token from tree is not longer than maximal distance.\n      if (treeTerms.length - fuzzy.length <= this._fuzziness) {\n\t\t\t\t// Iterate over all subtrees.\n        let keys = Object.keys(root);\n        for (let i = 0; i < keys.length; i++) {\n          if (keys[i].length === 1) {\n            stack.push(root[keys[i]]);\n            treeStack.push(treeTerms + keys[i]);\n          }\n        }\n      }\n    } while (stack.length !== 0);\n\n    return similarTokens;\n  }\n}\n\nclass WildcardSearch {\n\n  constructor(query) {\n    this._wildcard = query.value;\n    this._result = [];\n  }\n\n\t/**\n\t * Performs a wild card search for a given query term.\n\t * @param {string} query - a wild card query to match.\n\t * @returns {Array} - array with all matching term indices.\n\t */\n  search(root) {\n\t\t// Todo: Need an implementation for star operator in the middle.\n    this._result = [];\n    this._recursive(root);\n    return this._result;\n  }\n\n\t/**\n\t *\n\t * @param root\n\t * @param idx\n\t * @param term\n\t * @param escaped\n\t * @private\n\t */\n  _recursive(root, idx = 0, term = '', escaped = false) {\n    if (root === null) {\n      return;\n    }\n\n    if (idx === this._wildcard.length) {\n      if (root.df !== undefined) {\n        this._result.push({index: root, term});\n      }\n      return;\n    }\n\n    if (!escaped && this._wildcard[idx] === '\\\\') {\n      this._recursive(root, idx + 1, term, true);\n    } else if (!escaped && this._wildcard[idx] === '?') {\n      let others = InvertedIndex.getNextTermIndex(root);\n      for (let i = 0; i < others.length; i++) {\n        this._recursive(others[i].index, idx + 1, term + others[i].term);\n      }\n    } else if (!escaped && this._wildcard[idx] === '*') {\n      // Check if asterisk is last wildcard character\n      if (idx + 1 === this._wildcard.length) {\n        let all = InvertedIndex.extendTermIndex(root);\n        for (let i = 0; i < all.length; i++) {\n          this._recursive(all[i].index, idx + 1, term + all[i].term);\n        }\n        return;\n      }\n      // Iterate over the whole tree.\n      this._recursive(root, idx + 1, term);\n      let roots = [{index: root, term: ''}];\n      do {\n        root = roots.pop();\n        let others = InvertedIndex.getNextTermIndex(root.index);\n        for (let i = 0; i < others.length; i++) {\n          this._recursive(others[i].index, idx + 1, term + root.term + others[i].term);\n          roots.push({index: others[i].index, term: root.term + others[i].term});\n        }\n      } while (roots.length !== 0);\n    } else {\n      this._recursive(InvertedIndex.getTermIndex(this._wildcard[idx], root), idx + 1, term + this._wildcard[idx]);\n    }\n  }\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./packages/full-text-search/src/index_searcher.js\n// module id = 7\n// module chunks = 0","/*\n * From MihaiValentin/lunr-languages.\n * Last update from 04/16/2017 - 19af41fb9bd644d9081ad274f96f700b21464290\n */\nexport function generateTrimmer(wordCharacters) {\n  const regex = new RegExp(\"^[^\" + wordCharacters + \"]+|[^\" + wordCharacters + \"]+$\", \"g\");\n  return (token) => token.replace(regex, '');\n}\n\nexport function generateStopWordFilter(stopWords) {\n  const words = new Set(stopWords);\n  return (token) => words.has(token) ? \"\" : token;\n}\n\nexport class Among {\n  constructor(s, substring_i, result, method) {\n    this.toCharArray = (s) => {\n      let sLength = s.length;\n      let charArr = new Array(sLength);\n      for (let i = 0; i < sLength; i++)\n        charArr[i] = s.charCodeAt(i);\n      return charArr;\n    };\n\n    if ((!s && s !== \"\") || (!substring_i && (substring_i !== 0)) || !result)\n      throw (\"Bad Among initialisation: s:\" + s + \", substring_i: \"\n\t\t\t+ substring_i + \", result: \" + result);\n    this.s_size = s.length;\n    this.s = this.toCharArray(s);\n    this.substring_i = substring_i;\n    this.result = result;\n    this.method = method;\n  }\n}\n\nexport class SnowballProgram {\n\n  constructor() {\n    this.current = null;\n    this.bra = 0;\n    this.ket = 0;\n    this.limit = 0;\n    this.cursor = 0;\n    this.limit_backward = 0;\n  }\n\n  setCurrent(word) {\n    this.current = word;\n    this.cursor = 0;\n    this.limit = word.length;\n    this.limit_backward = 0;\n    this.bra = this.cursor;\n    this.ket = this.limit;\n  }\n\n  getCurrent() {\n    let result = this.current;\n    this.current = null;\n    return result;\n  }\n\n  in_grouping(s, min, max) {\n    if (this.cursor < this.limit) {\n      let ch = this.current.charCodeAt(this.cursor);\n      if (ch <= max && ch >= min) {\n        ch -= min;\n        if (s[ch >> 3] & (0X1 << (ch & 0X7))) {\n          this.cursor++;\n          return true;\n        }\n      }\n    }\n    return false;\n  }\n\n  in_grouping_b(s, min, max) {\n    if (this.cursor > this.limit_backward) {\n      let ch = this.current.charCodeAt(this.cursor - 1);\n      if (ch <= max && ch >= min) {\n        ch -= min;\n        if (s[ch >> 3] & (0X1 << (ch & 0X7))) {\n          this.cursor--;\n          return true;\n        }\n      }\n    }\n    return false;\n  }\n\n  out_grouping(s, min, max) {\n    if (this.cursor < this.limit) {\n      let ch = this.current.charCodeAt(this.cursor);\n      if (ch > max || ch < min) {\n        this.cursor++;\n        return true;\n      }\n      ch -= min;\n      if (!(s[ch >> 3] & (0X1 << (ch & 0X7)))) {\n        this.cursor++;\n        return true;\n      }\n    }\n    return false;\n  }\n\n  out_grouping_b(s, min, max) {\n    if (this.cursor > this.limit_backward) {\n      let ch = this.current.charCodeAt(this.cursor - 1);\n      if (ch > max || ch < min) {\n        this.cursor--;\n        return true;\n      }\n      ch -= min;\n      if (!(s[ch >> 3] & (0X1 << (ch & 0X7)))) {\n        this.cursor--;\n        return true;\n      }\n    }\n    return false;\n  }\n\n  eq_s(s_size, s) {\n    if (this.limit - this.cursor < s_size)\n      return false;\n    for (let i = 0; i < s_size; i++)\n      if (this.current.charCodeAt(this.cursor + i) !== s.charCodeAt(i))\n        return false;\n    this.cursor += s_size;\n    return true;\n  }\n\n  eq_s_b(s_size, s) {\n    if (this.cursor - this.limit_backward < s_size)\n      return false;\n    for (let i = 0; i < s_size; i++)\n      if (this.current.charCodeAt(this.cursor - s_size + i) !== s.charCodeAt(i))\n        return false;\n    this.cursor -= s_size;\n    return true;\n  }\n\n  find_among(v, v_size) {\n    let i = 0;\n    let j = v_size;\n    let c = this.cursor;\n    let l = this.limit;\n    let common_i = 0;\n    let common_j = 0;\n    let first_key_inspected = false;\n    while (true) {\n      let k = i + ((j - i) >> 1);\n      let diff = 0;\n\n      let common = common_i < common_j\n\t\t\t\t? common_i\n\t\t\t\t: common_j;\n\n      let w = v[k];\n      for (let i2 = common; i2 < w.s_size; i2++) {\n        if (c + common === l) {\n          diff = -1;\n          break;\n        }\n        diff = this.current.charCodeAt(c + common) - w.s[i2];\n        if (diff)\n          break;\n        common++;\n      }\n      if (diff < 0) {\n        j = k;\n        common_j = common;\n      } else {\n        i = k;\n        common_i = common;\n      }\n      if (j - i <= 1) {\n        if (i > 0 || j === i || first_key_inspected)\n          break;\n        first_key_inspected = true;\n      }\n    }\n    while (true) {\n      let w = v[i];\n      if (common_i >= w.s_size) {\n        this.cursor = c + w.s_size;\n        if (!w.method)\n          return w.result;\n        let res = w.method();\n        this.cursor = c + w.s_size;\n        if (res)\n          return w.result;\n      }\n      i = w.substring_i;\n      if (i < 0)\n        return 0;\n    }\n  }\n\n  find_among_b(v, v_size) {\n    let i = 0;\n    let j = v_size;\n    let c = this.cursor;\n    let lb = this.limit_backward;\n    let common_i = 0;\n    let common_j = 0;\n    let first_key_inspected = false;\n    while (true) {\n      let k = i + ((j - i) >> 1);\n      let diff = 0;\n\n      let common = common_i < common_j\n\t\t\t\t? common_i\n\t\t\t\t: common_j;\n\n      let w = v[k];\n      for (let i2 = w.s_size - 1 - common; i2 >= 0; i2--) {\n        if (c - common === lb) {\n          diff = -1;\n          break;\n        }\n        diff = this.current.charCodeAt(c - 1 - common) - w.s[i2];\n        if (diff)\n          break;\n        common++;\n      }\n      if (diff < 0) {\n        j = k;\n        common_j = common;\n      } else {\n        i = k;\n        common_i = common;\n      }\n      if (j - i <= 1) {\n        if (i > 0 || j === i || first_key_inspected)\n          break;\n        first_key_inspected = true;\n      }\n    }\n    while (true) {\n      let w = v[i];\n      if (common_i >= w.s_size) {\n        this.cursor = c - w.s_size;\n        if (!w.method)\n          return w.result;\n        let res = w.method();\n        this.cursor = c - w.s_size;\n        if (res)\n          return w.result;\n      }\n      i = w.substring_i;\n      if (i < 0)\n        return 0;\n    }\n  }\n\n  replace_s(c_bra, c_ket, s) {\n    let adjustment = s.length - (c_ket - c_bra);\n\n    let left = this.current\n\t\t\t.substring(0, c_bra);\n\n    let right = this.current.substring(c_ket);\n    this.current = left + s + right;\n    this.limit += adjustment;\n    if (this.cursor >= c_ket)\n      this.cursor += adjustment;\n    else if (this.cursor > c_bra)\n      this.cursor = c_bra;\n    return adjustment;\n  }\n\n  slice_check() {\n    if (this.bra < 0 || this.bra > this.ket || this.ket > this.limit\n\t\t\t|| this.limit > this.current.length)\n      throw (\"faulty slice operation\");\n  }\n\n  slice_from(s) {\n    this.slice_check();\n    this.replace_s(this.bra, this.ket, s);\n  }\n\n  slice_del() {\n    this.slice_from(\"\");\n  }\n\n  insert(c_bra, c_ket, s) {\n    let adjustment = this.replace_s(c_bra, c_ket, s);\n    if (c_bra <= this.bra)\n      this.bra += adjustment;\n    if (c_bra <= this.ket)\n      this.ket += adjustment;\n  }\n\n  slice_to() {\n    this.slice_check();\n    return this.current.substring(this.bra, this.ket);\n  }\n\n  eq_v_b(s) {\n    return this.eq_s_b(s.length, s);\n  }\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./packages/full-text-search/src/language/support.js\n// module id = 8\n// module chunks = 0","/**\n * Checks if the variable is a function.\n * @param {*} x - the variable\n * @return {boolean} true if function, otherwise false\n */\nexport function isFunction(x) {\n  return Object.prototype.toString.call(x) === \"[object Function]\";\n}\n\n/**\n * Checks if the variable is an object.\n * @param {*} x - the variable\n * @return {boolean} true if object, otherwise false\n */\nexport function isObject(x) {\n  return Object.prototype.toString.call(x) === \"[object Object]\";\n}\n\n/**\n * Checks if the variable is a number.\n * @param {*} x - the variable\n * @return {boolean} true if number, otherwise false\n */\nexport function isNumber(x) {\n  return Object.prototype.toString.call(x) === \"[object Number]\";\n}\n\n/**\n * Checks if the variable is a boolean.\n * @param {*} x - the variable\n * @return {boolean} true if boolean, otherwise false\n */\nexport function isBoolean(x) {\n  return Object.prototype.toString.call(x) === \"[object Boolean]\";\n}\n\n/**\n * Checks if the variable is a string.\n * @param {*} x - the variable\n * @return {boolean} true if string, otherwise false\n */\nexport function isString(x) {\n  return Object.prototype.toString.call(x) === \"[object String]\";\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./packages/full-text-search/src/utils.js\n// module id = 9\n// module chunks = 0"],"sourceRoot":""}