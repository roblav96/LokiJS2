{"version":3,"sources":["webpack:///webpack/universalModuleDefinition","webpack:///webpack/bootstrap cd2043baf7046fa7110f","webpack:///./src/inverted_index/tokenizer.js","webpack:///./src/inverted_index/inverted_index.js","webpack:///./src/inverted_index/utils.js","webpack:///./src/inverted_index/queries.js","webpack:///./src/inverted_index/scorer.js","webpack:///./src/inverted_index/full_text_search.js","webpack:///./src/inverted_index/language/de.js","webpack:///./src/inverted_index/index.js","webpack:///./src/inverted_index/index_searcher.js","webpack:///./src/inverted_index/language/support.js"],"names":[],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mCAAmC;AACnC,CAAC;AACD,O;ACVA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;;AAGA;AACA;;AAEA;AACA;;AAEA;AACA,mDAA2C,cAAc;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAK;AACL;AACA;;AAEA;AACA;AACA;AACA,mCAA2B,0BAA0B,EAAE;AACvD,yCAAiC,eAAe;AAChD;AACA;AACA;;AAEA;AACA,8DAAsD,+DAA+D;;AAErH;AACA;;AAEA;AACA;;;;;;;;;AChEA;;AAEA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,SAAS;AACtB;AACA;AACA;AACA;AACA,gBAAgB,mBAAmB;AACnC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,2CAA2C,4BAA4B;AACvE,yCAAyC,oBAAoB,GAAG,0BAA0B;AAC1F,MAAM,yBAAyB;AAC/B,+CAA+C,uBAAuB;AACtE,kCAAkC,sBAAsB;AACxD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,YAAY,OAAO;AACnB,YAAY,SAAS;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,yBAAyB;AACtC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,gBAAgB;AAC5B,cAAc,QAAQ;AACtB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,YAAY,gBAAgB;AAC5B,aAAa,yBAAyB;AACtC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,YAAY,OAAO;AACnB,YAAY,SAAS;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,YAAY,gBAAgB;AAC5B,YAAY,OAAO;AACnB,YAAY,SAAS;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,YAAY,gBAAgB;AAC5B,YAAY,OAAO;AACnB,YAAY,SAAS;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,gBAAgB;AAC5B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,aAAa,SAAS;AACtB;AACA;AACA;AACA;AACA;AACA,iBAAiB,wBAAwB;AACzC;AACA,kBAAkB,mBAAmB;AACrC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe,yCAAyC;AACxD;AACA;AACA;AACA,oBAAoB;AACpB;AACA;AACA;AACA,iBAAiB,wBAAwB;AACzC;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,wCAAwC;AACrD,YAAY,oCAAoC;AAChD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,kBAAkB,kCAAkC;AACpD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,kBAAkB,kCAAkC;AACpD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,gBAAgB;AAC5B,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,kBAAkB,wBAAwB;AAC1C;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,YAAY,SAAS;AACrB,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;;;;;;;;ACzRkB;;AAElB;AACA;AACA;AACA,WAAW,QAAQ;AACnB;AACA;AACA;AACA,YAAY,QAAQ;AACpB,YAAY,UAAU;AACtB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,2BAA2B;AAC3B;AACA,cAAc;AACd,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kBAAkB,wBAAwB;AAC1C;AACA;AACA;AACA;;AAEA;AACA;AACA,kBAAkB,iBAAiB;AACnC;AACA;AACA;AACA;AACA,WAAW;AACX,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,iBAAiB,qBAAqB;AACtC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,oBAAoB,iBAAiB;AACrC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,YAAY,OAAO;AACnB,YAAY,OAAO;AACnB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA,qBAAqB,iBAAiB;AACtC;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA,iBAAiB,iBAAiB;AAClC;AACA,sBAAsB,oCAAoC;AAC1D;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,cAAc,MAAM;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,sBAAsB,6BAA6B;AACnD;;AAEA;AACA,kBAAkB,iBAAiB;AACnC;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,mCAAmC;AAChD,YAAY,oCAAoC;AAChD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,UAAU;AACV,KAAK;AACL;;AAEA;AACA;AACA,kBAAkB,iBAAiB;AACnC;AACA;AACA;AACA;AACA,oBAAoB,mBAAmB;AACvC;AACA;AACA;AACA;AACA,mBAAmB;AACnB,QAAQ;AACR;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;;;;;;;;;;;;;;;AC1SA;AAAA;AACA;AACA,WAAW,EAAE;AACb,YAAY,QAAQ;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW,EAAE;AACb,YAAY,QAAQ;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW,EAAE;AACb,YAAY,QAAQ;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW,EAAE;AACb,YAAY,QAAQ;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW,EAAE;AACb,YAAY,QAAQ;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW,EAAE;AACb,YAAY,QAAQ;AACpB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,EAAE;AACb,WAAW,MAAM;AACjB,YAAY,QAAQ;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,EAAE;AACb,WAAW,MAAM;AACjB,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,EAAE;AACb,WAAW,MAAM;AACjB,YAAY,SAAS;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB,cAAc;AAC9B;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;AC7GA;AAAA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA,4BAA4B;AAC5B;AACA;AACA;;AAEA;AACA;AACA;AACA,iCAAiC;AACjC,8BAA8B,qFAAqF;AACnH;AACA,YAAY,OAAO;AACnB,aAAa,UAAU;AACvB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA,+BAA+B;AAC/B,iCAAiC,gGAAgG;AACjI;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA;AACA;AACA,mCAAmC;AACnC;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA,oCAAoC;AACpC,kCAAkC,iGAAiG;AACnI;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,SAAS;AACpB;AACA;AACA;AACA,oCAAoC;AACpC;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kDAAkD,kCAAkC;AACpF;AACA,mCAAmC;AACnC,qCAAqC,oGAAoG;AACzI;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA;AACA;AACA,uCAAuC;AACvC;AACA;AACA;AACA;;AAEA;AACA,gEAAgE,gBAAgB;AAChF,YAAY,QAAQ;AACpB,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA,mCAAmC,yEAAyE;AAC5G;AACA;AACA;AACA;AACA,oDAAoD,2BAA2B;AAC/E,8EAA8E,8BAA8B;AAC5G;AACA,gCAAgC;AAChC,kCAAkC,iGAAiG;AACnI;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA;AACA;AACA,oCAAoC;AACpC;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,cAAc;AAC1B;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,WAAW;AACxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,aAAa,WAAW;AACxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA,kDAAkD,kCAAkC;AACpF;AACA,iCAAiC;AACjC,kCAAkC;AAClC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA;AACA;AACA,qCAAqC;AACrC;AACA;AACA;AACA;;AAEA;AACA,gEAAgE,gBAAgB;AAChF,YAAY,QAAQ;AACpB,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA,wCAAwC,kGAAkG;AAC1I;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA,6BAA6B;AAC7B;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,uIAAuI,oCAAoC;AAC3K;AACA;AACA,6BAA6B,iBAAiB,sBAAsB,2BAA2B,MAAM,8BAA8B;AACnI;AACA,uBAAuB;AACvB,kCAAkC,iGAAiG;AACnI;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA;AACA;AACA,oCAAoC;AACpC;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,WAAW;AACxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,aAAa,WAAW;AACxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,cAAc;AAC1B;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,WAAW;AACxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,aAAa,WAAW;AACxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA,6CAA6C,gBAAgB;AAC7D;AACA;AACA,uCAAuC;AACvC,qCAAqC,qGAAqG;AAC1I;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,sBAAsB;AACtB;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA;AACA;AACA,8CAA8C;AAC9C;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,kCAAkC;AAClC,0CAA0C,0GAA0G;AACpJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,sBAAsB;AACtB;AACA;;AAEA;AACA;AACA,aAAa,WAAW;AACxB;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uIAAuI,mCAAmC;AAC1K;AACA;AACA;AACA;AACA,kCAAkC;AAClC,iCAAiC,gGAAgG;AACjI;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,sBAAsB;AACtB;AACA;;AAEA;AACA;AACA,aAAa,WAAW;AACxB;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,aAAa,WAAW;AACxB;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,aAAa,WAAW;AACxB;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,aAAa,WAAW;AACxB;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,UAAU;AACvB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI,gBAAgB,GAAG,iBAAiB,GAAG,iBAAiB,GAAG,oBAAoB;AACnF,IAAI,iBAAiB,GAAG,oBAAoB,GAAG,kBAAkB,IAAI,gBAAgB;AACrF,IAAI,yBAAyB,GAAG;AAChC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB;AAChB;AACA;;AAEA;AACA;AACA,YAAY,QAAQ;AACpB,aAAa;AACb;AACA;AACA;AACA;AACA;;AAEA;AACA,qBAAqB,+CAA+C;AACpE;AACA,wCAAwC;AACxC,6BAA6B,kGAAkG;AAC/H;AACA,YAAY,OAAO;AACnB;AACA,YAAY,OAAO;AACnB;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;;;;;;;AC1tBA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,8DAA8D;AAC9D;AACA;AACA;;AAEA;AACA;AACA,iBAAiB,mBAAmB;AACpC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,IAAI;AACJ;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;;AAEA,4CAA4C;AAC5C;AACA;AACA;AACA,0BAA0B,kCAAkC;AAC5D;AACA;;AAEA,kCAAkC;AAClC;AACA;AACA;;AAEA;AACA,wBAAwB,kCAAkC;AAC1D;AACA,kBAAkB,8BAA8B;AAChD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yCAAyC;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,iBAAiB,mBAAmB;AACpC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,6BAA6B,QAAQ;AACrC;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,YAAY,OAAO;AACnB,cAAc,OAAO;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAAA;AAAA;;;;;;;;;;;;ACnJsB;AACA;AACJ;AAClB;;AAEA;AACA;AACA;AACA,YAAY,SAAS;AACrB,YAAY,OAAO;AACnB,YAAY,aAAa;AACzB;AACA,WAAW,oBAAoB;AAC/B;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,kBAAkB,mBAAmB;AACrC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,4BAA4B,kDAAkD;AAC9E;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,iBAAiB,uBAAuB;AACxC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,4BAA4B,kDAAkD;AAC9E;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,4BAA4B,kDAAkD;AAC9E;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAAA;AAAA;;;;;;;;;;AC5GA;AAAA;AACA;AACA;AACA;AACwE;AACtD;;AAElB;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,EAAE;;AAEF;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,gBAAgB,mBAAmB;AACnC;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;;AAEA;AACA;;AAEQ;;;;;;;;;;;;;;;;;;;;;AC9Ue;AACL;AACG;AACV;AACW;AACP;;AASf;;;;;;;;;;;ACde;AACO;AACD;;AAErB;AACA;AACA;AACA,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,2BAA2B,kCAAkC;AAC7D;AACA;AACA;AACA,QAAQ;AACR;AACA,QAAQ;AACR;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2BAA2B,kCAAkC;AAC7D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,wBAAwB;AAC3C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,cAAc;AACjC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,cAAc;AACjC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,iBAAiB;AACpC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,oBAAoB,oBAAoB;AACxC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,oBAAoB,iBAAiB;AACrC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,oBAAoB,kBAAkB;AACtC;AACA;AACA,KAAK;AACL;AACA,oBAAoB,kBAAkB;AACtC;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,iBAAiB,mBAAmB;AACpC;AACA;AACA;AACA;AACA;;AAEA;AACA,yBAAyB,kCAAkC;AAC3D;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,iBAAiB,mBAAmB;AACpC;AACA;AACA,yBAAyB,kCAAkC;AAC3D;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,eAAe;AAC5B;AACA;;AAEA;AACA,aAAa,eAAe;AAC5B;AACA,cAAc,eAAe;AAC7B,gCAAgC;AAChC;AACA,KAAK;AACL;AACA;AACA,oBAAoB;;AAEpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,YAAY,OAAO;AACnB,cAAc,MAAM;AACpB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,YAAY,8CAA8C;AAC1D;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yBAAyB,yBAAyB;AAClD;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,mBAAmB,iBAAiB;AACpC;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,cAAc,MAAM;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,uBAAuB,kBAAkB;AACzC;AACA;AACA;;AAEA;AACA;AACA,GAAG;AACH;AACA,kBAAkB,mBAAmB;AACrC;AACA;AACA,GAAG;AACH;AACA,kBAAkB,gBAAgB;AAClC;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;;;;;;;;AClbA;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,kBAAkB,aAAa;AAC/B;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,iBAAiB,YAAY;AAC7B;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,iBAAiB,YAAY;AAC7B;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,wBAAwB,eAAe;AACvC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,uCAAuC,SAAS;AAChD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AAAA;AAAA","file":"loki.FullTextSearch.js","sourcesContent":["(function webpackUniversalModuleDefinition(root, factory) {\n\tif(typeof exports === 'object' && typeof module === 'object')\n\t\tmodule.exports = factory();\n\telse if(typeof define === 'function' && define.amd)\n\t\tdefine(\"FullTextSearch\", [], factory);\n\telse if(typeof exports === 'object')\n\t\texports[\"FullTextSearch\"] = factory();\n\telse\n\t\troot[\"Loki\"] = root[\"Loki\"] || {}, root[\"Loki\"][\"FullTextSearch\"] = factory();\n})(this, function() {\nreturn \n\n\n// WEBPACK FOOTER //\n// webpack/universalModuleDefinition"," \t// The module cache\n \tvar installedModules = {};\n\n \t// The require function\n \tfunction __webpack_require__(moduleId) {\n\n \t\t// Check if module is in cache\n \t\tif(installedModules[moduleId]) {\n \t\t\treturn installedModules[moduleId].exports;\n \t\t}\n \t\t// Create a new module (and put it into the cache)\n \t\tvar module = installedModules[moduleId] = {\n \t\t\ti: moduleId,\n \t\t\tl: false,\n \t\t\texports: {}\n \t\t};\n\n \t\t// Execute the module function\n \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n \t\t// Flag the module as loaded\n \t\tmodule.l = true;\n\n \t\t// Return the exports of the module\n \t\treturn module.exports;\n \t}\n\n\n \t// expose the modules object (__webpack_modules__)\n \t__webpack_require__.m = modules;\n\n \t// expose the module cache\n \t__webpack_require__.c = installedModules;\n\n \t// identity function for calling harmony imports with the correct context\n \t__webpack_require__.i = function(value) { return value; };\n\n \t// define getter function for harmony exports\n \t__webpack_require__.d = function(exports, name, getter) {\n \t\tif(!__webpack_require__.o(exports, name)) {\n \t\t\tObject.defineProperty(exports, name, {\n \t\t\t\tconfigurable: false,\n \t\t\t\tenumerable: true,\n \t\t\t\tget: getter\n \t\t\t});\n \t\t}\n \t};\n\n \t// getDefaultExport function for compatibility with non-harmony modules\n \t__webpack_require__.n = function(module) {\n \t\tvar getter = module && module.__esModule ?\n \t\t\tfunction getDefault() { return module['default']; } :\n \t\t\tfunction getModuleExports() { return module; };\n \t\t__webpack_require__.d(getter, 'a', getter);\n \t\treturn getter;\n \t};\n\n \t// Object.prototype.hasOwnProperty.call\n \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n \t// __webpack_public_path__\n \t__webpack_require__.p = \"\";\n\n \t// Load entry module and return exports\n \treturn __webpack_require__(__webpack_require__.s = 7);\n\n\n\n// WEBPACK FOOTER //\n// webpack/bootstrap cd2043baf7046fa7110f","import * as Utils from './utils.js';\n\n/**\n * Splits a string at non-alphanumeric characters into lower case tokens.\n * @param {string} str - the string\n * @returns {string[]} - the tokens\n * @private\n */\nfunction defaultSplitter(str) {\n\tlet tokens = str.split(/[^\\w]+/);\n\tfor (let i = 0; i < tokens.length; i++) {\n\t\ttokens[i] = tokens[i].toLowerCase();\n\t}\n\treturn tokens;\n}\n\n/**\n * The tokenizer is used to prepare the string content of a document field for the inverted index.\n * Firstly the string gets split into tokens.\n * After that the tokens will be trimmed/stemmed with defined functions from the queue.\n *\n * * To change the splitter function, use {@link Tokenizer#setSplitter}.\n * * To add functions to the queue, use {@link Tokenizer#add}, {@link Tokenizer#addBefore} and\n *   {@link Tokenizer#addAfter}.\n * * To remove a function from the queue, use {@link Tokenizer#remove}.\n * * To reset the tokenizer, use {@link Tokenizer#reset}.\n */\nexport class Tokenizer {\n\t/**\n\t * Initializes the tokenizer with a splitter, which splits a string at non-alphanumeric characters.\n\t * The queue is empty.\n\t */\n\tconstructor() {\n\t\tthis._splitter = null;\n\t\tthis._queue = [];\n\t\tthis._symbol = Symbol('label');\n\t\tthis.reset();\n\t}\n\n\t/**\n\t * Sets a function with defined label as the splitter function.\n\t * The function must take a string as argument and return an array of tokens.\n\t *\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t */\n\tsetSplitter(label, func) {\n\t\tlabel = Utils.asString(label);\n\t\tif (!Utils.isFunction(func)) {\n\t\t\tthrow TypeError(\"Splitter must be a function.\");\n\t\t}\n\t\tif (label === \"\") {\n\t\t\tthrow Error(\"Label cannot be empty.\");\n\t\t}\n\t\tfunc[this._symbol] = label;\n\t\tthis._splitter = func;\n\t}\n\n\t/**\n\t * Gets the splitter.\n\t * @return {Array.<string, function>} - tuple with label and function\n\t */\n\tgetSplitter() {\n\t\treturn [this._splitter[this._symbol], this._splitter];\n\t}\n\n\t/**\n\t * Resets the splitter to default.\n\t */\n\tresetSplitter() {\n\t\tthis._splitter = defaultSplitter;\n\t}\n\n\t/**\n\t * Checks if a function is inside the queue.\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @returns {boolean} true if exists, otherwise false\n\t */\n\thas(labelFunc) {\n\t\treturn this._getPosition(labelFunc) !== -1;\n\t}\n\n\t/**\n\t * Gets a function from the queue.\n\t * Only the first found function gets returned if a label or a function is multiple used.\n\t *\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @return {Array.<string, function>} - tuple with label and function\n\t */\n\tget(labelFunc) {\n\t\tlet pos = this._getPosition(labelFunc);\n\t\tif (pos === -1) {\n\t\t\tthrow Error('Cannot find existing function.');\n\t\t}\n\t\treturn [this._queue[pos][this._symbol], this._queue[pos]];\n\t}\n\n\t/**\n\t * Adds a function with defined label to the end of the queue.\n\t * The function must take a token string as argument and return a token.\n\t *\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t */\n\tadd(label, func) {\n\t\tthis._addFunction(label, func, this._queue.length);\n\t}\n\n\t/**\n\t * Adds a function with defined label before an existing function to the queue.\n\t * The function must take a token string as argument and return a token.\n\t *\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t */\n\taddBefore(labelFunc, label, func) {\n\t\tlet pos = this._getPosition(labelFunc);\n\t\tif (pos === -1) {\n\t\t\tthrow Error('Cannot find existing function.');\n\t\t}\n\t\tthis._addFunction(label, func, pos);\n\t}\n\n\t/**\n\t * Adds a function with defined label after an existing function to the queue.\n\t * The function must take a token string as argument and return a token.\n\t *\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t */\n\taddAfter(labelFunc, label, func) {\n\t\tlet pos = this._getPosition(labelFunc);\n\t\tif (pos === -1) {\n\t\t\tthrow Error('Cannot find existing function.');\n\t\t}\n\t\tthis._addFunction(label, func, pos + 1);\n\t}\n\n\t/**\n\t * Removes a function from the queue.\n\t * @param {string|function} labelFunc - an existing label or function\n\t */\n\tremove(labelFunc) {\n\t\tlet pos = this._getPosition(labelFunc);\n\t\tif (pos === -1) {\n\t\t\tthrow Error('Cannot find existing function.');\n\t\t}\n\t\tthis._queue.splice(pos, 1);\n\t}\n\n\t/**\n\t * Resets the splitter and tokenize queue to default.\n\t */\n\treset() {\n\t\tthis._splitter = defaultSplitter;\n\t\tthis._queue = [];\n\t}\n\n\t/**\n\t * Tokenizes a string into tokens.\n\t * @param {string} str - the string\n\t * @return {string[]} the tokens\n\t * @private\n\t */\n\ttokenize(str) {\n\t\tlet tokens = this._splitter(str);\n\t\t// Apply each token over the queue functions.\n\t\tfor (let i = 0; i < this._queue.length; i++) {\n\t\t\tlet newTokens = [];\n\t\t\tfor (let j = 0; j < tokens.length; j++) {\n\t\t\t\tlet token = this._queue[i](tokens[j]);\n\t\t\t\tif (token) {\n\t\t\t\t\tnewTokens.push(token);\n\t\t\t\t}\n\t\t\t}\n\t\t\ttokens = newTokens;\n\t\t}\n\t\treturn tokens;\n\t}\n\n\t/**\n\t * Serializes the tokenizer by returning the labels of the used functions.\n\t * @returns {{splitter: string?, tokenizers: string[]}} - the serialization\n\t * @private\n\t */\n\ttoJSON() {\n\t\tlet serialized = {tokenizers: []};\n\t\tif (this._splitter !== defaultSplitter) {\n\t\t\tserialized.splitter = this._splitter[this._symbol];\n\t\t}\n\t\tfor (let i = 0; i < this._queue.length; i++) {\n\t\t\tserialized.tokenizers.push(this._queue[i][this._symbol]);\n\t\t}\n\t\treturn serialized;\n\t}\n\n\t/**\n\t * Deserializes the tokenizer by reassign the correct function to each label.\n\t * @param {{splitter: string, tokenizers: string[]}} serialized - the serialized labels\n\t * @param {Object.<string, function>|Tokenizer} funcTok - the depending functions with labels\n\t * \tor an equivalent tokenizer\n\t * @private\n\t */\n\tstatic fromJSON(serialized, funcTok) {\n\t\tlet tokenizer = new Tokenizer();\n\n\t\tif (funcTok !== undefined && funcTok instanceof Tokenizer) {\n\t\t\tif (serialized.splitter !== undefined) {\n\t\t\t\tlet splitter = funcTok.getSplitter();\n\t\t\t\tif (serialized.splitter !== splitter[0]) {\n\t\t\t\t\tthrow Error(\"Splitter function not found.\");\n\t\t\t\t}\n\t\t\t\ttokenizer.setSplitter(splitter[0], splitter[1]);\n\t\t\t}\n\n\t\t\tfor (let i = 0; i < serialized.tokenizers.length; i++) {\n\t\t\t\tif (!funcTok.has(serialized.tokenizers[i])) {\n\t\t\t\t\tthrow Error(\"Tokenizer function not found.\");\n\t\t\t\t}\n\t\t\t\tlet labelFunc = funcTok.get(serialized.tokenizers[i]);\n\t\t\t\ttokenizer.add(labelFunc[0], labelFunc[1]);\n\t\t\t}\n\t\t} else {\n\t\t\tif (serialized.splitter !== undefined) {\n\t\t\t\tif (funcTok.splitters[serialized.splitter] === undefined) {\n\t\t\t\t\tthrow Error(\"Splitter function not found.\");\n\t\t\t\t}\n\t\t\t\ttokenizer.setSplitter(serialized.splitter, funcTok.splitters[serialized.splitter]);\n\t\t\t}\n\t\t\tfor (let i = 0; i < serialized.tokenizers.length; i++) {\n\t\t\t\tif (funcTok.tokenizers[serialized.tokenizers[i]] === undefined) {\n\t\t\t\t\tthrow Error(\"Tokenizer function not found.\");\n\t\t\t\t}\n\t\t\t\ttokenizer.add(serialized.tokenizers[i], funcTok.tokenizers[serialized.tokenizers[i]]);\n\t\t\t}\n\t\t}\n\t\treturn tokenizer;\n\t}\n\n\t/**\n\t * Returns the position of a function inside the queue.\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @return {number} the position\n\t * @private\n\t */\n\t_getPosition(labelFunc) {\n\t\tif (Utils.isFunction(labelFunc)) {\n\t\t\treturn this._queue.indexOf(labelFunc);\n\t\t} else if (Utils.isConvertibleToString(labelFunc)) {\n\t\t\tlabelFunc = String(labelFunc);\n\t\t\tfor (let i = 0; i < this._queue.length; i++) {\n\t\t\t\tif (this._queue[i][this._symbol] === labelFunc) {\n\t\t\t\t\treturn i;\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tthrow TypeError(\"Type of labelFunc must be string or function.\");\n\t\t}\n\t\treturn -1;\n\t}\n\n\t/**\n\t * Adds a function with defined label at a specific position to the queue.\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t * @param {number} pos - the position\n\t * @private\n\t */\n\t_addFunction(label, func, pos) {\n\t\tlabel = Utils.asString(label);\n\t\tif (!Utils.isFunction(func)) {\n\t\t\tthrow TypeError(\"Type of func must be function.\");\n\t\t}\n\t\tif (label === \"\") {\n\t\t\tthrow Error(\"Label cannot be empty.\");\n\t\t}\n\t\tfunc[this._symbol] = label;\n\t\tthis._queue.splice(pos, 0, func);\n\t}\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/inverted_index/tokenizer.js\n// module id = 0\n// module chunks = 0","import {Tokenizer} from './tokenizer';\n\n/**\n * Inverted index class handles featured text search for specific document fields.\n * @constructor InvertedIndex\n * @param {boolean} [options.store=true] - inverted index will be stored at serialization rather than rebuilt on load.\n */\nexport class InvertedIndex {\n\t/**\n\t * @param {boolean} store\n\t * @param {Tokenizer} tokenizer\n\t */\n\tconstructor(store = true, tokenizer = new Tokenizer) {\n\t\tthis._store = store;\n\t\tthis._tokenizer = tokenizer;\n\t\tthis._docCount = 0;\n\t\tthis._docStore = {};\n\t\tthis._totalFieldLength = 0;\n\t\tthis._root = {};\n\t}\n\n\tget store() {\n\t\treturn this._store;\n\t}\n\n\tget tokenizer() {\n\t\treturn this._tokenizer;\n\t}\n\n\tget documentCount() {\n\t\treturn this._docCount;\n\t}\n\n\tget documentStore() {\n\t\treturn this._docStore;\n\t}\n\n\tget totalFieldLength() {\n\t\treturn this._totalFieldLength;\n\t}\n\n\tget root() {\n\t\treturn this._root;\n\t}\n\n\t/**\n\t * Adds defined fields of a document to the inverted index.\n\t * @param {object} field - the field to add\n\t * @param {number} docId - the doc id of the field\n\t */\n\tinsert(field, docId) {\n\t\tif (this._docStore[docId] !== undefined) {\n\t\t\tthrow Error('Field already added.');\n\t\t}\n\n\t\tthis._docCount += 1;\n\t\tthis._docStore[docId] = {};\n\n\t\t// Tokenize document field.\n\t\tlet fieldTokens = this._tokenizer.tokenize(field);\n\t\tthis._totalFieldLength += fieldTokens.length;\n\n\t\tlet termRefs = [];\n\t\tthis._docStore[docId] = {fieldLength: fieldTokens.length};\n\t\tObject.defineProperties(this._docStore[docId], {\n\t\t\ttermRefs: {enumerable: false, configurable: true, writable: true, value: termRefs}\n\t\t});\n\n\t\t// Iterate over all unique field terms.\n\t\tfor (let term of new Set(fieldTokens)) {\n\t\t\tif (term === '') {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t// Calculate term frequency.\n\t\t\tlet tf = 0;\n\t\t\tfor (let j = 0; j < fieldTokens.length; j++) {\n\t\t\t\tif (fieldTokens[j] === term) {\n\t\t\t\t\ttf++;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Add term to index tree.\n\t\t\tlet branch = this._root;\n\t\t\tfor (let i = 0; i < term.length; i++) {\n\t\t\t\tlet c = term[i];\n\t\t\t\tif (branch[c] === undefined) {\n\t\t\t\t\tlet child = {};\n\t\t\t\t\tObject.defineProperties(child, {\n\t\t\t\t\t\tpa: {enumerable: false, configurable: true, writable: true, value: branch}\n\t\t\t\t\t});\n\t\t\t\t\tbranch[c] = child;\n\t\t\t\t}\n\t\t\t\tbranch = branch[c];\n\t\t\t}\n\t\t\t// Add term info to index leaf.\n\t\t\tif (branch.dc === undefined) {\n\t\t\t\tbranch.dc = {};\n\t\t\t\tbranch.df = 0;\n\t\t\t}\n\t\t\tbranch.dc[docId] = tf;\n\t\t\tbranch.df += 1;\n\n\t\t\t// Store index leaf for deletion.\n\t\t\ttermRefs.push(branch);\n\t\t}\n\t}\n\n\t/**\n\t * Removes all relevant terms of a document from the inverted index.\n\t * @param {number} docId - the document.\n\t */\n\tremove(docId) {\n\t\tif (this._docStore[docId] === undefined) {\n\t\t\treturn;\n\t\t}\n\t\tlet docStore = this._docStore[docId];\n\t\t// Remove document.\n\t\tdelete this._docStore[docId];\n\t\tthis._docCount -= 1;\n\n\t\t// Reduce total field length.\n\t\tthis._totalFieldLength -= docStore.fieldLength;\n\n\t\t// Iterate over all term references.\n\t\t// Remove docId from docs and decrement document frequency.\n\t\tlet termRefs = docStore.termRefs;\n\t\tfor (let j = 0; j < termRefs.length; j++) {\n\t\t\tlet index = termRefs[j];\n\t\t\tindex.df -= 1;\n\t\t\tdelete index.dc[docId];\n\n\t\t\t// Check if no document is left for current tree.\n\t\t\tif (index.df === 0) {\n\t\t\t\t// Delete unused meta data of branch.\n\t\t\t\tdelete index.df;\n\t\t\t\tdelete index.dc;\n\n\t\t\t\t// Check for sub branches.\n\t\t\t\tif (Object.keys(index).length !== 0) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\t// Delete term branch if not used anymore.\n\t\t\t\tlet keys = [];\n\t\t\t\tdo {\n\t\t\t\t\t// Go tree upwards.\n\t\t\t\t\tlet parent = index.pa;\n\t\t\t\t\t// Delete parent reference for preventing memory leak (cycle reference)\n\t\t\t\t\tdelete index.pa;\n\n\t\t\t\t\t// Iterate over all children.\n\t\t\t\t\tkeys = Object.keys(parent);\n\t\t\t\t\tfor (let k = 0; k < keys.length; k++) {\n\t\t\t\t\t\tlet key = keys[k];\n\t\t\t\t\t\tif (key.length !== 1) {\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Remove previous child form parent.\n\t\t\t\t\t\tif (parent[key] === index) {\n\t\t\t\t\t\t\tdelete parent[key];\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tindex = parent;\n\t\t\t\t} while (index.pa !== undefined && keys.length === 1);\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Gets the term index of a term.\n\t * @param {string} term - the term.\n\t * @param {object} root - the term index to start from\n\t * @param {number} start - the position of the term string to start from\n\t * @return {object} - The term index or null if the term is not in the term tree.\n\t */\n\tstatic getTermIndex(term, root, start = 0) {\n\t\tif (start >= term.length) {\n\t\t\treturn null;\n\t\t}\n\t\tfor (let i = start; i < term.length; i++) {\n\t\t\tif (root[term[i]] === undefined) {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t\troot = root[term[i]];\n\t\t}\n\t\treturn root;\n\t}\n\n\t/**\n\t * Extends a term index for the one branch.\n\t * @param {object} root - the term index to start from\n\t * @return {Array} - array with term indices and extension\n\t */\n\tstatic getNextTermIndex(root) {\n\t\tlet termIndices = [];\n\t\tlet keys = Object.keys(root);\n\t\tfor (let i = 0; i < keys.length; i++) {\n\t\t\tif (keys[i].length === 1) {\n\t\t\t\ttermIndices.push({index: root[keys[i]], term: keys[i]});\n\t\t\t}\n\t\t}\n\t\treturn termIndices;\n\t}\n\n\t/**\n\t * Extends a term index to all available term leafs.\n\t * @param {object} root - the term index to start from\n\t * @returns {Array} - Array with term indices and extension\n\t */\n\tstatic extendTermIndex(root) {\n\t\tlet termIndices = [];\n\t\tlet stack = [root];\n\t\tlet treeStack = [''];\n\t\tdo {\n\t\t\tlet root = stack.pop();\n\t\t\tlet treeTermn = treeStack.pop();\n\n\t\t\tif (root.df !== undefined) {\n\t\t\t\ttermIndices.push({index: root, term: treeTermn});\n\t\t\t}\n\n\t\t\tlet keys = Object.keys(root);\n\t\t\tfor (let i = 0; i < keys.length; i++) {\n\t\t\t\tif (keys[i].length === 1) {\n\t\t\t\t\tstack.push(root[keys[i]]);\n\t\t\t\t\ttreeStack.push(treeTermn + keys[i]);\n\t\t\t\t}\n\t\t\t}\n\t\t} while (stack.length !== 0);\n\n\t\treturn termIndices;\n\t}\n\n\t/**\n\t * Serialize the inverted index.\n\t * @returns {{docStore: *, _fields: *, index: *}}\n\t */\n\ttoJSON() {\n\t\tif (this._store) {\n\t\t\treturn this;\n\t\t} else {\n\t\t\treturn {\n\t\t\t\t_tokenizer: this._tokenizer,\n\t\t\t\t_store: false\n\t\t\t};\n\t\t}\n\t}\n\n\t/**\n\t * Deserialize the inverted index.\n\t * @param {{docStore: *, _fields: *, index: *}} serialized - The serialized inverted index.\n\t * @param {Object.<string, function>|Tokenizer} funcTok[undefined] - the depending functions with labels\n\t *  or an equivalent tokenizer\n\t */\n\tloadJSON(serialized, funcTok = undefined) {\n\t\tlet dbObject = serialized;\n\n\t\tthis._tokenizer = Tokenizer.fromJSON(dbObject._tokenizer, funcTok);\n\t\tthis._docCount = dbObject._docCount;\n\t\tthis._docStore = dbObject._docStore;\n\t\tthis._totalFieldLength = dbObject._totalFieldLength;\n\t\tthis._root = dbObject._root;\n\n\t\tlet regenerate = (index, parent) => {\n\t\t\t// Set parent.\n\t\t\tif (parent !== null) {\n\t\t\t\tObject.defineProperties(index, {\n\t\t\t\t\tpa: {enumerable: false, configurable: true, writable: false, value: parent}\n\t\t\t\t});\n\t\t\t}\n\n\t\t\t// Iterate over all keys.\n\t\t\tlet keys = Object.keys(index);\n\t\t\tfor (let i = 0; i < keys.length; i++) {\n\t\t\t\t// Found term, save in document store.\n\t\t\t\tif (keys[i] === 'dc') {\n\t\t\t\t\t// Get documents of term.\n\t\t\t\t\tlet docIds = Object.keys(index.dc);\n\t\t\t\t\tfor (let j = 0; j < docIds.length; j++) {\n\t\t\t\t\t\t// Get document store at specific document/field.\n\t\t\t\t\t\tlet ref = this._docStore[docIds[j]];\n\t\t\t\t\t\tif (ref.termRefs === undefined) {\n\t\t\t\t\t\t\tObject.defineProperties(ref, {\n\t\t\t\t\t\t\t\ttermRefs: {enumerable: false, configurable: true, writable: true, value: []}\n\t\t\t\t\t\t\t});\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Set reference to term index.\n\t\t\t\t\t\tref.termRefs.push(index);\n\t\t\t\t\t}\n\t\t\t\t} else if (keys[i].length === 1) {\n\t\t\t\t\t// Iterate over subtree.\n\t\t\t\t\tregenerate(index[keys[i]], index);\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\tregenerate(this._root, null);\n\t}\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/inverted_index/inverted_index.js\n// module id = 1\n// module chunks = 0","/**\n * Checks if the variable is a function.\n * @param {*} x - the variable\n * @return {boolean} true if function, otherwise false\n * @protected\n */\nexport function isFunction(x) {\n\treturn Object.prototype.toString.call(x) === \"[object Function]\";\n}\n\n/**\n * Checks if the variable is an object.\n * @param {*} x - the variable\n * @return {boolean} true if object, otherwise false\n * @protected\n */\nexport function isObject(x) {\n\treturn Object.prototype.toString.call(x) === \"[object Object]\";\n}\n\n/**\n * Checks if the variable is a number.\n * @param {*} x - the variable\n * @return {boolean} true if number, otherwise false\n * @protected\n */\nexport function isNumber(x) {\n\treturn Object.prototype.toString.call(x) === \"[object Number]\";\n}\n\n/**\n * Checks if the variable is a boolean.\n * @param {*} x - the variable\n * @return {boolean} true if boolean, otherwise false\n * @protected\n */\nexport function isBoolean(x) {\n\treturn Object.prototype.toString.call(x) === \"[object Boolean]\";\n}\n\n/**\n * Checks if the variable is a string.\n * @param {*} x - the variable\n * @return {boolean} true if string, otherwise false\n * @protected\n */\nexport function isString(x) {\n\treturn Object.prototype.toString.call(x) === \"[object String]\";\n}\n\n/**\n * Checks if the variable is convertible to a string.\n * @param {*} x - the variable\n * @return {boolean} true if convertible, otherwise false\n */\nexport function isConvertibleToString(x) {\n\treturn isString(x) || isNumber(x) || isObject(x) && Object.prototype.toString !== x.toString && isString(x.toString());\n}\n\n/**\n * Converts a variable to a boolean (from boolean or number).\n * Throws an error if not possible.\n * @param {*} x - the variable\n * @param {error} [error=TypeError] - the error to throw\n * @return {boolean} the converted boolean\n * @protected\n */\nexport function asBoolean(x, error = TypeError(\"Value is not convertible to boolean\")) {\n\tif (isBoolean(x) || isNumber(x)) {\n\t\treturn Boolean(x);\n\t}\n\tthrow error;\n}\n\n/**\n * Converts a variable to a string (from string, number or obj.toString).\n * Throws an error if not possible.\n * @param {*} x - the variable\n * @param {error} [error=TypeError] - the error to throw\n * @return {string} the converted string\n * @protected\n */\nexport function asString(x, error = TypeError(\"Value is not convertible to string.\")) {\n\tif (isConvertibleToString(x)) {\n\t\treturn String(x);\n\t}\n\tthrow error;\n}\n\n/**\n * Converts a variable to a array of string (from an array of string, number or obj.toString).\n * Throws an error if not possible.\n * @param {*} x - the variable\n * @param {error} [error=TypeError] - the error to throw\n * @return {string[]} the converted array of string\n * @protected\n */\nexport function asArrayOfString(x, error = TypeError(\"Value is not convertible to an array of strings.\")) {\n\tif (!Array.isArray(x)) {\n\t\tthrow error;\n\t}\n\tlet array = [];\n\tfor (let i = 0; i < x.length; i++) {\n\t\tif (!isConvertibleToString(x[i])) {\n\t\t\tthrow error;\n\t\t}\n\t\tarray.push(String(x[i]));\n\t}\n\treturn array;\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/inverted_index/utils.js\n// module id = 2\n// module chunks = 0","/**\n * Query builder\n */\nimport * as Utils from './utils.js';\n\n/**\n * The base query class to enable boost to a query type.\n *\n * @param {string} type - the type name of the query\n */\nexport class BaseQuery {\n\tconstructor(type, data = {}) {\n\t\tthis._data = data;\n\t\tthis._data.type = Utils.asString(type);\n\t}\n\n\t/**\n\t * Boosts the query result.\n\t *\n\t * See also [Lucene#BoostQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/BoostQuery.html}\n\t * and [Elasticsearch#boost]{@link https://www.elastic.co/guide/en/elasticsearch/reference/5.2/mapping-boost.html}.\n\t *\n\t * @param {number} value - the positive boost\n\t * @return {BaseQuery} object itself for cascading\n\t */\n\tboost(value) {\n\t\tif (!Utils.isNumber(value) || value < 0) {\n\t\t\tthrow TypeError(\"Boost must be a positive number.\");\n\t\t}\n\t\tthis._data.boost = value;\n\t\treturn this;\n\t}\n\n\t/**\n\t * Build the final query.\n\t * @return {Object} - the final query\n\t */\n\tbuild() {\n\t\treturn this._data;\n\t}\n}\n\n/**\n * A query which finds documents where a document field contains a term.\n *\n * See also [Lucene#TermQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/TermQuery.html}\n * and [Elasticsearch#TermQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-term-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .term(\"name\", \"infinity\"])\n * .build();\n * // The resulting documents:\n * // contains the term infinity\n *\n * @param {string} field - the field name of the document\n * @param {string} term - the term\n * @extends BaseQuery\n */\nexport class TermQuery extends BaseQuery {\n\tconstructor(field, term, data = {}) {\n\t\tsuper(\"term\", data);\n\t\tthis._data.field = Utils.asString(field);\n\t\tthis._data.value = Utils.asString(term);\n\t}\n}\n\n/**\n * A query which finds documents where a document field contains any of the terms.\n *\n * See also [Lucene#TermRangeQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/TermRangeQuery.html}\n * and [Elasticsearch#TermsQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-terms-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .terms(\"quotes\", [\"infinity\", \"atom\", \"energy\"])\n * .build();\n * // The resulting documents:\n * // contains the terms infinity, atom or energy\n *\n * @param {string} field - the field name of the document\n * @param {string[]} terms - the terms\n * @extends BaseQuery\n */\nexport class TermsQuery extends BaseQuery {\n\tconstructor(field, terms, data = {}) {\n\t\tsuper(\"terms\", data);\n\t\tthis._data.field = Utils.asString(field);\n\t\tthis._data.value = Utils.asArrayOfString(terms);\n\t}\n}\n\n/**\n * A query which finds documents where the wildcard term can be applied at an existing document field term.\n *\n * Wildcard | Description\n * -------- | ------------\n * ? (question mark) | Skips a single character.\n *\n * To escape a wildcard character, use _\\_ (backslash), e.g. \\?.\n *\n * * To enable scoring for wildcard queries, use {@link WildcardQuery#enableScoring}.\n *\n * See also [Lucene#WildcardQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/WildcardQuery.html}\n * and [Elasticsearch#WildcardQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-wildcard-query.html}.\n *\n * _TODO: Implement wildcard * (asterisk) to skip zero or more characters._\n * @todo Implement wildcard * (asterisk) to skip zero or more characters.\n *\n * @example\n * new QueryBuilder()\n *   .wildcard(\"question\", \"e?nste?n\\?\")\n * .build();\n * // The resulting documents:\n * // contains the wildcard surname e?nste?n\\? (like Einstein? or Eynsteyn? but not Einsteine or Ensten?)\n *\n * @param {string} field - the field name of the document\n * @param {string} wildcard - the wildcard term\n * @extends BaseQuery\n */\nexport class WildcardQuery extends BaseQuery {\n\tconstructor(field, wildcard, data = {}) {\n\t\tsuper(\"wildcard\", data);\n\t\tthis._data.field = Utils.asString(field);\n\t\tthis._data.value = Utils.asString(wildcard);\n\t}\n\n\t/**\n\t * This flag enables scoring for wildcard results, similar to {@link TermQuery}.\n\t * @param {boolean} enable - flag to enable or disable scoring\n\t * @return {WildcardQuery}\n\t */\n\tenableScoring(enable) {\n\t\tthis._data.enable_scoring = Utils.asBoolean(enable);\n\t\treturn this;\n\t}\n}\n\n/**\n * A query which finds documents where the fuzzy term can be transformed into an existing document field term within a\n * given edit distance\n * ([Damerauâ€“Levenshtein distance]{@link https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance}).\n *\n * The edit distance is the minimum number of an insertion, deletion or substitution of a single character\n * or a transposition of two adjacent characters.\n *\n * * To set the maximal allowed edit distance, use {@link FuzzyQuery#fuzziness} (default is AUTO).\n * * To set the initial word length, which should ignored for fuzziness, use {@link FuzzyQuery#prefixLength}.\n *\n * See also [Lucene#FuzzyQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/FuzzyQuery.html}\n * and [Elasticsearch#FuzzyQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-fuzzy-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .fuzzy(\"surname\", \"einsten\")\n *     .fuzziness(3)\n *     .prefixLength(3)\n * .build();\n * // The resulting documents:\n * // contains the fuzzy surname einstn (like Einstein or Einst but not Eisstein or Insten)\n *\n * @param {string} field - the field name of the document\n * @param {string} fuzzy - the fuzzy term\n * @extends BaseQuery\n */\nexport class FuzzyQuery extends BaseQuery {\n\tconstructor(field, fuzzy, data = {}) {\n\t\tsuper(\"fuzzy\", data);\n\t\tthis._data.field = Utils.asString(field);\n\t\tthis._data.value = Utils.asString(fuzzy);\n\t}\n\n\t/**\n\t * Sets the maximal allowed fuzziness.\n\t * @param {number|string} fuzziness - the edit distance as number or AUTO\n\t *\n\t * AUTO generates an edit distance based on the length of the term:\n\t * * 0..2 -> must match exactly\n\t * * 3..5 -> one edit allowed\n\t * * >5 two edits allowed\n\t *\n\t * @return {FuzzyQuery} - object itself for cascading\n\t */\n\tfuzziness(fuzziness) {\n\t\tif ((!Utils.isString(fuzziness) || fuzziness !== \"AUTO\") && (!Utils.isNumber(fuzziness) || fuzziness < 0)) {\n\t\t\tthrow TypeError(\"Fuzziness must be a positive number or AUTO.\");\n\t\t}\n\t\tthis._data.fuzziness = fuzziness;\n\t\treturn this;\n\t}\n\n\t/**\n\t * Sets the initial word length.\n\t * @param {number} prefixLength - the positive prefix length\n\t * @return {FuzzyQuery}  object itself for cascading\n\t */\n\tprefixLength(prefixLength) {\n\t\tif (!Utils.isNumber(prefixLength) || prefixLength < 0) {\n\t\t\tthrow TypeError(\"Prefix length must be a positive number.\");\n\t\t}\n\t\tthis._data.prefix_length = prefixLength;\n\t\treturn this;\n\t}\n}\n\n/**\n * A query which matches documents containing the prefix of a term inside a field.\n *\n * * To enable scoring for wildcard queries, use {@link WildcardQuery#enableScoring}.\n *\n * See also [Lucene#PrefixQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/PrefixQuery.html}\n * and [Elasticsearch#MatchQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-prefix-query.html}\n *\n * @example\n * new QueryBuilder()\n *   .prefix(\"surname\", \"alb\")\n * .build()\n * // The resulting documents:\n * // contains the term prefix alb as surname\n *\n * @param {string} field - the field name of the document\n * @param {string} prefix - the prefix of a term\n * @extends BaseQuery\n */\nexport class PrefixQuery extends BaseQuery {\n\tconstructor(field, prefix, data = {}) {\n\t\tsuper(\"prefix\", data);\n\t\tthis._data.field = Utils.asString(field);\n\t\tthis._data.value = Utils.asString(prefix);\n\t}\n\n\t/**\n\t * This flag enables scoring for wildcard results, similar to {@link TermQuery}.\n\t * @param {boolean} enable - flag to enable or disable scoring\n\t * @return {PrefixQuery}\n\t */\n\tenableScoring(enable) {\n\t\tthis._data.enable_scoring = Utils.asBoolean(enable);\n\t\treturn this;\n\t}\n}\n\n/**\n * A query which matches all documents with a given field.\n *\n * See also [Elasticsearch#ExistsQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-exists-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .exists(\"name\")\n * .build()\n * // The resulting documents:\n * // has the field \"name\"\n *\n * @param {string} field - the field name of the document\n * @extends BaseQuery\n */\nexport class ExistsQuery extends BaseQuery {\n\tconstructor(field, data = {}) {\n\t\tsuper(\"exists\", data);\n\t\tthis._data.field = Utils.asString(field);\n\t}\n}\n\n/**\n * A query which tokenizes the given query text, performs a query foreach token and combines the results using a boolean\n * operator.\n *\n * Operator      | Description\n * ------------- | -------------\n * or (default) | Finds documents which matches some tokens. The minimum amount of matches can be controlled with [minimumShouldMatch]{@link MatchQuery#minimumShouldMatch} (default is 1).\n * and | Finds documents which matches all tokens.\n *\n * To enable a [fuzzy query]{@link FuzzyQuery} for the tokens, use {@link MatchQuery#fuzziness} and {@link MatchQuery#prefixLength}.\n *\n * See also [Lucene#?]{@link ?}\n * and [Elasticsearch#MatchQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .match(\"name\", \"albrt einsten\")\n *     .boost(2.5)\n *     .operator(\"and\")\n *     .fuzziness(2)\n *     .prefixLength(3)\n * .build();\n * // The resulting documents:\n * // contains the fuzzy name albrt einsten (like Albert Einstein) with a boost of 2.5\n *\n * @param {string} field - the field name of the document\n * @param {string} query - the query text\n * @extends BaseQuery\n */\nexport class MatchQuery extends BaseQuery {\n\tconstructor(field, query, data = {}) {\n\t\tsuper(\"match\", data);\n\t\tthis._data.field = Utils.asString(field);\n\t\tthis._data.value = Utils.asString(query);\n\t}\n\n\t/**\n\t * Controls the amount of minimum matching sub queries before a document will be considered.\n\t * @param {number} minShouldMatch - number of minimum matching sub queries\n\t *   minShouldMatch >= 1: Indicates a fixed value regardless of the number of sub queries.\n\t *   minShouldMatch <= -1: Indicates that the number of sub queries, minus this number should be mandatory.\n\t *   minShouldMatch < 0: Indicates that this percent of the total number of sub queries can be missing.\n\t *     The number computed from the percentage is rounded down, before being subtracted from the total to determine\n\t *     the minimum.\n\t *   minShouldMatch < 1: Indicates that this percent of the total number of sub queries are necessary.\n\t *     The number computed from the percentage is rounded down and used as the minimum.\n\t * @return {MatchQuery} object itself for cascading\n\t */\n\tminimumShouldMatch(minShouldMatch) {\n\t\tif (!Utils.isNumber(minShouldMatch)) {\n\t\t\tthrow TypeError(\"Minimum should match must be a number or a string.\");\n\t\t}\n\t\tif (this._data.operator !== undefined && this._data.operator === \"and\") {\n\t\t\tthrow SyntaxError(\"Match query with \\\"and\\\" operator does not support minimum should match.\");\n\t\t}\n\t\tthis._data.minimum_should_match = minShouldMatch;\n\t\treturn this;\n\t}\n\n\t/**\n\t * Sets the boolean operator.\n\t * @param {string} op - the operator (_or_/_and_)\n\t * @return {MatchQuery} object itself for cascading\n\t */\n\toperator(op) {\n\t\top = Utils.asString(op);\n\t\tif (op !== 'and' && op !== 'or') {\n\t\t\tthrow SyntaxError(\"Unknown operator.\");\n\t\t}\n\t\tthis._data.operator = op;\n\t\tif (this._data.minimum_should_match !== undefined && this._data.operator === \"and\") {\n\t\t\tthrow SyntaxError(\"Match query with \\\"and\\\" operator does not support minimum should match.\");\n\t\t}\n\t\treturn this;\n\t}\n\n\t/**\n\t * Sets the maximal allowed fuzziness.\n\t * @param {number|string} fuzziness - the edit distance as number or AUTO\n\t *\n\t * AUTO generates an edit distance based on the length of the term:\n\t * * 0..2 -> must match exactly\n\t * * 3..5 -> one edit allowed\n\t * * >5 two edits allowed\n\t *\n\t * @return {MatchQuery} - object itself for cascading\n\t */\n\tfuzziness(fuzziness) {\n\t\tif (!(Utils.isString(fuzziness) && fuzziness === \"AUTO\") && !(Utils.isNumber(fuzziness) && fuzziness >= 0)) {\n\t\t\tthrow TypeError(\"Fuzziness must be a positive number or AUTO.\");\n\t\t}\n\t\tthis._data.fuzziness = fuzziness;\n\t\treturn this;\n\t}\n\n\t/**\n\t * Sets the starting word length which should not be considered for fuzziness.\n\t * @param {number} prefixLength - the positive prefix length\n\t * @return {MatchQuery} - object itself for cascading\n\t */\n\tprefixLength(prefixLength) {\n\t\tif (!Utils.isNumber(prefixLength) || prefixLength < 0) {\n\t\t\tthrow TypeError(\"Prefix length must be a positive number.\");\n\t\t}\n\t\tthis._data.prefix_length = prefixLength;\n\t\treturn this;\n\t}\n}\n\n/**\n * A query that matches all documents and giving them a constant score equal to the query boost.\n *\n * Typically used inside a must clause of a {@link BoolQuery} to subsequently reject non matching documents with the not\n * clause.\n *\n * See also [Lucene#MatchAllDocsQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/MatchAllDocsQuery.html}\n * and [Elasticsearch#MatchAllQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-all-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .matchAll()\n *     .boost(2.5)\n * .build()\n * // The resulting documents:\n * // all documents and giving a score of 2.5\n *\n * @extends BaseQuery\n */\nexport class MatchAllQuery extends BaseQuery {\n\tconstructor(data = {}) {\n\t\tsuper(\"match_all\", data);\n\t}\n}\n\n/**\n * A query which holds all sub queries like an array.\n * @private\n */\nclass ArrayQuery extends BaseQuery {\n\tconstructor(callbackName, callback, data = {}) {\n\t\tsuper(\"array\", data);\n\t\tthis._data.values = [];\n\t\tthis._callbackName = callbackName;\n\t\tthis[callbackName] = callback;\n\n\t\tthis._prepare = (queryType, ...args) => {\n\t\t\tlet data = {};\n\t\t\tlet query = new queryType(...args, data);\n\t\t\tthis._data.values.push(data);\n\t\t\tquery.bool = this.bool;\n\t\t\tquery.constantScore = this.constantScore;\n\t\t\tquery.term = this.term;\n\t\t\tquery.terms = this.terms;\n\t\t\tquery.wildcard = this.wildcard;\n\t\t\tquery.fuzzy = this.fuzzy;\n\t\t\tquery.match = this.match;\n\t\t\tquery.matchAll = this.matchAll;\n\t\t\tquery.prefix = this.prefix;\n\t\t\tquery.exists = this.exists;\n\t\t\tquery._prepare = this._prepare;\n\t\t\tquery[this._callbackName] = this[this._callbackName];\n\t\t\treturn query;\n\t\t};\n\t}\n\n\tbool() {\n\t\treturn this._prepare(BoolQuery);\n\t}\n\n\tconstantScore() {\n\t\treturn this._prepare(ConstantScoreQuery);\n\t}\n\n\tterm(field, term) {\n\t\treturn this._prepare(TermQuery, field, term);\n\t}\n\n\tterms(field, terms) {\n\t\treturn this._prepare(TermsQuery, field, terms);\n\t}\n\n\twildcard(field, wildcard) {\n\t\treturn this._prepare(WildcardQuery, field, wildcard);\n\t}\n\n\tfuzzy(field, fuzzy) {\n\t\treturn this._prepare(FuzzyQuery, field, fuzzy);\n\t}\n\n\tmatch(field, query) {\n\t\treturn this._prepare(MatchQuery, field, query);\n\t}\n\n\tmatchAll() {\n\t\treturn this._prepare(MatchAllQuery);\n\t}\n\n\tprefix(field, prefix) {\n\t\treturn this._prepare(PrefixQuery, field, prefix);\n\t}\n\n\texists(field) {\n\t\treturn this._prepare(ExistsQuery, field);\n\t}\n}\n\n/**\n * A query that wraps sub queries and returns a constant score equal to the query boost for every document in the filter.\n *\n * See also [Lucene#BooleanQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/ConstantScoreQuery.html}\n * and [Elasticsearch#ConstantScoreQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-constant-score-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .constantScore()\n *     .boost(1.5)\n *     .beginFilter()\n *       .term(\"first_name\", \"albert\")\n *       .term(\"surname\", \"einstein\")\n *     .endFilter()\n * .build()\n * // The resulting documents:\n * // * contains albert as first name, einstein as surname and the document score is 42.\n *\n * @extends BaseQuery\n */\nexport class ConstantScoreQuery extends BaseQuery {\n\tconstructor(data = {}) {\n\t\tsuper(\"constant_score\", data);\n\t}\n\n\t/**\n\t * Starts an array of queries. Use endFilter() to finish the array.\n\t * @return {ArrayQuery} array query for holding sub queries\n\t */\n\tbeginFilter() {\n\t\tthis._data.filter = {};\n\t\treturn new ArrayQuery(\"endFilter\", () => {\n\t\t\treturn this;\n\t\t}, this._data.filter);\n\t}\n}\n\n/**\n * A query that matches documents matching boolean combinations of sub queries.\n *\n * This query consists of one or more boolean clauses with different behavior but interrelated to each other.\n *\n * Occur         | Description\n * ------------- | -------------\n * must  | Finds documents which matches all sub queries.\n * filter  | Finds documents which matches all sub queries but these documents do not contribute to the score.\n * should  | Finds documents which matches some sub queries. The minimum amount of matches can be controlled with [minimumShouldMatch]{@link BoolQuery#minimumShouldMatch} (default is 1).\n * not  | Documents which match any sub query will be ignored.\n *\n * A sub query can be any other query type and also the bool query itself.\n *\n * See also [Lucene#BooleanQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/BooleanQuery.html}\n * and [Elasticsearch#BoolQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .bool()\n *     .beginMust().boost(2)\n *       .term(\"first_name\", \"albert\")\n *     .endMust()\n *     .beginFilter()\n *       .term(\"birthplace\", \"ulm\")\n *     .endFilter()\n *     .beginShould().minimumShouldMatch(2)\n *       .fuzzy(\"surname\", \"einstin\")\n *       .wildcard(\"name\", \"geni?s\")\n *       .term(\"quotes\", \"infinity\")\n *     .endShould()\n *     .beginNot()\n *       .terms(\"research_field\", [\"biology\", \"geography\"])\n *     .endNot()\n * .build();\n * // The resulting documents:\n * // contains the name albert (must: contribute to the score with a boost of 2)\n * // contains the birthplace ulm (filter: not contribute to the score)\n * // contains a minimum of two matches from the fuzzy, wildcard and/or term query (should: contribute to the score)\n * // do not contains biology or geography as research field (not: not contribute to the score)\n *\n * @extends BaseQuery\n */\nexport class BoolQuery extends BaseQuery {\n\tconstructor(data = {}) {\n\t\tsuper(\"bool\", data);\n\t}\n\n\t/**\n\t * Starts an array of queries for must clause. Use endMust() to finish the array.\n\t * @return {ArrayQuery} array query for holding sub queries\n\t */\n\tbeginMust() {\n\t\tthis._data.must = {};\n\t\treturn new ArrayQuery(\"endMust\", () => {\n\t\t\treturn this;\n\t\t}, this._data.must);\n\t}\n\n\t/**\n\t * Starts an array of queries for filter clause. Use endFilter() to finish the array.\n\t * @return {ArrayQuery} array query for holding sub queries\n\t */\n\tbeginFilter() {\n\t\tthis._data.filter = {};\n\t\treturn new ArrayQuery(\"endFilter\", () => {\n\t\t\treturn this;\n\t\t}, this._data.filter);\n\t}\n\n\t/**\n\t * Starts an array of queries for should clause. Use endShould() to finish the array.\n\t * @return {ArrayQuery} array query for holding sub queries\n\t */\n\tbeginShould() {\n\t\tthis._data.should = {};\n\t\treturn new ArrayQuery(\"endShould\", () => {\n\t\t\treturn this;\n\t\t}, this._data.should);\n\t}\n\n\t/**\n\t * Starts an array of queries for not clause. Use endNot() to finish the array.\n\t * @return {ArrayQuery} array query for holding sub queries\n\t */\n\tbeginNot() {\n\t\tthis._data.not = {};\n\t\treturn new ArrayQuery(\"endNot\", () => {\n\t\t\treturn this;\n\t\t}, this._data.not);\n\t}\n\n\t/**\n\t * Controls the amount of minimum matching sub queries before a document will be considered.\n\t * @param {number} minShouldMatch - number of minimum matching sub queries\n\t *   minShouldMatch >= 1: Indicates a fixed value regardless of the number of sub queries.\n\t *   minShouldMatch <= -1: Indicates that the number of sub queries, minus this number should be mandatory.\n\t *   minShouldMatch < 0: Indicates that this percent of the total number of sub queries can be missing.\n\t *     The number computed from the percentage is rounded down, before being subtracted from the total to determine\n\t *     the minimum.\n\t *   minShouldMatch < 1: Indicates that this percent of the total number of sub queries are necessary.\n\t *     The number computed from the percentage is rounded down and used as the minimum.\n\t * @return {BoolQuery} object itself for cascading\n\t */\n\tminimumShouldMatch(minShouldMatch) {\n\t\tif (!Utils.isNumber(minShouldMatch)) {\n\t\t\tthrow TypeError(\"Minimum should match must be a number or a string.\");\n\t\t}\n\t\tthis._data.minimum_should_match = minShouldMatch;\n\t\treturn this;\n\t}\n}\n\n/**\n * This query builder is the root of each query search.\n * The query contains a sub query and parameters for setup scoring and search options.\n *\n * Possible sub query types are:\n * {@link TermQuery}, {@link TermsQuery}, {@link FuzzyQuery}, {@link WildcardQuery},\n * {@link MatchQuery}, {@link MatchAllQuery}, {@link PrefixQuery},  {@link BoolQuery},\n * {@link ConstantScoreQuery}, {@link ExistsQuery}\n *\n * @example\n * new QueryBuilder()\n *   .finalScoring(true)\n *   .useBM25(1.5, 0.5)\n *   .term(\"first_name\", \"albert\")\n * .build();\n * // The resulting documents:\n * // contains the first name albert\n * // are scored and ranked using BM25 with k1=1.5 and b=0.5\n */\nexport class QueryBuilder {\n\tconstructor() {\n\t\tthis._data = {query: {}};\n\t\tthis.useBM25();\n\t}\n\n\t/**\n\t * The query performs a final scoring over all scored sub queries and rank documents by there relevant.\n\t * @param {boolean} enable - flag to enable or disable final scoring\n\t * @return {QueryBuilder}\n\t */\n\tenableFinalScoring(enable) {\n\t\tthis._data.final_scoring = Utils.asBoolean(enable);\n\t\treturn this;\n\t}\n\n\t/**\n\t * Use [Okapi BM25]{@link https://en.wikipedia.org/wiki/Okapi_BM25} as scoring model (default).\n\t *\n\t * See also [Lucene#MatchAllDocsQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/similarities/BM25Similarity.html}\n\t * and [Elasticsearch#BM25]{@link https://www.elastic.co/guide/en/elasticsearch/guide/current/pluggable-similarites.html#bm25}.\n\t *\n\t * @param {number} [k1=1.2] - controls how quickly an increase in term frequency results in term-frequency saturation.\n\t * \t\t\t\t\t\t\t\t\t\t\t\t\t\tLower values result in quicker saturation, and higher values in slower saturation.\n\t * @param {number} [b=0.75] - controls how much effect field-length normalization should have.\n\t * \t\t\t\t\t\t\t\t\t\t\t\t\t\tA value of 0.0 disables normalization completely, and a value of 1.0 normalizes fully.\n\t * @return {QueryBuilder}\n\t */\n\tuseBM25(k1 = 1.2, b = 0.75) {\n\t\tif (!Utils.isNumber(k1) || k1 < 0) {\n\t\t\tthrow TypeError(\"BM25s k1 must be a positive number.\");\n\t\t}\n\t\tif (!Utils.isNumber(b) || b < 0 || b > 1) {\n\t\t\tthrow TypeError(\"BM25s b must be a number between 0 and 1 inclusive.\");\n\t\t}\n\n\t\tthis._data.scoring = {\n\t\t\ttype: \"BM25\",\n\t\t\tk1,\n\t\t\tb\n\t\t};\n\t\treturn this;\n\t}\n\n\tbool() {\n\t\treturn this._prepare(BoolQuery);\n\t}\n\n\tconstantScore() {\n\t\treturn this._prepare(ConstantScoreQuery);\n\t}\n\n\tterm(field, term) {\n\t\treturn this._prepare(TermQuery, field, term);\n\t}\n\n\tterms(field, terms) {\n\t\treturn this._prepare(TermsQuery, field, terms);\n\t}\n\n\twildcard(field, wildcard) {\n\t\treturn this._prepare(WildcardQuery, field, wildcard);\n\t}\n\n\tfuzzy(field, fuzzy) {\n\t\treturn this._prepare(FuzzyQuery, field, fuzzy);\n\t}\n\n\tmatch(field, query) {\n\t\treturn this._prepare(MatchQuery, field, query);\n\t}\n\n\tmatchAll() {\n\t\treturn this._prepare(MatchAllQuery);\n\t}\n\n\tprefix(field, prefix) {\n\t\treturn this._prepare(PrefixQuery, field, prefix);\n\t}\n\n\texists(field) {\n\t\treturn this._prepare(ExistsQuery, field);\n\t}\n\n\t_prepare(queryType, ...args) {\n\t\tthis._child = new queryType(...args, this._data.query);\n\t\tthis._child.build = () => {\n\t\t\treturn this._data;\n\t\t};\n\t\treturn this._child;\n\t}\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/inverted_index/queries.js\n// module id = 3\n// module chunks = 0","export class Scorer {\n\tconstructor(invIdxs) {\n\t\tthis._invIdxs = invIdxs;\n\t\tthis._cache = {};\n\t}\n\n\tsetDirty() {\n\t\tthis._cache = {};\n\t}\n\n\tprepare(fieldName, boost, termIdx, doScoring, docResults = {}, term = null) {\n\t\tif (termIdx === null || termIdx.dc === undefined) {\n\t\t\treturn null;\n\t\t}\n\n\t\tlet idf = this._idf(fieldName, termIdx.df);\n\t\tlet docIds = Object.keys(termIdx.dc);\n\t\tfor (let j = 0; j < docIds.length; j++) {\n\t\t\tlet docId = docIds[j];\n\t\t\tif (docResults[docId] === undefined) {\n\t\t\t\tdocResults[docId] = [];\n\t\t\t}\n\n\t\t\tif (doScoring) {\n\t\t\t\tlet tf = termIdx.dc[docId];\n\t\t\t\tdocResults[docId].push({\n\t\t\t\t\ttype: 'BM25',\n\t\t\t\t\ttf,\n\t\t\t\t\tidf,\n\t\t\t\t\tboost,\n\t\t\t\t\tfieldName,\n\t\t\t\t\tterm\n\t\t\t\t});\n\t\t\t} else {\n\t\t\t\tdocResults[docId] = [{\n\t\t\t\t\ttype: \"constant\", value: 1, boost, fieldName\n\t\t\t\t}];\n\t\t\t}\n\t\t}\n\n\t\treturn docResults;\n\t}\n\n\tscoreConstant(boost, docId, docResults = {}) {\n\t\tif (docResults[docId] === undefined) {\n\t\t\tdocResults[docId] = [];\n\t\t}\n\t\tdocResults[docId].push({type: \"constant\", value: 1, boost});\n\t\treturn docResults;\n\t}\n\n\tfinalScore(query, docResults = {}) {\n\t\tlet result = {};\n\t\tlet k1 = query.scoring.k1;\n\t\tlet b = query.scoring.b;\n\n\t\tlet docs = Object.keys(docResults);\n\t\tfor (let i = 0, docId; i < docs.length, docId = docs[i]; i++) {\n\t\t\tlet docScore = 0;\n\t\t\tfor (let j = 0; j < docResults[docId].length; j++) {\n\t\t\t\tlet docResult = docResults[docId][j];\n\n\t\t\t\tlet res = 0;\n\t\t\t\tswitch (docResult.type) {\n\t\t\t\t\tcase 'BM25': {\n\t\t\t\t\t\tlet tf = docResult.tf;\n\t\t\t\t\t\tlet fieldLength = Scorer._calculateFieldLength(this._invIdxs[docResult.fieldName].documentStore[docId]\n\t\t\t\t\t\t\t.fieldLength);\n\t\t\t\t\t\tlet avgFieldLength = this._avgFieldLength(docResult.fieldName);\n\t\t\t\t\t\t// tfNorm, computed as (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * fieldLength / avgFieldLength)) from\n\t\t\t\t\t\tlet tfNorm = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (fieldLength / avgFieldLength)));\n\t\t\t\t\t\tres = docResult.idf * tfNorm * docResult.boost;\n\t\t\t\t\t\t// console.log(\n\t\t\t\t\t\t// \tdocId + \":\" + docResult.fieldName + \":\" + docResult.term + \" = \" + res,\n\t\t\t\t\t\t// \t\"\\n\\ttype: BM25\",\n\t\t\t\t\t\t// \t\"\\n\\tboost: \" + docResult.boost,\n\t\t\t\t\t\t// \t\"\\n\\tidf : \" + docResult.idf,\n\t\t\t\t\t\t// \t\"\\n\\ttfNorm : \" + tfNorm,\n\t\t\t\t\t\t// \t\"\\n\\ttf : \" + tf,\n\t\t\t\t\t\t// \t\"\\n\\tavg : \" + avgFieldLength,\n\t\t\t\t\t\t// \t\"\\n\\tfl : \" + fieldLength);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\tcase 'constant':\n\t\t\t\t\t\tres = docResult.value * docResult.boost;\n\t\t\t\t\t\t/*console.log(\n\t\t\t\t\t\t \"Constant: \" + res,\n\t\t\t\t\t\t \"\\n\\tboost: \" + docResult.boost,\n\t\t\t\t\t\t \"\\n\\tvalue : \" + docResult.value);*/\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdocScore += res;\n\t\t\t}\n\t\t\t//console.log(docId, \" === \", docScore);\n\t\t\tresult[docId] = docScore;\n\t\t}\n\t\treturn result;\n\t}\n\n\tstatic _calculateFieldLength(fieldLength) {\n\t\t// Lucene uses a SmallFloat (size of 1 byte) to store the field length in scoring.\n\t\t// This is useless in javascript, because every number is represented as a double (8 byte).\n\t\t// To align the scoring result with lucene, this calculation is still needed.\n\t\t// Lucene also includes the field boost, but field boost is deprecated and not supported by Loki.\n\n\t\t// Find closest value in array.\n\t\tconst lockUp = [1, 1.30612242, 1.77777779, 2.55999994, 4, 5.22448969, 7.11111116, 10.2399998, 16, 20.8979588,\n\t\t\t28.4444447, 40.9599991, 64, 83.591835, 113.777779, 163.839996, 256, 334.36734, 455.111115, 655.359985, 1024,\n\t\t\t1337.46936, 1820.44446, 2621.43994, 4096, 5349.87744, 7281.77783, 10485.7598, 16384, 21399.5098, 29127.1113,\n\t\t\t41943.0391, 65536, 85598.0391, 116508.445, 167772.156, 262144, 342392.156, 466033.781, 671088.625, 1048576,\n\t\t\t1369568.62, 1864135.12, 2684354.5, 4194304, 5478274.5, 7456540.5, 10737418, 16777216, 21913098, 29826162,\n\t\t\t42949672, 67108864, 87652392, 119304648, 171798688, 268435456, 350609568, 477218592, 687194752];\n\n\t\tfor (let i = 0; i < lockUp.length; i++) {\n\t\t\tif (lockUp[i] >= fieldLength) {\n\t\t\t\treturn lockUp[i];\n\t\t\t}\n\t\t}\n\t\tthrow RangeError(\"Unsupported field length.\");\n\t}\n\n\t_getCache(fieldName) {\n\t\tif (this._cache[fieldName] === undefined) {\n\t\t\tlet avgFieldLength = this._invIdxs[fieldName].totalFieldLength / this._invIdxs[fieldName].documentCount;\n\t\t\tthis._cache[fieldName] = {idfs: {}, avgFieldLength};\n\t\t}\n\t\treturn this._cache[fieldName];\n\t}\n\n\t/**\n\t * Returns the idf by either calculate it or use a cached one.\n\t * @param {string} fieldName - the name of the field\n\t * @param {number} docFreq - the doc frequency of the term\n\t * @returns {number} the idf\n\t * @private\n\t */\n\t_idf(fieldName, docFreq) {\n\t\tlet cache = this._getCache(fieldName);\n\t\tif (cache.idfs[docFreq] !== undefined) {\n\t\t\treturn cache.idfs[docFreq];\n\t\t}\n\t\treturn cache.idfs[docFreq] = Math.log(1 + (this._invIdxs[fieldName].documentCount - docFreq + 0.5) / (docFreq + 0.5));\n\t}\n\n\t_avgFieldLength(fieldName) {\n\t\treturn this._getCache(fieldName).avgFieldLength;\n\t}\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/inverted_index/scorer.js\n// module id = 4\n// module chunks = 0","import {InvertedIndex} from './inverted_index';\nimport {IndexSearcher} from './index_searcher';\nimport {Tokenizer} from './tokenizer';\nimport * as Utils from './utils.js';\n\nexport class FullTextSearch {\n\t/**\n\t * Initialize the full text search for the given fields.\n\t * @param {object[]} fields - the field options\n\t * @param {string} fields.name - the name of the field\n\t * @param {boolean=true} fields.store - flag to indicate if the full text search should be stored on serialization or\n\t *\trebuild on deserialization\n\t *@param {Tokenizer=Tokenizer} fields.tokenizer - the tokenizer of the field\n\t */\n\tconstructor(fields) {\n\t\tif (fields === undefined) {\n\t\t\tthrow new SyntaxError('Fields needs to be defined!');\n\t\t}\n\n\t\tthis._invIdxs = {};\n\t\t// Get field names and tokenizers.\n\t\tif (Array.isArray(fields)) {\n\t\t\tfor (let i = 0; i < fields.length; i++) {\n\t\t\t\tlet field = fields[i];\n\t\t\t\tlet name = Utils.asString(field.name, TypeError('Field name needs to be a string.'));\n\n\t\t\t\tlet store = field.store !== undefined ?\n\t\t\t\t\tUtils.asBoolean(field.store, TypeError(\"Field store flag needs to be a boolean\")) : true;\n\n\t\t\t\tlet tokenizer = null;\n\t\t\t\tif (field.tokenizer !== undefined) {\n\t\t\t\t\tif (!(field.tokenizer instanceof Tokenizer)) {\n\t\t\t\t\t\tthrow new TypeError(\"Field tokenizer needs to be a instance of tokenizer.\");\n\t\t\t\t\t}\n\t\t\t\t\ttokenizer = field.tokenizer;\n\t\t\t\t} else {\n\t\t\t\t\ttokenizer = new Tokenizer();\n\t\t\t\t}\n\t\t\t\tthis._invIdxs[name] = new InvertedIndex(store, tokenizer);\n\t\t\t}\n\t\t} else {\n\t\t\tthrow new TypeError('fields needs to be an array with field name and a tokenizer (optional).');\n\t\t}\n\n\t\tthis._docs = new Set();\n\t\tthis._idxSearcher = new IndexSearcher(this._invIdxs, this._docs);\n\t}\n\n\taddDocument(doc) {\n\t\tif (doc.$loki === undefined) {\n\t\t\tthrow new Error('Document is not stored in the collection.');\n\t\t}\n\n\t\tlet fieldNames = Object.keys(doc);\n\t\tfor (let i = 0, fieldName; i < fieldNames.length, fieldName = fieldNames[i]; i++) {\n\t\t\tif (this._invIdxs[fieldName] !== undefined) {\n\t\t\t\tthis._invIdxs[fieldName].insert(doc[fieldName], doc.$loki);\n\t\t\t}\n\t\t}\n\n\t\tthis._docs.add(doc.$loki);\n\t\tthis.setDirty();\n\t}\n\n\tremoveDocument(doc) {\n\t\tif (doc.$loki === undefined) {\n\t\t\tthrow new Error('Document is not stored in the collection.');\n\t\t}\n\n\t\tlet fieldNames = Object.keys(this._invIdxs);\n\t\tfor (let i = 0; i < fieldNames.length; i++) {\n\t\t\tthis._invIdxs[fieldNames[i]].remove(doc.$loki);\n\t\t}\n\n\t\tthis._docs.delete(doc.$loki);\n\t\tthis.setDirty();\n\t}\n\n\tupdateDocument(doc) {\n\t\tthis.removeDocument(doc);\n\t\tthis.addDocument(doc);\n\t}\n\n\tsearch(query) {\n\t\treturn this._idxSearcher.search(query);\n\t}\n\n\ttoJSON() {\n\t\tlet serialized = {};\n\t\tlet fieldNames = Object.keys(this._invIdxs);\n\t\tfor (let i = 0, fieldName; i < fieldNames.length, fieldName = fieldNames[i]; i++) {\n\t\t\tserialized[fieldName] = this._invIdxs[fieldName].toJSON();\n\t\t}\n\t\treturn serialized;\n\t}\n\n\tloadJSON(serialized, tokenizers) {\n\t\tlet db = JSON.parse(serialized);\n\t\tlet fieldNames = Object.keys(db);\n\t\tfor (let i = 0, fieldName; i < fieldNames.length, fieldName = fieldNames[i]; i++) {\n\t\t\tthis._invIdxs[fieldName] = new InvertedIndex();\n\t\t\tthis._invIdxs[fieldName].loadJSON(db[fieldName], tokenizers[fieldName]);\n\t\t}\n\t}\n\n\tsetDirty() {\n\t\tthis._idxSearcher.setDirty();\n\t}\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/inverted_index/full_text_search.js\n// module id = 5\n// module chunks = 0","/*\n * From MihaiValentin/lunr-languages.\n * Last update from 04/16/2017 - 19af41fb9bd644d9081ad274f96f700b21464290\n */\nimport {generateTrimmer, generateStopWordFilter, Among, SnowballProgram} from './support.js';\nimport {Tokenizer} from '../tokenizer';\n\nlet wordCharacters = \"A-Za-z\\xAA\\xBA\\xC0-\\xD6\\xD8-\\xF6\\xF8-\\u02B8\\u02E0-\\u02E4\\u1D00-\\u1D25\\u1D2C-\\u1D5C\\u1D62-\\u1D65\\u1D6B-\\u1D77\\u1D79-\\u1DBE\\u1E00-\\u1EFF\\u2071\\u207F\\u2090-\\u209C\\u212A\\u212B\\u2132\\u214E\\u2160-\\u2188\\u2C60-\\u2C7F\\uA722-\\uA787\\uA78B-\\uA7AD\\uA7B0-\\uA7B7\\uA7F7-\\uA7FF\\uAB30-\\uAB5A\\uAB5C-\\uAB64\\uFB00-\\uFB06\\uFF21-\\uFF3A\\uFF41-\\uFF5A\";\nlet trimmer = generateTrimmer(wordCharacters);\n\nlet tkz = new Tokenizer();\n\ntkz.add('trimmer-de', trimmer);\n\nlet stemmer = ((() => {\n\t/* create the wrapped stemmer object */\n\tlet st = new (function GermanStemmer() {\n\t\tlet a_0 = [new Among(\"\", -1, 6), new Among(\"U\", 0, 2),\n\t\t\tnew Among(\"Y\", 0, 1), new Among(\"\\u00E4\", 0, 3),\n\t\t\tnew Among(\"\\u00F6\", 0, 4), new Among(\"\\u00FC\", 0, 5)\n\t\t];\n\n\t\tlet a_1 = [\n\t\t\tnew Among(\"e\", -1, 2), new Among(\"em\", -1, 1),\n\t\t\tnew Among(\"en\", -1, 2), new Among(\"ern\", -1, 1),\n\t\t\tnew Among(\"er\", -1, 1), new Among(\"s\", -1, 3),\n\t\t\tnew Among(\"es\", 5, 2)\n\t\t];\n\n\t\tlet a_2 = [new Among(\"en\", -1, 1),\n\t\t\tnew Among(\"er\", -1, 1), new Among(\"st\", -1, 2),\n\t\t\tnew Among(\"est\", 2, 1)\n\t\t];\n\n\t\tlet a_3 = [new Among(\"ig\", -1, 1),\n\t\t\tnew Among(\"lich\", -1, 1)\n\t\t];\n\n\t\tlet a_4 = [new Among(\"end\", -1, 1),\n\t\t\tnew Among(\"ig\", -1, 2), new Among(\"ung\", -1, 1),\n\t\t\tnew Among(\"lich\", -1, 3), new Among(\"isch\", -1, 2),\n\t\t\tnew Among(\"ik\", -1, 2), new Among(\"heit\", -1, 3),\n\t\t\tnew Among(\"keit\", -1, 4)\n\t\t];\n\n\t\tlet g_v = [17, 65, 16, 1, 0, 0, 0, 0, 0, 0,\n\t\t\t0, 0, 0, 0, 0, 0, 8, 0, 32, 8\n\t\t];\n\n\t\tlet g_s_ending = [117, 30, 5];\n\n\t\tlet g_st_ending = [\n\t\t\t117, 30, 4\n\t\t];\n\n\t\tlet I_x;\n\t\tlet I_p2;\n\t\tlet I_p1;\n\t\tlet sbp = new SnowballProgram();\n\t\tthis.setCurrent = word => {\n\t\t\tsbp.setCurrent(word);\n\t\t};\n\t\tthis.getCurrent = () => sbp.getCurrent();\n\n\t\tfunction habr1(c1, c2, v_1) {\n\t\t\tif (sbp.eq_s(1, c1)) {\n\t\t\t\tsbp.ket = sbp.cursor;\n\t\t\t\tif (sbp.in_grouping(g_v, 97, 252)) {\n\t\t\t\t\tsbp.slice_from(c2);\n\t\t\t\t\tsbp.cursor = v_1;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn false;\n\t\t}\n\n\t\tfunction r_prelude() {\n\t\t\tlet v_1 = sbp.cursor;\n\t\t\tlet v_2;\n\t\t\tlet v_3;\n\t\t\tlet v_4;\n\t\t\tlet v_5;\n\t\t\twhile (true) {\n\t\t\t\tv_2 = sbp.cursor;\n\t\t\t\tsbp.bra = v_2;\n\t\t\t\tif (sbp.eq_s(1, \"\\u00DF\")) {\n\t\t\t\t\tsbp.ket = sbp.cursor;\n\t\t\t\t\tsbp.slice_from(\"ss\");\n\t\t\t\t} else {\n\t\t\t\t\tif (v_2 >= sbp.limit)\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tsbp.cursor = v_2 + 1;\n\t\t\t\t}\n\t\t\t}\n\t\t\tsbp.cursor = v_1;\n\t\t\twhile (true) {\n\t\t\t\tv_3 = sbp.cursor;\n\t\t\t\twhile (true) {\n\t\t\t\t\tv_4 = sbp.cursor;\n\t\t\t\t\tif (sbp.in_grouping(g_v, 97, 252)) {\n\t\t\t\t\t\tv_5 = sbp.cursor;\n\t\t\t\t\t\tsbp.bra = v_5;\n\t\t\t\t\t\tif (habr1(\"u\", \"U\", v_4))\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tsbp.cursor = v_5;\n\t\t\t\t\t\tif (habr1(\"y\", \"Y\", v_4))\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\tif (v_4 >= sbp.limit) {\n\t\t\t\t\t\tsbp.cursor = v_3;\n\t\t\t\t\t\treturn;\n\t\t\t\t\t}\n\t\t\t\t\tsbp.cursor = v_4 + 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tfunction habr2() {\n\t\t\twhile (!sbp.in_grouping(g_v, 97, 252)) {\n\t\t\t\tif (sbp.cursor >= sbp.limit)\n\t\t\t\t\treturn true;\n\t\t\t\tsbp.cursor++;\n\t\t\t}\n\t\t\twhile (!sbp.out_grouping(g_v, 97, 252)) {\n\t\t\t\tif (sbp.cursor >= sbp.limit)\n\t\t\t\t\treturn true;\n\t\t\t\tsbp.cursor++;\n\t\t\t}\n\t\t\treturn false;\n\t\t}\n\n\t\tfunction r_mark_regions() {\n\t\t\tI_p1 = sbp.limit;\n\t\t\tI_p2 = I_p1;\n\t\t\tlet c = sbp.cursor + 3;\n\t\t\tif (0 <= c && c <= sbp.limit) {\n\t\t\t\tI_x = c;\n\t\t\t\tif (!habr2()) {\n\t\t\t\t\tI_p1 = sbp.cursor;\n\t\t\t\t\tif (I_p1 < I_x)\n\t\t\t\t\t\tI_p1 = I_x;\n\t\t\t\t\tif (!habr2())\n\t\t\t\t\t\tI_p2 = sbp.cursor;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tfunction r_postlude() {\n\t\t\tlet among_var;\n\t\t\tlet v_1;\n\t\t\twhile (true) {\n\t\t\t\tv_1 = sbp.cursor;\n\t\t\t\tsbp.bra = v_1;\n\t\t\t\tamong_var = sbp.find_among(a_0, 6);\n\t\t\t\tif (!among_var)\n\t\t\t\t\treturn;\n\t\t\t\tsbp.ket = sbp.cursor;\n\t\t\t\tswitch (among_var) {\n\t\t\t\t\tcase 1:\n\t\t\t\t\t\tsbp.slice_from(\"y\");\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase 2:\n\t\t\t\t\tcase 5:\n\t\t\t\t\t\tsbp.slice_from(\"u\");\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase 3:\n\t\t\t\t\t\tsbp.slice_from(\"a\");\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase 4:\n\t\t\t\t\t\tsbp.slice_from(\"o\");\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase 6:\n\t\t\t\t\t\tif (sbp.cursor >= sbp.limit)\n\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\tsbp.cursor++;\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tfunction r_R1() {\n\t\t\treturn I_p1 <= sbp.cursor;\n\t\t}\n\n\t\tfunction r_R2() {\n\t\t\treturn I_p2 <= sbp.cursor;\n\t\t}\n\n\t\tfunction r_standard_suffix() {\n\t\t\tlet among_var;\n\t\t\tlet v_1 = sbp.limit - sbp.cursor;\n\t\t\tlet v_2;\n\t\t\tlet v_3;\n\t\t\tlet v_4;\n\t\t\tsbp.ket = sbp.cursor;\n\t\t\tamong_var = sbp.find_among_b(a_1, 7);\n\t\t\tif (among_var) {\n\t\t\t\tsbp.bra = sbp.cursor;\n\t\t\t\tif (r_R1()) {\n\t\t\t\t\tswitch (among_var) {\n\t\t\t\t\t\tcase 1:\n\t\t\t\t\t\t\tsbp.slice_del();\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase 2:\n\t\t\t\t\t\t\tsbp.slice_del();\n\t\t\t\t\t\t\tsbp.ket = sbp.cursor;\n\t\t\t\t\t\t\tif (sbp.eq_s_b(1, \"s\")) {\n\t\t\t\t\t\t\t\tsbp.bra = sbp.cursor;\n\t\t\t\t\t\t\t\tif (sbp.eq_s_b(3, \"nis\"))\n\t\t\t\t\t\t\t\t\tsbp.slice_del();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase 3:\n\t\t\t\t\t\t\tif (sbp.in_grouping_b(g_s_ending, 98, 116))\n\t\t\t\t\t\t\t\tsbp.slice_del();\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tsbp.cursor = sbp.limit - v_1;\n\t\t\tsbp.ket = sbp.cursor;\n\t\t\tamong_var = sbp.find_among_b(a_2, 4);\n\t\t\tif (among_var) {\n\t\t\t\tsbp.bra = sbp.cursor;\n\t\t\t\tif (r_R1()) {\n\t\t\t\t\tswitch (among_var) {\n\t\t\t\t\t\tcase 1:\n\t\t\t\t\t\t\tsbp.slice_del();\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase 2:\n\t\t\t\t\t\t\tif (sbp.in_grouping_b(g_st_ending, 98, 116)) {\n\t\t\t\t\t\t\t\tlet c = sbp.cursor - 3;\n\t\t\t\t\t\t\t\tif (sbp.limit_backward <= c && c <= sbp.limit) {\n\t\t\t\t\t\t\t\t\tsbp.cursor = c;\n\t\t\t\t\t\t\t\t\tsbp.slice_del();\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tsbp.cursor = sbp.limit - v_1;\n\t\t\tsbp.ket = sbp.cursor;\n\t\t\tamong_var = sbp.find_among_b(a_4, 8);\n\t\t\tif (among_var) {\n\t\t\t\tsbp.bra = sbp.cursor;\n\t\t\t\tif (r_R2()) {\n\t\t\t\t\tswitch (among_var) {\n\t\t\t\t\t\tcase 1:\n\t\t\t\t\t\t\tsbp.slice_del();\n\t\t\t\t\t\t\tsbp.ket = sbp.cursor;\n\t\t\t\t\t\t\tif (sbp.eq_s_b(2, \"ig\")) {\n\t\t\t\t\t\t\t\tsbp.bra = sbp.cursor;\n\t\t\t\t\t\t\t\tv_2 = sbp.limit - sbp.cursor;\n\t\t\t\t\t\t\t\tif (!sbp.eq_s_b(1, \"e\")) {\n\t\t\t\t\t\t\t\t\tsbp.cursor = sbp.limit - v_2;\n\t\t\t\t\t\t\t\t\tif (r_R2())\n\t\t\t\t\t\t\t\t\t\tsbp.slice_del();\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase 2:\n\t\t\t\t\t\t\tv_3 = sbp.limit - sbp.cursor;\n\t\t\t\t\t\t\tif (!sbp.eq_s_b(1, \"e\")) {\n\t\t\t\t\t\t\t\tsbp.cursor = sbp.limit - v_3;\n\t\t\t\t\t\t\t\tsbp.slice_del();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase 3:\n\t\t\t\t\t\t\tsbp.slice_del();\n\t\t\t\t\t\t\tsbp.ket = sbp.cursor;\n\t\t\t\t\t\t\tv_4 = sbp.limit - sbp.cursor;\n\t\t\t\t\t\t\tif (!sbp.eq_s_b(2, \"er\")) {\n\t\t\t\t\t\t\t\tsbp.cursor = sbp.limit - v_4;\n\t\t\t\t\t\t\t\tif (!sbp.eq_s_b(2, \"en\"))\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tsbp.bra = sbp.cursor;\n\t\t\t\t\t\t\tif (r_R1())\n\t\t\t\t\t\t\t\tsbp.slice_del();\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase 4:\n\t\t\t\t\t\t\tsbp.slice_del();\n\t\t\t\t\t\t\tsbp.ket = sbp.cursor;\n\t\t\t\t\t\t\tamong_var = sbp.find_among_b(a_3, 2);\n\t\t\t\t\t\t\tif (among_var) {\n\t\t\t\t\t\t\t\tsbp.bra = sbp.cursor;\n\t\t\t\t\t\t\t\tif (r_R2() && among_var === 1)\n\t\t\t\t\t\t\t\t\tsbp.slice_del();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tthis.stem = () => {\n\t\t\tlet v_1 = sbp.cursor;\n\t\t\tr_prelude();\n\t\t\tsbp.cursor = v_1;\n\t\t\tr_mark_regions();\n\t\t\tsbp.limit_backward = v_1;\n\t\t\tsbp.cursor = sbp.limit;\n\t\t\tr_standard_suffix();\n\t\t\tsbp.cursor = sbp.limit_backward;\n\t\t\tr_postlude();\n\t\t\treturn true;\n\t\t};\n\t});\n\n\t/* and return a function that stems a word for the current locale */\n\treturn token => {\n\t\tst.setCurrent(token);\n\t\tst.stem();\n\t\treturn st.getCurrent();\n\t};\n}))();\n\ntkz.setSplitter(\"whitespace-splitter\", function defaultSplitter(str) {\n\tlet trimmedTokens = [];\n\tlet tokens = str.split(/[\\s\\-]+/);\n\tfor (let i = 0; i < tokens.length; i++) {\n\t\tif (tokens[i] !== '') {\n\t\t\ttrimmedTokens.push(tokens[i].toLowerCase());\n\t\t}\n\t}\n\treturn trimmedTokens;\n});\n\ntkz.add('stemmer-de', stemmer);\n\nlet stopWordFilter = generateStopWordFilter([\"aber\", \"alle\", \"allem\", \"allen\", \"aller\", \"alles\", \"als\", \"also\", \"am\", \"an\", \"ander\", \"andere\", \"anderem\", \"anderen\", \"anderer\", \"anderes\", \"anderm\", \"andern\", \"anderr\", \"anders\", \"auch\", \"auf\", \"aus\", \"bei\", \"bin\", \"bis\", \"bist\", \"da\", \"damit\", \"dann\", \"das\", \"dasselbe\", \"dazu\", \"daÃŸ\", \"dein\", \"deine\", \"deinem\", \"deinen\", \"deiner\", \"deines\", \"dem\", \"demselben\", \"den\", \"denn\", \"denselben\", \"der\", \"derer\", \"derselbe\", \"derselben\", \"des\", \"desselben\", \"dessen\", \"dich\", \"die\", \"dies\", \"diese\", \"dieselbe\", \"dieselben\", \"diesem\", \"diesen\", \"dieser\", \"dieses\", \"dir\", \"doch\", \"dort\", \"du\", \"durch\", \"ein\", \"eine\", \"einem\", \"einen\", \"einer\", \"eines\", \"einig\", \"einige\", \"einigem\", \"einigen\", \"einiger\", \"einiges\", \"einmal\", \"er\", \"es\", \"etwas\", \"euch\", \"euer\", \"eure\", \"eurem\", \"euren\", \"eurer\", \"eures\", \"fÃ¼r\", \"gegen\", \"gewesen\", \"hab\", \"habe\", \"haben\", \"hat\", \"hatte\", \"hatten\", \"hier\", \"hin\", \"hinter\", \"ich\", \"ihm\", \"ihn\", \"ihnen\", \"ihr\", \"ihre\", \"ihrem\", \"ihren\", \"ihrer\", \"ihres\", \"im\", \"in\", \"indem\", \"ins\", \"ist\", \"jede\", \"jedem\", \"jeden\", \"jeder\", \"jedes\", \"jene\", \"jenem\", \"jenen\", \"jener\", \"jenes\", \"jetzt\", \"kann\", \"kein\", \"keine\", \"keinem\", \"keinen\", \"keiner\", \"keines\", \"kÃ¶nnen\", \"kÃ¶nnte\", \"machen\", \"man\", \"manche\", \"manchem\", \"manchen\", \"mancher\", \"manches\", \"mein\", \"meine\", \"meinem\", \"meinen\", \"meiner\", \"meines\", \"mich\", \"mir\", \"mit\", \"muss\", \"musste\", \"nach\", \"nicht\", \"nichts\", \"noch\", \"nun\", \"nur\", \"ob\", \"oder\", \"ohne\", \"sehr\", \"sein\", \"seine\", \"seinem\", \"seinen\", \"seiner\", \"seines\", \"selbst\", \"sich\", \"sie\", \"sind\", \"so\", \"solche\", \"solchem\", \"solchen\", \"solcher\", \"solches\", \"soll\", \"sollte\", \"sondern\", \"sonst\", \"um\", \"und\", \"uns\", \"unse\", \"unsem\", \"unsen\", \"unser\", \"unses\", \"unter\", \"viel\", \"vom\", \"von\", \"vor\", \"war\", \"waren\", \"warst\", \"was\", \"weg\", \"weil\", \"weiter\", \"welche\", \"welchem\", \"welchen\", \"welcher\", \"welches\", \"wenn\", \"werde\", \"werden\", \"wie\", \"wieder\", \"will\", \"wir\", \"wird\", \"wirst\", \"wo\", \"wollen\", \"wollte\", \"wÃ¤hrend\", \"wÃ¼rde\", \"wÃ¼rden\", \"zu\", \"zum\", \"zur\", \"zwar\", \"zwischen\", \"Ã¼ber\"]);\ntkz.add('stopWordFilter-de', stopWordFilter);\n\nexport {tkz as DE};\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/inverted_index/language/de.js\n// module id = 6\n// module chunks = 0","import {FullTextSearch} from './full_text_search';\nimport {Tokenizer} from './tokenizer';\nimport {QueryBuilder} from './queries';\nimport {DE} from './language/de';\nimport {InvertedIndex} from './inverted_index';\nimport {Scorer} from './scorer';\n\nexport {\n\tFullTextSearch,\n\tTokenizer,\n\tQueryBuilder,\n\tDE,\n\tInvertedIndex,\n\tScorer\n};\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/inverted_index/index.js\n// module id = 7\n// module chunks = 0","import {Scorer} from './scorer';\nimport {InvertedIndex} from './inverted_index';\nimport {QueryBuilder} from './queries';\n\nexport class IndexSearcher {\n\t/**\n\t *\n\t * @param {object} invIdxs\n\t */\n\tconstructor(invIdxs, docs) {\n\t\tthis._invIdxs = invIdxs;\n\t\tthis._docs = docs;\n\t\tthis._scorer = new Scorer(this._invIdxs);\n\t}\n\n\tsearch(query) {\n\t\tlet docResults = this._recursive(query.query, true);\n\n\t\t// Final scoring.\n\t\tlet finalScoring = query.final_scoring !== undefined ? query.final_scoring : true;\n\t\tif (finalScoring) {\n\t\t\treturn this._scorer.finalScore(query, docResults);\n\t\t}\n\t\treturn docResults;\n\t}\n\n\tsetDirty() {\n\t\tthis._scorer.setDirty();\n\t}\n\n\t_recursive(query, doScoring) {\n\t\tlet docResults = {};\n\t\tlet boost = query.boost !== undefined ? query.boost : 1;\n\t\tlet fieldName = query.field !== undefined ? query.field : null;\n\t\tlet enableScoring = query.enable_scoring !== undefined ? query.enable_scoring : false;\n\n\t\tlet root = null;\n\t\tlet tokenizer = null;\n\t\tif (this._invIdxs[fieldName] !== undefined) {\n\t\t\troot = this._invIdxs[fieldName].root;\n\t\t\ttokenizer = this._invIdxs[fieldName].tokenizer;\n\t\t}\n\n\t\tswitch (query.type) {\n\t\t\tcase \"bool\": {\n\t\t\t\tdocResults = null;\n\t\t\t\tif (query.must !== undefined) {\n\t\t\t\t\tdocResults = this._getUnique(query.must.values, doScoring, docResults);\n\t\t\t\t}\n\t\t\t\tif (query.filter !== undefined) {\n\t\t\t\t\tdocResults = this._getUnique(query.filter.values, false, docResults);\n\t\t\t\t}\n\n\t\t\t\tif (query.should !== undefined) {\n\t\t\t\t\tlet shouldDocs = this._getAll(query.should.values, doScoring);\n\n\t\t\t\t\tlet empty = false;\n\t\t\t\t\tif (docResults === null) {\n\t\t\t\t\t\tdocResults = {};\n\t\t\t\t\t\tempty = true;\n\t\t\t\t\t}\n\n\t\t\t\t\tlet msm = 1;\n\t\t\t\t\t// TODO: Enable percent and ranges.\n\t\t\t\t\tif (query.minimum_should_match !== undefined) {\n\t\t\t\t\t\tmsm = query.minimum_should_match;\n\t\t\t\t\t\tlet shouldLength = query.should.values.length;\n\t\t\t\t\t\tif (msm <= -1) {\n\t\t\t\t\t\t\tmsm = shouldLength + msm;\n\t\t\t\t\t\t} else if (msm < 0) {\n\t\t\t\t\t\t\tmsm = shouldLength - Math.floor(shouldLength * -msm);\n\t\t\t\t\t\t} else if (msm < 1) {\n\t\t\t\t\t\t\tmsm = Math.floor(shouldLength * msm);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t// Remove all docs with fewer matches.\n\t\t\t\t\tlet docs = Object.keys(shouldDocs);\n\t\t\t\t\tfor (let i = 0, docId; i < docs.length, docId = docs[i]; i++) {\n\t\t\t\t\t\tif (shouldDocs[docId].length >= msm) {\n\t\t\t\t\t\t\tif (docResults[docId] !== undefined) {\n\t\t\t\t\t\t\t\tdocResults[docId].push(...shouldDocs[docId]);\n\t\t\t\t\t\t\t} else if (empty) {\n\t\t\t\t\t\t\t\tdocResults[docId] = shouldDocs[docId];\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tdelete docResults[docId];\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (query.not !== undefined) {\n\t\t\t\t\tlet notDocs = this._getAll(query.not.values, false);\n\t\t\t\t\t// Remove all docs.\n\t\t\t\t\tlet docs = Object.keys(notDocs);\n\t\t\t\t\tfor (let i = 0, docId; i < docs.length, docId = docs[i]; i++) {\n\t\t\t\t\t\tif (docResults[docId] !== undefined) {\n\t\t\t\t\t\t\tdelete docResults[docId];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"term\": {\n\t\t\t\tlet termIdx = InvertedIndex.getTermIndex(query.value, root);\n\t\t\t\tthis._scorer.prepare(fieldName, boost, termIdx, doScoring, docResults, query.value);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"terms\": {\n\t\t\t\tfor (let i = 0; i < query.value.length; i++) {\n\t\t\t\t\tlet termIdx = InvertedIndex.getTermIndex(query.value[i], root);\n\t\t\t\t\tthis._scorer.prepare(fieldName, boost, termIdx, doScoring, docResults, query.value[i]);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"fuzzy\": {\n\t\t\t\tlet f = new FuzzySearch(query);\n\t\t\t\tlet b = f.search(root);\n\t\t\t\tfor (let i = 0; i < b.length; i++) {\n\t\t\t\t\tthis._scorer.prepare(fieldName, boost * b[i].boost, b[i].index, doScoring, docResults, b[i].term);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"wildcard\": {\n\t\t\t\tlet w = new WildcardSearch(query);\n\t\t\t\tlet a = w.search(root);\n\t\t\t\tfor (let i = 0; i < a.length; i++) {\n\t\t\t\t\tthis._scorer.prepare(fieldName, boost, a[i].index, doScoring && enableScoring, docResults, a[i].term);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"match_all\": {\n\t\t\t\tfor (let docId of this._docs) {\n\t\t\t\t\tthis._scorer.scoreConstant(boost, docId, docResults);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"constant_score\": {\n\t\t\t\tlet tmpDocResults = this._getAll(query.filter.values, false);\n\t\t\t\tlet docs = Object.keys(tmpDocResults);\n\t\t\t\t// Add to each document a constant score.\n\t\t\t\tfor (let i = 0; i < docs.length; i++) {\n\t\t\t\t\tthis._scorer.scoreConstant(boost, docs[i], docResults);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"prefix\": {\n\t\t\t\tlet termIdx = InvertedIndex.getTermIndex(query.value, root);\n\t\t\t\tif (termIdx !== null) {\n\t\t\t\t\ttermIdx = InvertedIndex.extendTermIndex(termIdx);\n\t\t\t\t\tfor (let i = 0; i < termIdx.length; i++) {\n\t\t\t\t\t\tthis._scorer.prepare(fieldName, boost, termIdx[i].index, doScoring && enableScoring, docResults, query.value + termIdx[i].term);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"exists\": {\n\t\t\t\tif (root !== null) {\n\t\t\t\t\tlet docs = Object.keys(this._invIdxs[fieldName].documentStore);\n\t\t\t\t\tfor (let i = 0; i < docs.length; i++) {\n\t\t\t\t\t\tthis._scorer.scoreConstant(boost, docs[i], docResults);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"match\": {\n\t\t\t\tlet terms = tokenizer.tokenize(query.value);\n\t\t\t\tlet operator = query.operator !== undefined ? query.operator : \"or\";\n\n\t\t\t\tlet tmpQuery = new QueryBuilder().bool();\n\t\t\t\tif (operator === \"or\") {\n\t\t\t\t\tif (query.minimum_should_match !== undefined) {\n\t\t\t\t\t\ttmpQuery = tmpQuery.minimumShouldMatch(query.minimum_should_match);\n\t\t\t\t\t}\n\t\t\t\t\t// Build a should query.\n\t\t\t\t\ttmpQuery = tmpQuery.beginShould();\n\t\t\t\t} else {\n\t\t\t\t\t// Build a must query.\n\t\t\t\t\ttmpQuery = tmpQuery.beginMust();\n\t\t\t\t}\n\t\t\t\ttmpQuery = tmpQuery.boost(boost);\n\n\t\t\t\tif (query.fuzziness !== undefined) {\n\t\t\t\t\tlet prefixLength = query.prefix_length !== undefined ? query.prefix_length : 2;\n\t\t\t\t\t// Add each fuzzy.\n\t\t\t\t\tfor (let i = 0; i < terms.length; i++) {\n\t\t\t\t\t\ttmpQuery = tmpQuery.fuzzy(fieldName, terms[i]).fuzziness(query.fuzziness).prefixLength(prefixLength);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// Add each term.\n\t\t\t\t\tfor (let i = 0; i < terms.length; i++) {\n\t\t\t\t\t\ttmpQuery = tmpQuery.term(fieldName, terms[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (operator === \"or\") {\n\t\t\t\t\ttmpQuery = tmpQuery.endShould();\n\t\t\t\t} else {\n\t\t\t\t\ttmpQuery = tmpQuery.endMust();\n\t\t\t\t}\n\t\t\t\tdocResults = this._recursive(tmpQuery.build().query, doScoring);\n\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t}\n\t\treturn docResults;\n\t}\n\n\t_getUnique(values, doScoring, docResults) {\n\t\tif (values.length === 0) {\n\t\t\treturn docResults;\n\t\t}\n\n\t\tfor (let i = 0; i < values.length; i++) {\n\t\t\tlet currDocs = this._recursive(values[i], doScoring);\n\t\t\tif (docResults === null) {\n\t\t\t\tdocResults = this._recursive(values[0], doScoring);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tlet docs = Object.keys(docResults);\n\t\t\tfor (let j = 0, docId; j < docs.length, docId = docs[j]; j++) {\n\t\t\t\tif (currDocs[docId] === undefined) {\n\t\t\t\t\tdelete docResults[docId];\n\t\t\t\t} else {\n\t\t\t\t\tdocResults[docId].push(...currDocs[docId]);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn docResults;\n\t}\n\n\t_getAll(values, doScoring) {\n\t\tlet docResults = {};\n\t\tfor (let i = 0; i < values.length; i++) {\n\t\t\tlet currDocs = this._recursive(values[i], doScoring);\n\t\t\tlet docs = Object.keys(currDocs);\n\t\t\tfor (let j = 0, docId; j < docs.length, docId = docs[j]; j++) {\n\t\t\t\tif (docResults[docId] === undefined) {\n\t\t\t\t\tdocResults[docId] = currDocs[docId];\n\t\t\t\t} else {\n\t\t\t\t\tdocResults[docId].push(...currDocs[docId]);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn docResults;\n\t}\n}\n\n\nclass FuzzySearch {\n\tconstructor(query) {\n\t\tthis._fuzzy = query.value;\n\t\tthis._fuzziness = query.fuzziness !== undefined ? query.fuzziness : \"AUTO\";\n\t\tif (this._fuzziness === \"AUTO\") {\n\t\t\tif (this._fuzzy.length <= 2) {\n\t\t\t\tthis._fuzziness = 0;\n\t\t\t} else if (this._fuzzy.length <= 5) {\n\t\t\t\tthis._fuzziness = 1;\n\t\t\t} else {\n\t\t\t\tthis._fuzziness = 2;\n\t\t\t}\n\t\t}\n\t\tthis._prefixLength = query.prefix_length !== undefined ? query.prefix_length : 2;\n\t}\n\n\t/**\n\t * Copyright Kigiri: https://github.com/kigiri\n\t *                     Milot Mirdita: https://github.com/milot-mirdita\n\t *                     Toni Neubert:  https://github.com/Viatorus/\n\t */\n\tlevenshtein_distance(a, b) {\n\t\tif (a.length === 0) return b.length;\n\t\tif (b.length === 0) return a.length;\n\t\tlet tmp;\n\t\tlet i;\n\t\tlet j;\n\t\tlet prev;\n\t\tlet val;\n\t\t// swap to save some memory O(min(a,b)) instead of O(a)\n\t\tif (a.length > b.length) {\n\t\t\ttmp = a;\n\t\t\ta = b;\n\t\t\tb = tmp;\n\t\t}\n\n\t\tconst row = Array(a.length + 1);\n\t\t// init the row\n\t\tfor (i = 0; i <= a.length; i++) {\n\t\t\trow[i] = i;\n\t\t}\n\n\t\t// fill in the rest\n\t\tfor (i = 1; i <= b.length; i++) {\n\t\t\tprev = i;\n\t\t\tfor (j = 1; j <= a.length; j++) {\n\t\t\t\tif (b[i - 1] === a[j - 1]) {\t// match\n\t\t\t\t\tval = row[j - 1];\n\t\t\t\t} else {\n\t\t\t\t\tval = Math.min(row[j - 1] + 1, // substitution\n\t\t\t\t\t\tMath.min(prev + 1,         // insertion\n\t\t\t\t\t\t\trow[j] + 1));          // deletion\n\n\t\t\t\t\t// transposition.\n\t\t\t\t\tif (i > 1 && j > 1 && b[i - 2] === a[j - 1] && a[j - 2] === b[i - 1]) {\n\t\t\t\t\t\tval = Math.min(val, row[j - 1] - (a[j - 1] === b[i - 1] ? 1 : 0));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trow[j - 1] = prev;\n\t\t\t\tprev = val;\n\t\t\t}\n\t\t\trow[a.length] = prev;\n\t\t}\n\t\treturn row[a.length];\n\t}\n\n\t/**\n\t * Performs a fuzzy search for a given term.\n\t * @param {string} query - a fuzzy term to match.\n\t * @param {number} [maxDistance=2] - maximal edit distance between terms\n\t * @returns {Array} - array with all matching term indices.\n\t */\n\tsearch(root) {\n\t\t// Todo: Include levenshtein to reduce similar iterations.\n\t\t// Tree tokens at same depth share same row until depth (should works if recursive).\n\t\t// Pregenerate tree token ?\n\t\t// var treeToken = Array(token.length + maxDistance);\n\n\t\tlet start = root;\n\t\tlet pre = this._fuzzy.slice(0, this._prefixLength);\n\t\tlet fuzzy = this._fuzzy;\n\t\tif (this._prefixLength !== 0) {\n\t\t\tstart = InvertedIndex.getTermIndex(pre, start);\n\t\t\tfuzzy = fuzzy.slice(this._prefixLength);\n\t\t}\n\t\tif (start === null) {\n\t\t\treturn [];\n\t\t}\n\t\tif (fuzzy.length === 0) {\n\t\t\t// Return if prefixLength == this._fuzzy length.\n\t\t\treturn [{term: this._fuzziness, index: start, boost: 1}];\n\t\t}\n\n\t\tlet similarTokens = [];\n\n\t\tlet stack = [start];\n\t\tlet treeStack = [''];\n\t\tdo {\n\t\t\tlet root = stack.pop();\n\t\t\tlet treeTerms = treeStack.pop();\n\n\t\t\t// Compare tokens if they are in near distance.\n\t\t\tif (root.df !== undefined && Math.abs(fuzzy.length - treeTerms.length) <= this._fuzziness) {\n\t\t\t\tconst distance = this.levenshtein_distance(fuzzy, treeTerms);\n\t\t\t\tif (distance <= this._fuzziness) {\n\t\t\t\t\tlet term = pre + treeTerms;\n\t\t\t\t\t// Calculate boost.\n\t\t\t\t\tlet boost = 1 - distance / Math.min(term.length, this._fuzzy.length);\n\t\t\t\t\tsimilarTokens.push({term, index: root, boost});\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Iterate over all subtrees.\n\t\t\t// If token from tree is not longer than maximal distance.\n\t\t\tif (treeTerms.length - fuzzy.length <= this._fuzziness) {\n\t\t\t\t// Iterate over all subtrees.\n\t\t\t\tlet keys = Object.keys(root);\n\t\t\t\tfor (let i = 0; i < keys.length; i++) {\n\t\t\t\t\tif (keys[i].length === 1) {\n\t\t\t\t\t\tstack.push(root[keys[i]]);\n\t\t\t\t\t\ttreeStack.push(treeTerms + keys[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t} while (stack.length !== 0);\n\n\t\treturn similarTokens;\n\t}\n}\n\nclass WildcardSearch {\n\n\tconstructor(query) {\n\t\tthis._wildcard = query.value;\n\t\tthis._result = [];\n\t}\n\n\t/**\n\t * Performs a wild card search for a given query term.\n\t * @param {string} query - a wild card query to match.\n\t * @returns {Array} - array with all matching term indices.\n\t */\n\tsearch(root) {\n\t\t// Todo: Need an implementation for star operator in the middle.\n\t\tthis._result = [];\n\t\tthis._recursive(root);\n\t\treturn this._result;\n\t}\n\n\t/**\n\t *\n\t * @param root\n\t * @param idx\n\t * @param term\n\t * @param escaped\n\t * @private\n\t */\n\t_recursive(root, idx = 0, term = '', escaped = false) {\n\t\tif (root === null) {\n\t\t\treturn;\n\t\t}\n\n\t\tif (idx === this._wildcard.length) {\n\t\t\tif (root.df !== undefined) {\n\t\t\t\tthis._result.push({index: root, term});\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\n\t\tif (!escaped && this._wildcard[idx] === '\\\\') {\n\t\t\tthis._recursive(root, idx + 1, term, true);\n\t\t} else if (!escaped && this._wildcard[idx] === '?') {\n\t\t\tlet others = InvertedIndex.getNextTermIndex(root);\n\t\t\tfor (let i = 0; i < others.length; i++) {\n\t\t\t\tthis._recursive(others[i].index, idx + 1, term + others[i].term);\n\t\t\t}\n\t\t} else if (!escaped && this._wildcard[idx] === '*') {\n\t\t\tlet all = InvertedIndex.extendTermIndex(root);\n\t\t\tfor (let i = 0; i < all.length; i++) {\n\t\t\t\tthis._recursive(all[i].index, idx + 1, term + all[i].term);\n\t\t\t}\n\t\t} else {\n\t\t\tthis._recursive(InvertedIndex.getTermIndex(this._wildcard[idx], root), idx + 1, term + this._wildcard[idx]);\n\t\t}\n\t}\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/inverted_index/index_searcher.js\n// module id = 8\n// module chunks = 0","/*\n * From MihaiValentin/lunr-languages.\n * Last update from 04/16/2017 - 19af41fb9bd644d9081ad274f96f700b21464290\n */\nexport function generateTrimmer(wordCharacters) {\n\tconst regex = new RegExp(\"^[^\" + wordCharacters + \"]+|[^\" + wordCharacters + \"]+$\", \"g\");\n\treturn (token) => token.replace(regex, '');\n}\n\nexport function generateStopWordFilter(stopWords) {\n\tconst words = new Set(stopWords);\n\treturn (token) => words.has(token) ? \"\" : token;\n}\n\nexport class Among {\n\tconstructor(s, substring_i, result, method) {\n\t\tthis.toCharArray = s => {\n\t\t\tlet sLength = s.length;\n\t\t\tlet charArr = new Array(sLength);\n\t\t\tfor (let i = 0; i < sLength; i++)\n\t\t\t\tcharArr[i] = s.charCodeAt(i);\n\t\t\treturn charArr;\n\t\t};\n\n\t\tif ((!s && s !== \"\") || (!substring_i && (substring_i !== 0)) || !result)\n\t\t\tthrow (\"Bad Among initialisation: s:\" + s + \", substring_i: \"\n\t\t\t+ substring_i + \", result: \" + result);\n\t\tthis.s_size = s.length;\n\t\tthis.s = this.toCharArray(s);\n\t\tthis.substring_i = substring_i;\n\t\tthis.result = result;\n\t\tthis.method = method;\n\t}\n}\n\nexport class SnowballProgram {\n\n\tconstructor() {\n\t\tthis.current = null;\n\t\tthis.bra = 0;\n\t\tthis.ket = 0;\n\t\tthis.limit = 0;\n\t\tthis.cursor = 0;\n\t\tthis.limit_backward = 0;\n\t}\n\n\tsetCurrent(word) {\n\t\tthis.current = word;\n\t\tthis.cursor = 0;\n\t\tthis.limit = word.length;\n\t\tthis.limit_backward = 0;\n\t\tthis.bra = this.cursor;\n\t\tthis.ket = this.limit;\n\t}\n\n\tgetCurrent() {\n\t\tlet result = this.current;\n\t\tthis.current = null;\n\t\treturn result;\n\t}\n\n\tin_grouping(s, min, max) {\n\t\tif (this.cursor < this.limit) {\n\t\t\tlet ch = this.current.charCodeAt(this.cursor);\n\t\t\tif (ch <= max && ch >= min) {\n\t\t\t\tch -= min;\n\t\t\t\tif (s[ch >> 3] & (0X1 << (ch & 0X7))) {\n\t\t\t\t\tthis.cursor++;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn false;\n\t}\n\n\tin_grouping_b(s, min, max) {\n\t\tif (this.cursor > this.limit_backward) {\n\t\t\tlet ch = this.current.charCodeAt(this.cursor - 1);\n\t\t\tif (ch <= max && ch >= min) {\n\t\t\t\tch -= min;\n\t\t\t\tif (s[ch >> 3] & (0X1 << (ch & 0X7))) {\n\t\t\t\t\tthis.cursor--;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn false;\n\t}\n\n\tout_grouping(s, min, max) {\n\t\tif (this.cursor < this.limit) {\n\t\t\tlet ch = this.current.charCodeAt(this.cursor);\n\t\t\tif (ch > max || ch < min) {\n\t\t\t\tthis.cursor++;\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\tch -= min;\n\t\t\tif (!(s[ch >> 3] & (0X1 << (ch & 0X7)))) {\n\t\t\t\tthis.cursor++;\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t\treturn false;\n\t}\n\n\tout_grouping_b(s, min, max) {\n\t\tif (this.cursor > this.limit_backward) {\n\t\t\tlet ch = this.current.charCodeAt(this.cursor - 1);\n\t\t\tif (ch > max || ch < min) {\n\t\t\t\tthis.cursor--;\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\tch -= min;\n\t\t\tif (!(s[ch >> 3] & (0X1 << (ch & 0X7)))) {\n\t\t\t\tthis.cursor--;\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t\treturn false;\n\t}\n\n\teq_s(s_size, s) {\n\t\tif (this.limit - this.cursor < s_size)\n\t\t\treturn false;\n\t\tfor (let i = 0; i < s_size; i++)\n\t\t\tif (this.current.charCodeAt(this.cursor + i) !== s.charCodeAt(i))\n\t\t\t\treturn false;\n\t\tthis.cursor += s_size;\n\t\treturn true;\n\t}\n\n\teq_s_b(s_size, s) {\n\t\tif (this.cursor - this.limit_backward < s_size)\n\t\t\treturn false;\n\t\tfor (let i = 0; i < s_size; i++)\n\t\t\tif (this.current.charCodeAt(this.cursor - s_size + i) !== s.charCodeAt(i))\n\t\t\t\treturn false;\n\t\tthis.cursor -= s_size;\n\t\treturn true;\n\t}\n\n\tfind_among(v, v_size) {\n\t\tlet i = 0;\n\t\tlet j = v_size;\n\t\tlet c = this.cursor;\n\t\tlet l = this.limit;\n\t\tlet common_i = 0;\n\t\tlet common_j = 0;\n\t\tlet first_key_inspected = false;\n\t\twhile (true) {\n\t\t\tlet k = i + ((j - i) >> 1);\n\t\t\tlet diff = 0;\n\n\t\t\tlet common = common_i < common_j\n\t\t\t\t? common_i\n\t\t\t\t: common_j;\n\n\t\t\tlet w = v[k];\n\t\t\tfor (let i2 = common; i2 < w.s_size; i2++) {\n\t\t\t\tif (c + common === l) {\n\t\t\t\t\tdiff = -1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdiff = this.current.charCodeAt(c + common) - w.s[i2];\n\t\t\t\tif (diff)\n\t\t\t\t\tbreak;\n\t\t\t\tcommon++;\n\t\t\t}\n\t\t\tif (diff < 0) {\n\t\t\t\tj = k;\n\t\t\t\tcommon_j = common;\n\t\t\t} else {\n\t\t\t\ti = k;\n\t\t\t\tcommon_i = common;\n\t\t\t}\n\t\t\tif (j - i <= 1) {\n\t\t\t\tif (i > 0 || j === i || first_key_inspected)\n\t\t\t\t\tbreak;\n\t\t\t\tfirst_key_inspected = true;\n\t\t\t}\n\t\t}\n\t\twhile (true) {\n\t\t\tlet w = v[i];\n\t\t\tif (common_i >= w.s_size) {\n\t\t\t\tthis.cursor = c + w.s_size;\n\t\t\t\tif (!w.method)\n\t\t\t\t\treturn w.result;\n\t\t\t\tlet res = w.method();\n\t\t\t\tthis.cursor = c + w.s_size;\n\t\t\t\tif (res)\n\t\t\t\t\treturn w.result;\n\t\t\t}\n\t\t\ti = w.substring_i;\n\t\t\tif (i < 0)\n\t\t\t\treturn 0;\n\t\t}\n\t}\n\n\tfind_among_b(v, v_size) {\n\t\tlet i = 0;\n\t\tlet j = v_size;\n\t\tlet c = this.cursor;\n\t\tlet lb = this.limit_backward;\n\t\tlet common_i = 0;\n\t\tlet common_j = 0;\n\t\tlet first_key_inspected = false;\n\t\twhile (true) {\n\t\t\tlet k = i + ((j - i) >> 1);\n\t\t\tlet diff = 0;\n\n\t\t\tlet common = common_i < common_j\n\t\t\t\t? common_i\n\t\t\t\t: common_j;\n\n\t\t\tlet w = v[k];\n\t\t\tfor (let i2 = w.s_size - 1 - common; i2 >= 0; i2--) {\n\t\t\t\tif (c - common === lb) {\n\t\t\t\t\tdiff = -1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdiff = this.current.charCodeAt(c - 1 - common) - w.s[i2];\n\t\t\t\tif (diff)\n\t\t\t\t\tbreak;\n\t\t\t\tcommon++;\n\t\t\t}\n\t\t\tif (diff < 0) {\n\t\t\t\tj = k;\n\t\t\t\tcommon_j = common;\n\t\t\t} else {\n\t\t\t\ti = k;\n\t\t\t\tcommon_i = common;\n\t\t\t}\n\t\t\tif (j - i <= 1) {\n\t\t\t\tif (i > 0 || j === i || first_key_inspected)\n\t\t\t\t\tbreak;\n\t\t\t\tfirst_key_inspected = true;\n\t\t\t}\n\t\t}\n\t\twhile (true) {\n\t\t\tlet w = v[i];\n\t\t\tif (common_i >= w.s_size) {\n\t\t\t\tthis.cursor = c - w.s_size;\n\t\t\t\tif (!w.method)\n\t\t\t\t\treturn w.result;\n\t\t\t\tlet res = w.method();\n\t\t\t\tthis.cursor = c - w.s_size;\n\t\t\t\tif (res)\n\t\t\t\t\treturn w.result;\n\t\t\t}\n\t\t\ti = w.substring_i;\n\t\t\tif (i < 0)\n\t\t\t\treturn 0;\n\t\t}\n\t}\n\n\treplace_s(c_bra, c_ket, s) {\n\t\tlet adjustment = s.length - (c_ket - c_bra);\n\n\t\tlet left = this.current\n\t\t\t.substring(0, c_bra);\n\n\t\tlet right = this.current.substring(c_ket);\n\t\tthis.current = left + s + right;\n\t\tthis.limit += adjustment;\n\t\tif (this.cursor >= c_ket)\n\t\t\tthis.cursor += adjustment;\n\t\telse if (this.cursor > c_bra)\n\t\t\tthis.cursor = c_bra;\n\t\treturn adjustment;\n\t}\n\n\tslice_check() {\n\t\tif (this.bra < 0 || this.bra > this.ket || this.ket > this.limit\n\t\t\t|| this.limit > this.current.length)\n\t\t\tthrow (\"faulty slice operation\");\n\t}\n\n\tslice_from(s) {\n\t\tthis.slice_check();\n\t\tthis.replace_s(this.bra, this.ket, s);\n\t}\n\n\tslice_del() {\n\t\tthis.slice_from(\"\");\n\t}\n\n\tinsert(c_bra, c_ket, s) {\n\t\tlet adjustment = this.replace_s(c_bra, c_ket, s);\n\t\tif (c_bra <= this.bra)\n\t\t\tthis.bra += adjustment;\n\t\tif (c_bra <= this.ket)\n\t\t\tthis.ket += adjustment;\n\t}\n\n\tslice_to() {\n\t\tthis.slice_check();\n\t\treturn this.current.substring(this.bra, this.ket);\n\t}\n\n\teq_v_b(s) {\n\t\treturn this.eq_s_b(s.length, s);\n\t}\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/inverted_index/language/support.js\n// module id = 9\n// module chunks = 0"],"sourceRoot":""}