{"version":3,"sources":["webpack:///webpack/universalModuleDefinition","webpack:///loki.FullTextSearch.min.js","webpack:///webpack/bootstrap ddc8f5cab96806dc262d","webpack:///./src/inverted_index/tokenizer.js","webpack:///./src/inverted_index/utils.js","webpack:///./src/inverted_index/queries.js","webpack:///./src/inverted_index/inverted_index.js","webpack:///./src/inverted_index/full_text_search.js","webpack:///./src/inverted_index/index_searcher.js","webpack:///./src/inverted_index/scorer.js"],"names":["root","factory","exports","module","define","amd","this","modules","__webpack_require__","moduleId","installedModules","i","l","call","m","c","value","d","name","getter","o","Object","defineProperty","configurable","enumerable","get","n","__esModule","object","property","prototype","hasOwnProperty","p","s","__webpack_exports__","defaultSplitter","str","trimmedTokens","tokens","split","length","push","toLowerCase","__WEBPACK_IMPORTED_MODULE_0__utils_js__","Tokenizer","[object Object]","_splitter","_queue","_symbol","Symbol","reset","label","func","TypeError","Error","labelFunc","_getPosition","pos","_addFunction","splice","serialized","tokenizers","splitter","funcTok","tokenizer","undefined","getSplitter","setSplitter","has","add","splitters","indexOf","String","isFunction","x","toString","isObject","isNumber","isBoolean","isString","isConvertibleToString","asBoolean","error","Boolean","asString","asArrayOfString","Array","isArray","array","BaseQuery","type","data","_data","boost","TermQuery","field","term","super","TermsQuery","terms","WildcardQuery","wildcard","FuzzyQuery","fuzzy","fuzziness","prefixLength","prefix_length","PrefixQuery","prefix","ExistsQuery","MatchQuery","query","minShouldMatch","operator","SyntaxError","minimum_should_match","op","MatchAllQuery","ArrayQuery","callbackName","callback","values","_callbackName","_prepare","queryType","args","bool","constantScore","match","matchAll","exists","BoolQuery","ConstantScoreQuery","filter","must","should","not","QueryBuilder","useBM25","enabled","final_scoring","k1","b","scoring","_child","build","__WEBPACK_IMPORTED_MODULE_0__tokenizer__","InvertedIndex","store","_store","_tokenizer","_docCount","_docStore","_totalFieldLength","_root","documentCount","documentStore","totalFieldLength","docId","fieldTokens","tokenize","termRefs","fieldLength","defineProperties","writable","Set","tf","j","branch","child","parent","docs","df","docStore","index","keys","k","key","start","termIndices","stack","treeStack","pop","treeTermn","regenerate","docIds","ref","self","dbObject","fromJSON","__WEBPACK_IMPORTED_MODULE_0__inverted_index__","__WEBPACK_IMPORTED_MODULE_1__index_searcher__","__WEBPACK_IMPORTED_MODULE_2__tokenizer__","__WEBPACK_IMPORTED_MODULE_3__utils_js__","FullTextSearch","fields","_invIdxs","_docs","_idxSearcher","doc","boosts","fieldNames","fieldName","insert","$loki","setDirty","remove","delete","removeDocument","addDocument","search","toJSON","db","JSON","parse","loadJSON","__WEBPACK_IMPORTED_MODULE_0__full_text_search__","__WEBPACK_IMPORTED_MODULE_1__tokenizer__","__WEBPACK_IMPORTED_MODULE_2__queries__","__WEBPACK_IMPORTED_MODULE_0__scorer__","__WEBPACK_IMPORTED_MODULE_1__inverted_index__","IndexSearcher","invIdxs","_scorer","docResults","_recursive","finalScore","doScoring","_getUnique","shouldDocs","_getAll","empty","msm","notDocs","termIdx","getTermIndex","prepare","f","FuzzySearch","w","WildcardSearch","a","scoreConstant","extendTermIndex","tmpQuery","minimumShouldMatch","startShould","startMust","endShould","endMust","currDocs","_fuzzy","_fuzziness","_prefixLength","tmp","prev","val","row","Math","min","pre","slice","similarTokens","treeTerms","abs","distance","levenshtein_distance","_wildcard","_result","idx","escaped","all","others","getNextTermIndex","Scorer","_cache","idf","_idf","result","docScore","docResult","res","pow","avgFieldLength","_avgFieldLength","tfNorm","idfs","docFreq","cache","_getCache","log"],"mappings":"CAAA,SAAAA,EAAAC,GACA,gBAAAC,UAAA,gBAAAC,QACAA,OAAAD,QAAAD,IACA,kBAAAG,gBAAAC,IACAD,OAAA,oBAAAH,GACA,gBAAAC,SACAA,QAAA,eAAAD,KAEAD,EAAA,KAAAA,EAAA,SAAmCA,EAAA,oBAAAC,MAClCK,KAAA,WACD,MCAgB,UAAUC,GCN1B,QAAAC,GAAAC,GAGA,GAAAC,EAAAD,GACA,MAAAC,GAAAD,GAAAP,OAGA,IAAAC,GAAAO,EAAAD,IACAE,EAAAF,EACAG,GAAA,EACAV,WAUA,OANAK,GAAAE,GAAAI,KAAAV,EAAAD,QAAAC,IAAAD,QAAAM,GAGAL,EAAAS,GAAA,EAGAT,EAAAD,QAvBA,GAAAQ,KA+DA,OAnCAF,GAAAM,EAAAP,EAGAC,EAAAO,EAAAL,EAGAF,EAAAG,EAAA,SAAAK,GAA2C,MAAAA,IAG3CR,EAAAS,EAAA,SAAAf,EAAAgB,EAAAC,GACAX,EAAAY,EAAAlB,EAAAgB,IACAG,OAAAC,eAAApB,EAAAgB,GACAK,cAAA,EACAC,YAAA,EACAC,IAAAN,KAMAX,EAAAkB,EAAA,SAAAvB,GACA,GAAAgB,GAAAhB,KAAAwB,WACA,WAA2B,MAAAxB,GAAA,SAC3B,WAAiC,MAAAA,GAEjC,OADAK,GAAAS,EAAAE,EAAA,IAAAA,GACAA,GAIAX,EAAAY,EAAA,SAAAQ,EAAAC,GAAsD,MAAAR,QAAAS,UAAAC,eAAAlB,KAAAe,EAAAC,IAGtDrB,EAAAwB,EAAA,GAGAxB,IAAAyB,EAAA,KDgBM,SAAU9B,EAAQ+B,EAAqB1B,GAE7C,YE1EA,SAAA2B,GAAAC,GACA,GAAAC,MACAC,EAAAF,EAAAG,MAAA,QACA,QAAA5B,GAAA,EAAgBA,EAAA2B,EAAAE,OAAmB7B,IACnC,KAAA2B,EAAA3B,IACA0B,EAAAI,KAAAH,EAAA3B,GAAA+B,cAGA,OAAAL,GFmEqB,GAAIM,GAA0CnC,EAAoB,QErDvFoC,GAKAC,cACAvC,KAAAwC,UAAA,KACAxC,KAAAyC,UACAzC,KAAA0C,QAAAC,OAAA,SACA3C,KAAA4C,QAUAL,YAAAM,EAAAC,GAEA,GADAD,EAAAR,EAAA,EAAAQ,IACAR,EAAA,EAAAS,GACA,KAAAC,WAAA,+BAEA,SAAAF,EACA,KAAAG,OAAA,yBAEAF,GAAA9C,KAAA0C,SAAAG,EACA7C,KAAAwC,UAAAM,EAOAP,cACA,OAAAvC,KAAAwC,UAAAxC,KAAA0C,SAAA1C,KAAAwC,WAMAD,gBACAvC,KAAAwC,UAAAX,EAQAU,IAAAU,GACA,WAAAjD,KAAAkD,aAAAD,GAUAV,IAAAU,GACA,GAAAE,GAAAnD,KAAAkD,aAAAD,EACA,SAAAE,EACA,KAAAH,OAAA,iCAEA,QAAAhD,KAAAyC,OAAAU,GAAAnD,KAAA0C,SAAA1C,KAAAyC,OAAAU,IAUAZ,IAAAM,EAAAC,GACA9C,KAAAoD,aAAAP,EAAAC,EAAA9C,KAAAyC,OAAAP,QAWAK,UAAAU,EAAAJ,EAAAC,GACA,GAAAK,GAAAnD,KAAAkD,aAAAD,EACA,SAAAE,EACA,KAAAH,OAAA,iCAEAhD,MAAAoD,aAAAP,EAAAC,EAAAK,GAWAZ,SAAAU,EAAAJ,EAAAC,GACA,GAAAK,GAAAnD,KAAAkD,aAAAD,EACA,SAAAE,EACA,KAAAH,OAAA,iCAEAhD,MAAAoD,aAAAP,EAAAC,EAAAK,EAAA,GAOAZ,OAAAU,GACA,GAAAE,GAAAnD,KAAAkD,aAAAD,EACA,SAAAE,EACA,KAAAH,OAAA,iCAEAhD,MAAAyC,OAAAY,OAAAF,EAAA,GAMAZ,QACAvC,KAAAwC,UAAAX,EACA7B,KAAAyC,UASAF,SAAAT,GACA,GAAAE,GAAAhC,KAAAwC,UAAAV,EACA,QAAAzB,GAAA,EAAiBA,EAAAL,KAAAyC,OAAAP,OAAwB7B,IACzC2B,EAAAhC,KAAAyC,OAAApC,GAAA2B,EAEA,OAAAA,GAQAO,SACA,GAAAe,IAAoBC,cACpBvD,MAAAwC,YAAAX,IACAyB,EAAAE,SAAAxD,KAAAwC,UAAAxC,KAAA0C,SAEA,QAAArC,GAAA,EAAiBA,EAAAL,KAAAyC,OAAAP,OAAwB7B,IACzCiD,EAAAC,WAAApB,KAAAnC,KAAAyC,OAAApC,GAAAL,KAAA0C,SAEA,OAAAY,GASAf,gBAAAe,EAAAG,GACA,GAAAC,GAAA,GAAApB,EAEA,QAAAqB,KAAAF,eAAAnB,GAAA,CACA,GAAAgB,EAAA7B,eAAA,aACA,GAAA+B,GAAAC,EAAAG,aACA,IAAAN,EAAAE,aAAA,GACA,KAAAR,OAAA,+BAEAU,GAAAG,YAAAL,EAAA,GAAAA,EAAA,IAGA,OAAAnD,GAAA,EAAkBA,EAAAiD,EAAAC,WAAArB,OAAkC7B,IAAA,CACpD,IAAAoD,EAAAK,IAAAR,EAAAC,WAAAlD,IACA,KAAA2C,OAAA,gCAEA,IAAAC,GAAAQ,EAAAtC,IAAAmC,EAAAC,WAAAlD,GACAqD,GAAAK,IAAAd,EAAA,GAAAA,EAAA,SAEG,CACH,GAAAK,EAAA7B,eAAA,aACA,IAAAgC,EAAAO,UAAAvC,eAAA6B,EAAAE,UACA,KAAAR,OAAA,+BAEAU,GAAAG,YAAAP,EAAAE,SAAAC,EAAAO,UAAAV,EAAAE,WAEA,OAAAnD,GAAA,EAAkBA,EAAAiD,EAAAC,WAAArB,OAAkC7B,IAAA,CACpD,IAAAoD,EAAAF,WAAA9B,eAAA6B,EAAAC,WAAAlD,IACA,KAAA2C,OAAA,gCAEAU,GAAAK,IAAAT,EAAAC,WAAAlD,GAAAoD,EAAAF,WAAAD,EAAAC,WAAAlD,MAGA,MAAAqD,GASAnB,aAAAU,GACA,GAAAZ,EAAA,EAAAY,GACA,MAAAjD,MAAAyC,OAAAwB,QAAAhB,EACG,KAAAZ,EAAA,EAAAY,GAQH,KAAAF,WAAA,gDAPAE,GAAAiB,OAAAjB,EACA,QAAA5C,GAAA,EAAkBA,EAAAL,KAAAyC,OAAAP,OAAwB7B,IAC1C,GAAAL,KAAAyC,OAAApC,GAAAL,KAAA0C,WAAAO,EACA,MAAA5C,EAMA,UAUAkC,aAAAM,EAAAC,EAAAK,GAEA,GADAN,EAAAR,EAAA,EAAAQ,IACAR,EAAA,EAAAS,GACA,KAAAC,WAAA,iCAEA,SAAAF,EACA,KAAAG,OAAA,yBAEAF,GAAA9C,KAAA0C,SAAAG,EACA7C,KAAAyC,OAAAY,OAAAF,EAAA,EAAAL,IAEAlB,EAAA,EAAAU,GF2FM,SAAUzC,EAAQ+B,EAAqB1B,GAE7C,YG1WA,SAAAiE,GAAAC,GACA,4BAAArD,OAAAS,UAAA6C,SAAA9D,KAAA6D,GASA,QAAAE,GAAAF,GACA,0BAAArD,OAAAS,UAAA6C,SAAA9D,KAAA6D,GASA,QAAAG,GAAAH,GACA,0BAAArD,OAAAS,UAAA6C,SAAA9D,KAAA6D,GASA,QAAAI,GAAAJ,GACA,2BAAArD,OAAAS,UAAA6C,SAAA9D,KAAA6D,GASA,QAAAK,GAAAL,GACA,0BAAArD,OAAAS,UAAA6C,SAAA9D,KAAA6D,GAQA,QAAAM,GAAAN,GACA,MAAAK,GAAAL,IAAAG,EAAAH,IAAAE,EAAAF,IAAArD,OAAAS,UAAA6C,WAAAD,EAAAC,UAAAI,EAAAL,EAAAC,YAWA,QAAAM,GAAAP,EAAAQ,EAAA7B,UAAA,wCACA,GAAAyB,EAAAJ,IAAAG,EAAAH,GACA,MAAAS,SAAAT,EAEA,MAAAQ,GAWA,QAAAE,GAAAV,EAAAQ,EAAA7B,UAAA,wCACA,GAAA2B,EAAAN,GACA,MAAAF,QAAAE,EAEA,MAAAQ,GAWA,QAAAG,GAAAX,EAAAQ,EAAA7B,UAAA,qDACA,IAAAiC,MAAAC,QAAAb,GACA,KAAAQ,EAEA,IAAAM,KACA,QAAA7E,GAAA,EAAgBA,EAAA+D,EAAAlC,OAAc7B,IAAA,CAC9B,IAAAqE,EAAAN,EAAA/D,IACA,KAAAuE,EAEAM,GAAA/C,KAAA+B,OAAAE,EAAA/D,KAEA,MAAA6E,GHqQiCtD,EAAuB,EAAIuC,EAE3BvC,EAAuB,EAAI2C,EAG3B3C,EAAuB,EAAI8C,EAC3B9C,EAAuB,EAAI+C,EAC3B/C,EAAuB,EAAIkD,EGxX5DlD,EAAA,EAAAmD,GH4eM,SAAUlF,EAAQ+B,EAAqB1B,GAE7C,YI9eA,IAAAmC,GAAAnC,EAAA,QAYAiF,GACA5C,YAAA6C,EAAAC,MACArF,KAAAsF,MAAAD,EACArF,KAAAsF,MAAAF,KAAA/C,EAAA,EAAA+C,GAYA7C,MAAA7B,GACA,IAAA2B,EAAA,EAAA3B,MAAA,EACA,KAAAqC,WAAA,mCAGA,OADA/C,MAAAsF,MAAAC,MAAA7E,EACAV,KAOAuC,QACA,MAAAvC,MAAAsF,YAqBAE,UAAAL,GACA5C,YAAAkD,EAAAC,EAAAL,MACAM,MAAA,OAAAN,GACArF,KAAAsF,MAAAG,MAAApD,EAAA,EAAAoD,GACAzF,KAAAsF,MAAA5E,MAAA2B,EAAA,EAAAqD,SAqBAE,UAAAT,GACA5C,YAAAkD,EAAAI,EAAAR,MACAM,MAAA,QAAAN,GACArF,KAAAsF,MAAAG,MAAApD,EAAA,EAAAoD,GACAzF,KAAAsF,MAAA5E,MAAA2B,EAAA,EAAAwD,SA8BAC,UAAAX,GACA5C,YAAAkD,EAAAM,EAAAV,MACAM,MAAA,WAAAN,GACArF,KAAAsF,MAAAG,MAAApD,EAAA,EAAAoD,GACAzF,KAAAsF,MAAA5E,MAAA2B,EAAA,EAAA0D,SA+BAC,UAAAb,GACA5C,YAAAkD,EAAAQ,EAAAZ,MACAM,MAAA,QAAAN,GACArF,KAAAsF,MAAAG,MAAApD,EAAA,EAAAoD,GACAzF,KAAAsF,MAAA5E,MAAA2B,EAAA,EAAA4D,GAQA1D,UAAA2D,GACA,IAAA7D,EAAA,EAAA6D,MAAA,EACA,KAAAnD,WAAA,uCAGA,OADA/C,MAAAsF,MAAAY,YACAlG,KAQAuC,aAAA4D,GACA,IAAA9D,EAAA,EAAA8D,MAAA,EACA,KAAApD,WAAA,2CAGA,OADA/C,MAAAsF,MAAAc,cAAAD,EACAnG,WAqBAqG,UAAAlB,GACA5C,YAAAkD,EAAAa,EAAAjB,MACAM,MAAA,SAAAN,GACArF,KAAAsF,MAAAG,MAAApD,EAAA,EAAAoD,GACAzF,KAAAsF,MAAA5E,MAAA2B,EAAA,EAAAiE,SAmBAC,UAAApB,GACA5C,YAAAkD,EAAAJ,MACAM,MAAA,SAAAN,GACArF,KAAAsF,MAAAG,MAAApD,EAAA,EAAAoD,SAiCAe,UAAArB,GACA5C,YAAAkD,EAAAgB,EAAApB,MACAM,MAAA,QAAAN,GACArF,KAAAsF,MAAAG,MAAApD,EAAA,EAAAoD,GACAzF,KAAAsF,MAAA5E,MAAA2B,EAAA,EAAAoE,GAQAlE,mBAAAmE,GACA,IAAArE,EAAA,EAAAqE,MAAA,EACA,KAAA3D,WAAA,4DAEA,IAAA/C,KAAAsF,MAAA7D,eAAA,oBAAAzB,KAAAsF,MAAAqB,SACA,KAAAC,aAAA,yEAGA,OADA5G,MAAAsF,MAAAuB,qBAAAH,EACA1G,KAQAuC,SAAAuE,GAEA,WADAA,EAAAzE,EAAA,EAAAyE,KACA,MAAAA,EACA,KAAAF,aAAA,oBAGA,IADA5G,KAAAsF,MAAAqB,SAAAG,EACA9G,KAAAsF,MAAA7D,eAAA,gCAAAzB,KAAAsF,MAAAqB,SACA,KAAAC,aAAA,yEAEA,OAAA5G,MAQAuC,UAAA2D,GACA,IAAA7D,EAAA,EAAA6D,MAAA,EACA,KAAAnD,WAAA,uCAGA,OADA/C,MAAAsF,MAAAY,YACAlG,KAQAuC,aAAA4D,GACA,IAAA9D,EAAA,EAAA8D,MAAA,EACA,KAAApD,WAAA,2CAGA,OADA/C,MAAAsF,MAAAc,cAAAD,EACAnG,WAuBA+G,UAAA5B,GACA5C,YAAA8C,MACAM,MAAA,YAAAN,SAQA2B,UAAA7B,GACA5C,YAAA0E,EAAAC,EAAA7B,MACAM,MAAA,QAAAN,GACArF,KAAAsF,MAAA6B,UACAnH,KAAAoH,cAAAH,EACAjH,KAAAiH,GAAAC,EAEAlH,KAAAqH,SAAA,EAAAC,KAAAC,KACA,GAAAlC,KACA,IAAAoB,GAAA,GAAAa,MAAAC,EAAAlC,EACArF,MAAAsF,MAAA6B,OAAAhF,KAAAkD,EACAoB,GAAAe,KAAAxH,KAAAwH,IACAf,GAAAgB,cAAAzH,KAAAyH,aACAhB,GAAAf,KAAA1F,KAAA0F,IACAe,GAAAZ,MAAA7F,KAAA6F,KACAY,GAAAV,SAAA/F,KAAA+F,QACAU,GAAAR,MAAAjG,KAAAiG,KACAQ,GAAAiB,MAAA1H,KAAA0H,KACAjB,GAAAkB,SAAA3H,KAAA2H,QACAlB,GAAAH,OAAAtG,KAAAsG,MACAG,GAAAmB,OAAA5H,KAAA4H,MACAnB,GAAAY,SAAArH,KAAAqH,QACAZ,GAAAzG,KAAAoH,eAAApH,UAAAoH,cACA,OAAAX,KAIAlE,OACA,MAAAvC,MAAAqH,SAAAQ,GAGAtF,gBACA,MAAAvC,MAAAqH,SAAAS,GAGAvF,KAAAkD,EAAAC,GACA,MAAA1F,MAAAqH,SAAA7B,EAAAC,EAAAC,GAGAnD,MAAAkD,EAAAI,GACA,MAAA7F,MAAAqH,SAAAzB,EAAAH,EAAAI,GAGAtD,SAAAkD,EAAAM,GACA,MAAA/F,MAAAqH,SAAAvB,EAAAL,EAAAM,GAGAxD,MAAAkD,EAAAQ,GACA,MAAAjG,MAAAqH,SAAArB,EAAAP,EAAAQ,GAGA1D,MAAAkD,EAAAgB,GACA,MAAAzG,MAAAqH,SAAAb,EAAAf,EAAAgB,GAGAlE,WACA,MAAAvC,MAAAqH,SAAAN,GAGAxE,OAAAkD,EAAAa,GACA,MAAAtG,MAAAqH,SAAAhB,EAAAZ,EAAAa,GAGA/D,OAAAkD,GACA,MAAAzF,MAAAqH,SAAAd,EAAAd,SAwBAqC,UAAA3C,GACA5C,YAAA8C,MACAM,MAAA,iBAAAN,GAOA9C,cAEA,MADAvC,MAAAsF,MAAAyC,UACA,GAAAf,GAAA,gBACAhH,KACGA,KAAAsF,MAAAyC,cA+CHF,UAAA1C,GACA5C,YAAA8C,MACAM,MAAA,OAAAN,GAOA9C,YAEA,MADAvC,MAAAsF,MAAA0C,QACA,GAAAhB,GAAA,cACAhH,KACGA,KAAAsF,MAAA0C,MAOHzF,cAEA,MADAvC,MAAAsF,MAAAyC,UACA,GAAAf,GAAA,gBACAhH,KACGA,KAAAsF,MAAAyC,QAOHxF,cAEA,MADAvC,MAAAsF,MAAA2C,UACA,GAAAjB,GAAA,gBACAhH,KACGA,KAAAsF,MAAA2C,QAOH1F,WAEA,MADAvC,MAAAsF,MAAA4C,OACA,GAAAlB,GAAA,aACAhH,KACGA,KAAAsF,MAAA4C,KAQH3F,mBAAAmE,GACA,uBAAAA,EAAA,EACA,KAAA3D,WAAA,2DAGA,OADA/C,MAAAsF,MAAAuB,qBAAAH,EACA1G,WAuBAmI,GACA5F,cACAvC,KAAAsF,OAAgBmB,UAChBzG,KAAAoI,UAQA7F,mBAAA8F,GAEA,MADArI,MAAAsF,MAAAgD,cAAAjG,EAAA,EAAAgG,GACArI,KAeAuC,QAAAgG,EAAA,IAAAC,EAAA,KACA,IAAAnG,EAAA,EAAAkG,MAAA,EACA,KAAAxF,WAAA,sCAEA,KAAAV,EAAA,EAAAmG,MAAA,GAAAA,EAAA,EACA,KAAAzF,WAAA,sDAQA,OALA/C,MAAAsF,MAAAmD,SACArD,KAAA,OACAmD,KACAC,KAEAxI,KAGAuC,OACA,MAAAvC,MAAAqH,SAAAQ,GAGAtF,gBACA,MAAAvC,MAAAqH,SAAAS,GAGAvF,KAAAkD,EAAAC,GACA,MAAA1F,MAAAqH,SAAA7B,EAAAC,EAAAC,GAGAnD,MAAAkD,EAAAI,GACA,MAAA7F,MAAAqH,SAAAzB,EAAAH,EAAAI,GAGAtD,SAAAkD,EAAAM,GACA,MAAA/F,MAAAqH,SAAAvB,EAAAL,EAAAM,GAGAxD,MAAAkD,EAAAQ,GACA,MAAAjG,MAAAqH,SAAArB,EAAAP,EAAAQ,GAGA1D,MAAAkD,EAAAgB,GACA,MAAAzG,MAAAqH,SAAAb,EAAAf,EAAAgB,GAGAlE,WACA,MAAAvC,MAAAqH,SAAAN,GAGAxE,OAAAkD,EAAAa,GACA,MAAAtG,MAAAqH,SAAAhB,EAAAZ,EAAAa,GAGA/D,OAAAkD,GACA,MAAAzF,MAAAqH,SAAAd,EAAAd,GAGAlD,SAAA+E,KAAAC,GAKA,MAJAvH,MAAA0I,OAAA,GAAApB,MAAAC,EAAAvH,KAAAsF,MAAAmB,OACAzG,KAAA0I,OAAAC,WACA3I,KAAAsF,OAEAtF,KAAA0I,QAEA9G,EAAA,EAAAuG,GJ6gBM,SAAUtI,EAAQ+B,EAAqB1B,GAE7C,YACqB,IAAI0I,GAA2C1I,EAAoB,QKnrCxF2I,GAKAtG,YAAAuG,GAAA,EAAApF,EAAA,GAAAkF,GAAA,GACA5I,KAAA+I,OAAAD,EACA9I,KAAAgJ,WAAAtF,EACA1D,KAAAiJ,UAAA,EACAjJ,KAAAkJ,aACAlJ,KAAAmJ,kBAAA,EACAnJ,KAAAoJ,SAGAN,YACA,MAAA9I,MAAA+I,OAGArF,gBACA,MAAA1D,MAAAgJ,WAGAK,oBACA,MAAArJ,MAAAiJ,UAGAK,oBACA,MAAAtJ,MAAAkJ,UAGAK,uBACA,MAAAvJ,MAAAmJ,kBAGAzJ,WACA,MAAAM,MAAAoJ,MASA7G,OAAAkD,EAAA+D,EAAAjE,EAAA,GACA,GAAAvF,KAAAkJ,UAAAzH,eAAA+H,GACA,KAAAxG,OAAA,uBAGAhD,MAAAiJ,WAAA,EACAjJ,KAAAkJ,UAAAM,KAGA,IAAAC,GAAAzJ,KAAAgJ,WAAAU,SAAAjE,EACAzF,MAAAmJ,mBAAAM,EAAAvH,MAEA,IAAAyH,KACA3J,MAAAkJ,UAAAM,IAA2BI,YAAAH,EAAAvH,OAAAqD,SAC3BxE,OAAA8I,iBAAA7J,KAAAkJ,UAAAM,IACAG,UAAczI,YAAA,EAAAD,cAAA,EAAA6I,UAAA,EAAApJ,MAAAiJ,IAId,QAAAjE,KAAA,IAAAqE,KAAAN,GACA,QAAA/D,EAAA,CAIA,GAAAsE,GAAA,CACA,QAAAC,GAAA,EAAkBA,EAAAR,EAAAvH,OAAwB+H,IAC1CR,EAAAQ,KAAAvE,GACAsE,GAKA,IAAAE,GAAAlK,KAAAoJ,KACA,QAAA/I,GAAA,EAAkBA,EAAAqF,EAAAxD,OAAiB7B,IAAA,CACnC,GAAAI,GAAAiF,EAAArF,EACA,KAAA6J,EAAAzI,eAAAhB,GAAA,CACA,GAAA0J,KACApJ,QAAA8I,iBAAAM,GACAC,QAAelJ,YAAA,EAAAD,cAAA,EAAA6I,UAAA,EAAApJ,MAAAwJ,KAEfA,EAAAzJ,GAAA0J,EAEAD,IAAAzJ,GAGAyJ,EAAAzI,eAAA,UACAyI,EAAAG,QACAH,EAAAI,GAAA,GAEAJ,EAAAG,KAAAb,GAAAQ,EACAE,EAAAI,IAAA,EAGAX,EAAAxH,KAAA+H,IAQA3H,OAAAiH,GACA,GAAAxJ,KAAAkJ,UAAAzH,eAAAyC,OAAAsF,IAAA,CAGA,GAAAe,GAAAvK,KAAAkJ,UAAAM,SAEAxJ,MAAAkJ,UAAAM,GACAxJ,KAAAiJ,WAAA,EAGAjJ,KAAAmJ,mBAAAoB,EAAAX,WAIA,IAAAD,GAAAY,EAAAZ,QACA,QAAAM,GAAA,EAAiBA,EAAAN,EAAAzH,OAAqB+H,IAAA,CACtC,GAAAO,GAAAb,EAAAM,EAKA,IAJAO,EAAAF,IAAA,QACAE,GAAAH,KAAAb,GAGA,IAAAgB,EAAAF,GAAA,CACA,GAAAG,KACA,IAEA,GAAAL,GAAAI,EAAAJ,aAEAI,GAAAJ,OAGAK,EAAA1J,OAAA0J,KAAAL,EACA,QAAAM,GAAA,EAAoBA,EAAAD,EAAAvI,OAAiBwI,IAAA,CACrC,GAAAC,GAAAF,EAAAC,EACA,WAAAC,GAAA,SAAAA,GAIAP,EAAAO,KAAAH,EAAA,OACAJ,GAAAO,EACA,QAGAH,EAAAJ,QACKI,EAAA/I,eAAA,eAAAgJ,EAAAvI,WAYLK,oBAAAmD,EAAAhG,EAAAkL,EAAA,GACA,GAAAA,GAAAlF,EAAAxD,OACA,WAEA,QAAA7B,GAAAuK,EAAqBvK,EAAAqF,EAAAxD,OAAiB7B,IAAA,CACtC,IAAAX,EAAA+B,eAAAiE,EAAArF,IACA,WAEAX,KAAAgG,EAAArF,IAEA,MAAAX,GAQA6C,wBAAA7C,GACA,GAAAmL,MACAJ,EAAA1J,OAAA0J,KAAA/K,EACA,QAAAW,GAAA,EAAiBA,EAAAoK,EAAAvI,OAAiB7B,IAClC,SAAAoK,EAAApK,IAAA,OAAAoK,EAAApK,IACAwK,EAAA1I,MAAsBqI,MAAA9K,EAAA+K,EAAApK,IAAAqF,KAAA+E,EAAApK,IAGtB,OAAAwK,GAQAtI,uBAAA7C,GACA,GAAAmL,MACAC,GAAApL,GACAqL,GAAA,GACA,IACA,GAAArL,GAAAoL,EAAAE,MACAC,EAAAF,EAAAC,KAEAtL,GAAA+B,eAAA,OACAoJ,EAAA1I,MAAsBqI,MAAA9K,EAAAgG,KAAAuF,GAGtB,IAAAR,GAAA1J,OAAA0J,KAAA/K,EACA,QAAAW,GAAA,EAAkBA,EAAAoK,EAAAvI,OAAiB7B,IACnC,SAAAoK,EAAApK,IAAA,OAAAoK,EAAApK,KACAyK,EAAA3I,KAAAzC,EAAA+K,EAAApK,KACA0K,EAAA5I,KAAA8I,EAAAR,EAAApK,WAGG,IAAAyK,EAAA5I,OAEH,OAAA2I,GAOAtI,SACA,MAAAvC,MAAA+I,OACA/I,MAGAgJ,WAAAhJ,KAAAgJ,YAWAzG,SAAAe,EAAAG,GAWA,QAAAyH,GAAAV,EAAAJ,GAEA,OAAAA,GACArJ,OAAA8I,iBAAAW,GACAJ,QAAclJ,YAAA,EAAAD,cAAA,EAAA6I,UAAA,EAAApJ,MAAA0J,IAKd,IAAAK,GAAA1J,OAAA0J,KAAAD,EACA,QAAAnK,GAAA,EAAkBA,EAAAoK,EAAAvI,OAAiB7B,IAEnC,YAAAoK,EAAApK,GAAA,CAEA,GAAA8K,GAAApK,OAAA0J,KAAAD,EAAAH,KACA,QAAAJ,GAAA,EAAoBA,EAAAkB,EAAAjJ,OAAmB+H,IAAA,CAEvC,GAAAmB,GAAAC,EAAAnC,UAAAiC,EAAAlB,GACAmB,GAAA3J,eAAA,aACAV,OAAA8I,iBAAAuB,GACAzB,UAAmBzI,YAAA,EAAAD,cAAA,EAAA6I,UAAA,EAAApJ,YAInB0K,EAAAzB,SAAAxH,KAAAqI,QAEK,OAAAC,EAAApK,IAEL6K,EAAAV,EAAAC,EAAApK,IAAAmK,GAtCA,GAAAc,GAAAhI,CAEAtD,MAAAgJ,WAAAJ,EAAA,EAAA2C,SAAAD,EAAAtC,WAAAvF,GACAzD,KAAAiJ,UAAAqC,EAAArC,UACAjJ,KAAAkJ,UAAAoC,EAAApC,UACAlJ,KAAAmJ,kBAAAmC,EAAAnC,kBACAnJ,KAAAoJ,MAAAkC,EAAAlC,KAEA,IAAAiC,GAAArL,IAmCAkL,GAAAlL,KAAAoJ,MAAA,OAEAxH,EAAA,EAAAiH,GLksCM,SAAUhJ,EAAQ+B,EAAqB1B,GAE7C,YACqB,IAAIsL,GAAgDtL,EAAoB,GACpEuL,EAAgDvL,EAAoB,GACpEwL,EAA2CxL,EAAoB,GAC/DyL,EAA0CzL,EAAoB,QMt+CvF0L,GAKArJ,YAAAsJ,GACA,OAAAlI,KAAAkI,EACA,SAAAjF,aAAA,8BAKA,IAFA5G,KAAA8L,aAEA9G,MAAAC,QAAA4G,GAoBA,SAAA9I,WAAA,0EAnBA,QAAA1C,GAAA,EAAkBA,EAAAwL,EAAA3J,OAAmB7B,IAAA,CACrC,GAAAoF,GAAAoG,EAAAxL,GACAO,EAAA+K,EAAA,EAAAlG,EAAA7E,KAAAmC,UAAA,qCAEA+F,GAAArD,EAAAhE,eAAA,UACAkK,EAAA,EAAAlG,EAAAqD,MAAA/F,UAAA,2CAEAW,EAAA,IACA,IAAA+B,EAAAhE,eAAA,cACA,KAAAgE,EAAA/B,oBAAAgI,GAAA,GACA,SAAA3I,WAAA,uDAEAW,GAAA+B,EAAA/B,cAEAA,GAAA,GAAAgI,GAAA,CAEA1L,MAAA8L,SAAAlL,GAAA,GAAA4K,GAAA,EAAA1C,EAAApF,GAMA1D,KAAA+L,MAAA,GAAAhC,KACA/J,KAAAgM,aAAA,GAAAP,GAAA,EAAAzL,KAAA8L,SAAA9L,KAAA+L,OAGAxJ,YAAA0J,EAAAC,MACA,IAAAD,EAAAxK,eAAA,SACA,SAAAuB,OAAA,4CAGA,IAAAmJ,GAAApL,OAAA0J,KAAAwB,EACA,QAAAG,GAAA/L,EAAA,EAA4B8L,EAAAjK,OAAAkK,EAAAD,EAAA9L,GAAkDA,IAC9E,GAAAL,KAAA8L,SAAArK,eAAA2K,GAAA,CACA,GAAA7G,GAAA2G,EAAAzK,eAAA2K,GAAAF,EAAAE,GAAA,CACApM,MAAA8L,SAAAM,GAAAC,OAAAJ,EAAAG,GAAAH,EAAAK,MAAA/G,GAIAvF,KAAA+L,MAAAhI,IAAAkI,EAAAK,OACAtM,KAAAuM,WAGAhK,eAAA0J,GACA,IAAAA,EAAAxK,eAAA,SACA,SAAAuB,OAAA,4CAGA,IAAAmJ,GAAApL,OAAA0J,KAAAzK,KAAA8L,SACA,QAAAzL,GAAA,EAAiBA,EAAA8L,EAAAjK,OAAuB7B,IACxCL,KAAA8L,SAAAK,EAAA9L,IAAAmM,OAAAP,EAAAK,MAGAtM,MAAA+L,MAAAU,OAAAR,EAAAK,OACAtM,KAAAuM,WAGAhK,eAAA0J,EAAAC,MACAlM,KAAA0M,eAAAT,GACAjM,KAAA2M,YAAAV,EAAAC,GAGA3J,OAAAkE,GACA,MAAAzG,MAAAgM,aAAAY,OAAAnG,GAGAlE,SACA,GAAAe,MACA6I,EAAApL,OAAA0J,KAAAzK,KAAA8L,SACA,QAAAM,GAAA/L,EAAA,EAA4B8L,EAAAjK,OAAAkK,EAAAD,EAAA9L,GAAkDA,IAC9EiD,EAAA8I,GAAApM,KAAA8L,SAAAM,GAAAS,QAEA,OAAAvJ,GAGAf,SAAAe,EAAAC,GACA,GAAAuJ,GAAAC,KAAAC,MAAA1J,GACA6I,EAAApL,OAAA0J,KAAAqC,EACA,QAAAV,GAAA/L,EAAA,EAA4B8L,EAAAjK,OAAAkK,EAAAD,EAAA9L,GAAkDA,IAC9EL,KAAA8L,SAAAM,GAAA,GAAAZ,GAAA,EACAxL,KAAA8L,SAAAM,GAAAa,SAAAH,EAAAV,GAAA7I,EAAA6I,IAIA7J,WACAvC,KAAAgM,aAAAO,YAEA3K,EAAA,EAAAgK,GNm/CM,SAAU/L,EAAQ+B,EAAqB1B,GAE7C,YACAa,QAAOC,eAAeY,EAAqB,cAAgBlB,OAAO,GAC7C,IAAIwM,GAAkDhN,EAAoB,GACtEiN,EAA2CjN,EAAoB,GAC/DkN,EAAyClN,EAAoB,EACrDA,GAAoBS,EAAEiB,EAAqB,iBAAkB,WAAa,MAAOsL,GAAmD,IACpIhN,EAAoBS,EAAEiB,EAAqB,YAAa,WAAa,MAAOuL,GAA4C,IACxHjN,EAAoBS,EAAEiB,EAAqB,eAAgB,WAAa,MAAOwL,GAA0C,KAiBpJ,SAAUvN,EAAQ+B,EAAqB1B,GAE7C,YACqB,IAAImN,GAAwCnN,EAAoB,GAC5DoN,EAAgDpN,EAAoB,GACpEkN,EAAyClN,EAAoB,QOvnDtFqN,GAKAhL,YAAAiL,EAAAnD,GACArK,KAAA8L,SAAA0B,EACAxN,KAAA+L,MAAA1B,EACArK,KAAAyN,QAAA,GAAAJ,GAAA,EAAArN,KAAA8L,UAGAvJ,OAAAkE,GACA,GAAAiH,GAAA1N,KAAA2N,WAAAlH,SAAA,EAIA,QADAA,EAAAhF,eAAA,kBAAAgF,EAAA6B,cAEAtI,KAAAyN,QAAAG,WAAAnH,EAAAiH,GAEAA,EAGAnL,WACAvC,KAAAyN,QAAAlB,WAGAhK,WAAAkE,EAAAoH,GACA,GAAAH,MACAnI,EAAAkB,EAAAhF,eAAA,SAAAgF,EAAAlB,MAAA,EACA6G,EAAA3F,EAAAhF,eAAA,SAAAgF,EAAAhB,MAAA,KAEA/F,EAAA,KACAgE,EAAA,IAMA,QALA1D,KAAA8L,SAAArK,eAAA2K,KACA1M,EAAAM,KAAA8L,SAAAM,GAAA1M,KACAgE,EAAA1D,KAAA8L,SAAAM,GAAA1I,WAGA+C,EAAArB,MACA,WASA,GARAsI,EAAA,KACAjH,EAAAhF,eAAA,UACAiM,EAAA1N,KAAA8N,WAAArH,EAAAuB,KAAAb,OAAA0G,EAAAH,IAEAjH,EAAAhF,eAAA,YACAiM,EAAA1N,KAAA8N,WAAArH,EAAAsB,OAAAZ,QAAA,EAAAuG,IAGAjH,EAAAhF,eAAA,WACA,GAAAsM,GAAA/N,KAAAgO,QAAAvH,EAAAwB,OAAAd,OAAA0G,GAEAI,GAAA,CACA,QAAAP,IACAA,KACAO,GAAA,EAGA,IAAAC,GAAAzH,EAAAhF,eAAA,wBAAAgF,EAAAI,qBAAA,EAGAwD,EAAAtJ,OAAA0J,KAAAsD,EACA,QAAAvE,GAAAnJ,EAAA,EAA2BgK,EAAAnI,OAAAsH,EAAAa,EAAAhK,GAAkCA,IAC7D0N,EAAAvE,GAAAtH,QAAAgM,IACAR,EAAAjM,eAAA+H,GACAkE,EAAAlE,GAAArH,QAAA4L,EAAAvE,IACQyE,EACRP,EAAAlE,GAAAuE,EAAAvE,SAEAkE,GAAAlE,IAKA,GAAA/C,EAAAhF,eAAA,QACA,GAAA0M,GAAAnO,KAAAgO,QAAAvH,EAAAyB,IAAAf,QAAA,GAEAkD,EAAAtJ,OAAA0J,KAAA0D,EACA,QAAA3E,GAAAnJ,EAAA,EAA2BgK,EAAAnI,OAAAsH,EAAAa,EAAAhK,GAAkCA,IAC7DqN,EAAAjM,eAAA+H,UACAkE,GAAAlE,GAIA,KAEA,aACA,GAAA4E,GAAAd,EAAA,EAAAe,aAAA5H,EAAA/F,MAAAhB,EACAM,MAAAyN,QAAAa,QAAAlC,EAAA7G,EAAA6I,EAAAP,EAAAH,EAAAjH,EAAA/F,MACA,OAEA,YACA,OAAAL,GAAA,EAAmBA,EAAAoG,EAAA/F,MAAAwB,OAAwB7B,IAAA,CAC3C,GAAA+N,GAAAd,EAAA,EAAAe,aAAA5H,EAAA/F,MAAAL,GAAAX,EACAM,MAAAyN,QAAAa,QAAAlC,EAAA7G,EAAA6I,EAAAP,EAAAH,EAAAjH,EAAA/F,MAAAL,IAEA,KAEA,cACA,GAAAkO,GAAA,GAAAC,GAAA/H,GACA+B,EAAA+F,EAAA3B,OAAAlN,EACA,QAAAW,GAAA,EAAmBA,EAAAmI,EAAAtG,OAAc7B,IACjCL,KAAAyN,QAAAa,QAAAlC,EAAA7G,EAAAiD,EAAAnI,GAAAkF,MAAAiD,EAAAnI,GAAAmK,MAAAqD,EAAAH,EAAAlF,EAAAnI,GAAAqF,KAEA,OAEA,gBACA,GAAA+I,GAAA,GAAAC,GAAAjI,GACAkI,EAAAF,EAAA7B,OAAAlN,EACA,QAAAW,GAAA,EAAmBA,EAAAsO,EAAAzM,OAAc7B,IACjCL,KAAAyN,QAAAa,QAAAlC,EAAA7G,EAAAoJ,EAAAtO,GAAAmK,MAAAqD,EAAAH,EAAAiB,EAAAtO,GAAAqF,KAEA,OAEA,gBACA,OAAA8D,KAAAxJ,MAAA+L,MACA/L,KAAAyN,QAAAmB,cAAArJ,EAAAiE,EAAAkE,EAEA,MAEA,uBACAA,EAAA1N,KAAAgO,QAAAvH,EAAAsB,OAAAZ,QAAA,EACA,IAAAkD,GAAAtJ,OAAA0J,KAAAiD,EAEA,QAAArN,GAAA,EAAmBA,EAAAgK,EAAAnI,OAAiB7B,IACpCL,KAAAyN,QAAAmB,cAAArJ,EAAA8E,EAAAhK,GAAAqN,EAEA,OAEA,cACA,GAAAU,GAAAd,EAAA,EAAAe,aAAA5H,EAAA/F,MAAAhB,EACA,OAAA0O,IACAA,EAAAd,EAAA,EAAAuB,gBAAAT,GAEA,QAAA/N,GAAA,EAAmBA,EAAA+N,EAAAlM,OAAoB7B,IACvCL,KAAAyN,QAAAa,QAAAlC,EAAA7G,EAAA6I,EAAA/N,GAAAmK,MAAAqD,EAAAH,EAAAjH,EAAA/F,MAAA0N,EAAA/N,GAAAqF,KAEA,OAEA,aACA,SAAAhG,EAAA,CACA,GAAA2K,GAAAtJ,OAAA0J,KAAAzK,KAAA8L,SAAAM,GAAA9C,cACA,QAAAjJ,GAAA,EAAoBA,EAAAgK,EAAAnI,OAAiB7B,IACrCL,KAAAyN,QAAAmB,cAAArJ,EAAA8E,EAAAhK,GAAAqN,GAGA,KAEA,cACA,GAAA7H,GAAAnC,EAAAgG,SAAAjD,EAAA/F,OACAiG,EAAAF,EAAAhF,eAAA,YAAAgF,EAAAE,SAAA,KAEAmI,GAAA,GAAA1B,GAAA,GAAA5F,MAaA,IAZA,OAAAb,GACAF,EAAAhF,eAAA,0BACAqN,IAAAC,mBAAAtI,EAAAI,uBAGAiI,IAAAE,eAGAF,IAAAG,YAEAH,IAAAvJ,SAEAkB,EAAAhF,eAAA,cACA,GAAA0E,GAAAM,EAAAhF,eAAA,iBAAAgF,EAAAL,cAAA,CAEA,QAAA/F,GAAA,EAAoBA,EAAAwF,EAAA3D,OAAkB7B,IACtCyO,IAAA7I,MAAAmG,EAAAvG,EAAAxF,IAAA6F,UAAAO,EAAAP,WAAAC,oBAIA,QAAA9F,GAAA,EAAoBA,EAAAwF,EAAA3D,OAAkB7B,IACtCyO,IAAApJ,KAAA0G,EAAAvG,EAAAxF,GAIAyO,GADA,OAAAnI,EACAmI,EAAAI,YAEAJ,EAAAK,UAGAzB,EAAA1N,KAAA2N,WAAAmB,EAAAnG,QAAAkF,EACA,QAKA,MAAAH,GAGAnL,WAAA4E,EAAA0G,EAAAH,GACA,OAAAvG,EAAAjF,OACA,MAAAwL,EAGA,QAAArN,GAAA,EAAiBA,EAAA8G,EAAAjF,OAAmB7B,IAAA,CACpC,GAAA+O,GAAApP,KAAA2N,WAAAxG,EAAA9G,GAAAwN,EACA,WAAAH,EAAA,CAKA,GAAArD,GAAAtJ,OAAA0J,KAAAiD,EACA,QAAAlE,GAAAS,EAAA,EAAyBI,EAAAnI,OAAAsH,EAAAa,EAAAJ,GAAkCA,IAC3DmF,EAAA3N,eAAA+H,GAGAkE,EAAAlE,GAAArH,QAAAiN,EAAA5F,UAFAkE,GAAAlE,OAPAkE,GAAA1N,KAAA2N,WAAAxG,EAAA,GAAA0G,GAaA,MAAAH,GAGAnL,QAAA4E,EAAA0G,GACA,GAAAH,KACA,QAAArN,GAAA,EAAiBA,EAAA8G,EAAAjF,OAAmB7B,IAAA,CACpC,GAAA+O,GAAApP,KAAA2N,WAAAxG,EAAA9G,GAAAwN,GACAxD,EAAAtJ,OAAA0J,KAAA2E,EACA,QAAA5F,GAAAS,EAAA,EAAyBI,EAAAnI,OAAAsH,EAAAa,EAAAJ,GAAkCA,IAC3DyD,EAAAjM,eAAA+H,GAGAkE,EAAAlE,GAAArH,QAAAiN,EAAA5F,IAFAkE,EAAAlE,GAAA4F,EAAA5F,GAMA,MAAAkE,IAEA9L,EAAA,EAAA2L,OAGAiB,GACAjM,YAAAkE,GACAzG,KAAAqP,OAAA5I,EAAA/F,MACAV,KAAAsP,WAAA7I,EAAAhF,eAAA,aAAAgF,EAAAP,UAAA,EACAlG,KAAAuP,cAAA9I,EAAAhF,eAAA,iBAAAgF,EAAAL,cAAA,EAQA7D,qBAAAoM,EAAAnG,GACA,OAAAmG,EAAAzM,OAAA,MAAAsG,GAAAtG,MACA,QAAAsG,EAAAtG,OAAA,MAAAyM,GAAAzM,MACA,IAAAsN,GAAAnP,EAAA4J,EAAAwF,EAAAC,CAEAf,GAAAzM,OAAAsG,EAAAtG,SACAsN,EAAAb,EACAA,EAAAnG,EACAA,EAAAgH,EAGA,IAAAG,GAAA3K,MAAA2J,EAAAzM,OAAA,EAEA,KAAA7B,EAAA,EAAaA,GAAAsO,EAAAzM,OAAe7B,IAC5BsP,EAAAtP,IAIA,KAAAA,EAAA,EAAaA,GAAAmI,EAAAtG,OAAe7B,IAAA,CAE5B,IADAoP,EAAApP,EACA4J,EAAA,EAAcA,GAAA0E,EAAAzM,OAAe+H,IAC7BzB,EAAAnI,EAAA,KAAAsO,EAAA1E,EAAA,GACAyF,EAAAC,EAAA1F,EAAA,IAEAyF,EAAAE,KAAAC,IAAAF,EAAA1F,EAAA,KACA2F,KAAAC,IAAAJ,EAAA,EACAE,EAAA1F,GAAA,IAGA5J,EAAA,GAAA4J,EAAA,GAAAzB,EAAAnI,EAAA,KAAAsO,EAAA1E,EAAA,IAAA0E,EAAA1E,EAAA,KAAAzB,EAAAnI,EAAA,KACAqP,EAAAE,KAAAC,IAAAH,EAAAC,EAAA1F,EAAA,IAAA0E,EAAA1E,EAAA,KAAAzB,EAAAnI,EAAA,WAGAsP,EAAA1F,EAAA,GAAAwF,EACAA,EAAAC,CAEAC,GAAAhB,EAAAzM,QAAAuN,EAEA,MAAAE,GAAAhB,EAAAzM,QASAK,OAAA7C,GAMA,GAAAkL,GAAAlL,EACAoQ,EAAA9P,KAAAqP,OAAAU,MAAA,EAAA/P,KAAAuP,eACAtJ,EAAAjG,KAAAqP,MAKA,IAJA,GAAArP,KAAAuP,gBACA3E,EAAA0C,EAAA,EAAAe,aAAAyB,EAAAlF,GACA3E,IAAA8J,MAAA/P,KAAAuP,gBAEA,OAAA3E,EACA,QAGA,IAAAoF,MAEAlF,GAAAF,GACAG,GAAA,GACA,IACA,GAAArL,GAAAoL,EAAAE,MACAiF,EAAAlF,EAAAC,KAGA,IAAAtL,EAAA+B,eAAA,OAAAmO,KAAAM,IAAAjK,EAAA/D,OAAA+N,EAAA/N,SAAAlC,KAAAsP,WAAA,CACA,KAAAa,GAAAnQ,KAAAoQ,qBAAAnK,EAAAgK,EACA,IAAAE,GAAAnQ,KAAAsP,WAAA,CAEA,GAAA/J,GAAA,EAAA4K,GAAAL,EAAA5N,OAAA+N,EAAA/N,OACA8N,GAAA7N,MAAyBuD,KAAAoK,EAAAG,EAAAzF,MAAA9K,EAAA6F,WAMzB,GAAA0K,EAAA/N,OAAA+D,EAAA/D,QAAAlC,KAAAsP,WAAA,CAEA,GAAA7E,GAAA1J,OAAA0J,KAAA/K,EACA,QAAAW,GAAA,EAAmBA,EAAAoK,EAAAvI,OAAiB7B,IACpC,SAAAoK,EAAApK,IAAA,OAAAoK,EAAApK,KACAyK,EAAA3I,KAAAzC,EAAA+K,EAAApK,KACA0K,EAAA5I,KAAA8N,EAAAxF,EAAApK,YAIG,IAAAyK,EAAA5I,OAEH,OAAA8N,SAIAtB,GAEAnM,YAAAkE,GACAzG,KAAAqQ,UAAA5J,EAAA/F,MACAV,KAAAsQ,WAQA/N,OAAA7C,GAIA,MAFAM,MAAAsQ,WACAtQ,KAAA2N,WAAAjO,GACAM,KAAAsQ,QAWA/N,WAAA7C,EAAA6Q,EAAA,EAAA7K,EAAA,GAAA8K,GAAA,GACA,UAAA9Q,EAAA,CAIA,GAAA6Q,IAAAvQ,KAAAqQ,UAAAnO,OAIA,YAHAxC,EAAA+B,eAAA,OACAzB,KAAAsQ,QAAAnO,MAAuBqI,MAAA9K,EAAAgG,SAKvB,IAAA8K,GAAA,OAAAxQ,KAAAqQ,UAAAE,GAEG,GAAAC,GAAA,MAAAxQ,KAAAqQ,UAAAE,GAKA,GAAAC,GAAA,MAAAxQ,KAAAqQ,UAAAE,GAMHvQ,KAAA2N,WAAAL,EAAA,EAAAe,aAAArO,KAAAqQ,UAAAE,GAAA7Q,GAAA6Q,EAAA,EAAA7K,EAAA1F,KAAAqQ,UAAAE,QANG,CACH,GAAAE,GAAAnD,EAAA,EAAAuB,gBAAAnP,EACA,QAAAW,GAAA,EAAkBA,EAAAoQ,EAAAvO,OAAgB7B,IAClCL,KAAA2N,WAAA8C,EAAApQ,GAAAmK,MAAA+F,EAAA,EAAA7K,EAAA+K,EAAApQ,GAAAqF,UARG,CACH,GAAAgL,GAAApD,EAAA,EAAAqD,iBAAAjR,EACA,QAAAW,GAAA,EAAkBA,EAAAqQ,EAAAxO,OAAmB7B,IACrCL,KAAA2N,WAAA+C,EAAArQ,GAAAmK,MAAA+F,EAAA,EAAA7K,EAAAgL,EAAArQ,GAAAqF,UAJA1F,MAAA2N,WAAAjO,EAAA6Q,EAAA,EAAA7K,GAAA,OPkpDM,SAAU7F,EAAQ+B,EAAqB1B,GAE7C,kBQzhEA0Q,GACArO,YAAAiL,GACAxN,KAAA8L,SAAA0B,EACAxN,KAAA6Q,UAGAtO,WACAvC,KAAA6Q,UAGAtO,QAAA6J,EAAA7G,EAAA6I,EAAAP,EAAAH,KAA8DhI,EAAA,MAC9D,SAAA0I,MAAA3M,eAAA,QACA,WAGA,IAAAqP,GAAA9Q,KAAA+Q,KAAA3E,EAAAgC,EAAA9D,IACAa,EAAApK,OAAA0J,KAAA2D,EAAA/D,KACA,QAAAJ,GAAA,EAAiBA,EAAAkB,EAAAjJ,OAAmB+H,IAAA,CACpC,GAAAT,GAAA2B,EAAAlB,EAKA,IAJAyD,EAAAjM,eAAA+H,KACAkE,EAAAlE,OAGAqE,EAAA,CACA,GAAA7D,GAAAoE,EAAA/D,KAAAb,EACAkE,GAAAlE,GAAArH,MACAiD,KAAA,OACA4E,KACA8G,MACAvL,QACA6G,YACA1G,aAIAgI,GAAAlE,GAAArH,MACAiD,KAAA,WAAA1E,MAAA,EAAA6E,QAAA6G,YACA1G,SAKA,MAAAgI,GAGAnL,cAAAgD,EAAAiE,EAAAkE,MAKA,MAJAA,GAAAjM,eAAA+H,KACAkE,EAAAlE,OAEAkE,EAAAlE,GAAArH,MAA0BiD,KAAA,WAAA1E,MAAA,EAAA6E,UAC1BmI,EAGAnL,WAAAkE,EAAAiH,MAEA,GAAAsD,MACAzI,EAAA9B,EAAAgC,QAAAF,GACAC,EAAA/B,EAAAgC,QAAAD,EAEA6B,EAAAtJ,OAAA0J,KAAAiD,EACA,QAAAlE,GAAAnJ,EAAA,EAAwBgK,EAAAnI,OAAAsH,EAAAa,EAAAhK,GAAkCA,IAAA,CAC1D,GAAA4Q,GAAA,CACA,QAAAhH,GAAA,EAAkBA,EAAAyD,EAAAlE,GAAAtH,OAA8B+H,IAAA,CAChD,GAAAiH,GAAAxD,EAAAlE,GAAAS,GAEAkH,EAAA,CACA,QAAAD,EAAA9L,MACA,YACA,GAAAwE,GAAA5J,KAAA8L,SAAAoF,EAAA9E,WAAA9C,cAAAE,GAAAI,YACAgG,KAAAwB,IAAApR,KAAA8L,SAAAoF,EAAA9E,WAAA9C,cAAAE,GAAAjE,MAAA,GACA8L,EAAArR,KAAAsR,gBAAAJ,EAAA9E,WACAmF,GAAAhJ,EAAA,GAAA2I,EAAAlH,IAAAzB,GAAA,EAAAC,EACAA,GAAAoB,EAAAyH,IAAAH,EAAAlH,GACAmH,GAAAD,EAAAJ,IAAAS,EAAAL,EAAA3L,KAUA,OAEA,eACA4L,EAAAD,EAAAxQ,MAAAwQ,EAAA3L,MAOA0L,GAAAE,EAGAH,EAAAxH,GAAAyH,EAEA,MAAAD,GAGAzO,UAAA6J,GACA,IAAApM,KAAA6Q,OAAApP,eAAA2K,GAAA,CACA,GAAAiF,GAAArR,KAAA8L,SAAAM,GAAA7C,iBAAAvJ,KAAA8L,SAAAM,GAAA/C,aACArJ,MAAA6Q,OAAAzE,IAA6BoF,QAAQH,kBAErC,MAAArR,MAAA6Q,OAAAzE,GASA7J,KAAA6J,EAAAqF,GACA,GAAAC,GAAA1R,KAAA2R,UAAAvF,EACA,OAAAsF,GAAAF,KAAA/P,eAAAyC,OAAAuN,IACAC,EAAAF,KAAAC,GAEAC,EAAAF,KAAAC,GAAA7B,KAAAgC,IAAA,GAAA5R,KAAA8L,SAAAM,GAAA/C,cAAAoI,EAAA,KAAAA,EAAA,KAGAlP,gBAAA6J,GACA,MAAApM,MAAA2R,UAAAvF,GAAAiF,gBAEAzP,EAAA,EAAAgP","file":"loki.FullTextSearch.min.js","sourcesContent":["(function webpackUniversalModuleDefinition(root, factory) {\n\tif(typeof exports === 'object' && typeof module === 'object')\n\t\tmodule.exports = factory();\n\telse if(typeof define === 'function' && define.amd)\n\t\tdefine(\"FullTextSearch\", [], factory);\n\telse if(typeof exports === 'object')\n\t\texports[\"FullTextSearch\"] = factory();\n\telse\n\t\troot[\"Loki\"] = root[\"Loki\"] || {}, root[\"Loki\"][\"FullTextSearch\"] = factory();\n})(this, function() {\nreturn \n\n\n// WEBPACK FOOTER //\n// webpack/universalModuleDefinition","(function webpackUniversalModuleDefinition(root, factory) {\n\tif(typeof exports === 'object' && typeof module === 'object')\n\t\tmodule.exports = factory();\n\telse if(typeof define === 'function' && define.amd)\n\t\tdefine(\"FullTextSearch\", [], factory);\n\telse if(typeof exports === 'object')\n\t\texports[\"FullTextSearch\"] = factory();\n\telse\n\t\troot[\"Loki\"] = root[\"Loki\"] || {}, root[\"Loki\"][\"FullTextSearch\"] = factory();\n})(this, function() {\nreturn /******/ (function(modules) { // webpackBootstrap\n/******/ \t// The module cache\n/******/ \tvar installedModules = {};\n/******/\n/******/ \t// The require function\n/******/ \tfunction __webpack_require__(moduleId) {\n/******/\n/******/ \t\t// Check if module is in cache\n/******/ \t\tif(installedModules[moduleId]) {\n/******/ \t\t\treturn installedModules[moduleId].exports;\n/******/ \t\t}\n/******/ \t\t// Create a new module (and put it into the cache)\n/******/ \t\tvar module = installedModules[moduleId] = {\n/******/ \t\t\ti: moduleId,\n/******/ \t\t\tl: false,\n/******/ \t\t\texports: {}\n/******/ \t\t};\n/******/\n/******/ \t\t// Execute the module function\n/******/ \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n/******/\n/******/ \t\t// Flag the module as loaded\n/******/ \t\tmodule.l = true;\n/******/\n/******/ \t\t// Return the exports of the module\n/******/ \t\treturn module.exports;\n/******/ \t}\n/******/\n/******/\n/******/ \t// expose the modules object (__webpack_modules__)\n/******/ \t__webpack_require__.m = modules;\n/******/\n/******/ \t// expose the module cache\n/******/ \t__webpack_require__.c = installedModules;\n/******/\n/******/ \t// identity function for calling harmony imports with the correct context\n/******/ \t__webpack_require__.i = function(value) { return value; };\n/******/\n/******/ \t// define getter function for harmony exports\n/******/ \t__webpack_require__.d = function(exports, name, getter) {\n/******/ \t\tif(!__webpack_require__.o(exports, name)) {\n/******/ \t\t\tObject.defineProperty(exports, name, {\n/******/ \t\t\t\tconfigurable: false,\n/******/ \t\t\t\tenumerable: true,\n/******/ \t\t\t\tget: getter\n/******/ \t\t\t});\n/******/ \t\t}\n/******/ \t};\n/******/\n/******/ \t// getDefaultExport function for compatibility with non-harmony modules\n/******/ \t__webpack_require__.n = function(module) {\n/******/ \t\tvar getter = module && module.__esModule ?\n/******/ \t\t\tfunction getDefault() { return module['default']; } :\n/******/ \t\t\tfunction getModuleExports() { return module; };\n/******/ \t\t__webpack_require__.d(getter, 'a', getter);\n/******/ \t\treturn getter;\n/******/ \t};\n/******/\n/******/ \t// Object.prototype.hasOwnProperty.call\n/******/ \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n/******/\n/******/ \t// __webpack_public_path__\n/******/ \t__webpack_require__.p = \"\";\n/******/\n/******/ \t// Load entry module and return exports\n/******/ \treturn __webpack_require__(__webpack_require__.s = 5);\n/******/ })\n/************************************************************************/\n/******/ ([\n/* 0 */\n/***/ (function(module, __webpack_exports__, __webpack_require__) {\n\n\"use strict\";\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_0__utils_js__ = __webpack_require__(1);\n\n\n/**\n * Splits a string at non-alphanumeric characters into lower case tokens.\n * @param {string} str - the string\n * @returns {string[]} - the tokens\n * @private\n */\nfunction defaultSplitter(str) {\n\tlet trimmedTokens = [];\n\tlet tokens = str.split(/[^\\w]/);\n\tfor (let i = 0; i < tokens.length; i++) {\n\t\tif (tokens[i] !== '') {\n\t\t\ttrimmedTokens.push(tokens[i].toLowerCase());\n\t\t}\n\t}\n\treturn trimmedTokens;\n}\n\n/**\n * The tokenizer is used to prepare the string content of a document field for the inverted index.\n * Firstly the string gets split into tokens.\n * After that the tokens will be trimmed/stemmed with defined functions from the queue.\n *\n * * To change the splitter function, use {@link Tokenizer#setSplitter}.\n * * To add functions to the queue, use {@link Tokenizer#add}, {@link Tokenizer#addBefore} and\n *   {@link Tokenizer#addAfter}.\n * * To remove a function from the queue, use {@link Tokenizer#remove}.\n * * To reset the tokenizer, use {@link Tokenizer#reset}.\n */\nclass Tokenizer {\n\t/**\n\t * Initializes the tokenizer with a splitter, which splits a string at non-alphanumeric characters.\n\t * The queue is empty.\n\t */\n\tconstructor() {\n\t\tthis._splitter = null;\n\t\tthis._queue = [];\n\t\tthis._symbol = Symbol('label');\n\t\tthis.reset();\n\t}\n\n\t/**\n\t * Sets a function with defined label as the splitter function.\n\t * The function must take a string as argument and return an array of tokens.\n\t *\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t */\n\tsetSplitter(label, func) {\n\t\tlabel = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"a\" /* asString */](label);\n\t\tif (!__WEBPACK_IMPORTED_MODULE_0__utils_js__[\"e\" /* isFunction */](func)) {\n\t\t\tthrow TypeError(\"Splitter must be a function.\");\n\t\t}\n\t\tif (label === \"\") {\n\t\t\tthrow Error(\"Label cannot be empty.\");\n\t\t}\n\t\tfunc[this._symbol] = label;\n\t\tthis._splitter = func;\n\t}\n\n\t/**\n\t * Gets the splitter.\n\t * @return {Array.<string, function>} - tuple with label and function\n\t */\n\tgetSplitter() {\n\t\treturn [this._splitter[this._symbol], this._splitter];\n\t}\n\n\t/**\n\t * Resets the splitter to default.\n\t */\n\tresetSplitter() {\n\t\tthis._splitter = defaultSplitter;\n\t}\n\n\t/**\n\t * Checks if a function is inside the queue.\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @returns {boolean} true if exists, otherwise false\n\t */\n\thas(labelFunc) {\n\t\treturn this._getPosition(labelFunc) !== -1;\n\t}\n\n\t/**\n\t * Gets a function from the queue.\n\t * Only the first found function gets returned if a label or a function is multiple used.\n\t *\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @return {Array.<string, function>} - tuple with label and function\n\t */\n\tget(labelFunc) {\n\t\tlet pos = this._getPosition(labelFunc);\n\t\tif (pos === -1) {\n\t\t\tthrow Error('Cannot find existing function.');\n\t\t}\n\t\treturn [this._queue[pos][this._symbol], this._queue[pos]];\n\t}\n\n\t/**\n\t * Adds a function with defined label to the end of the queue.\n\t * The function must take an array of tokens as argument and return an array of tokens.\n\t *\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t */\n\tadd(label, func) {\n\t\tthis._addFunction(label, func, this._queue.length);\n\t}\n\n\t/**\n\t * Adds a function with defined label before an existing function to the queue.\n\t * The function must take an array of tokens as argument and return an array of tokens.\n\t *\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t */\n\taddBefore(labelFunc, label, func) {\n\t\tlet pos = this._getPosition(labelFunc);\n\t\tif (pos === -1) {\n\t\t\tthrow Error('Cannot find existing function.');\n\t\t}\n\t\tthis._addFunction(label, func, pos);\n\t}\n\n\t/**\n\t * Adds a function with defined label after an existing function to the queue.\n\t * The function must take an array of tokens as argument and return an array of tokens.\n\t *\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t */\n\taddAfter(labelFunc, label, func) {\n\t\tlet pos = this._getPosition(labelFunc);\n\t\tif (pos === -1) {\n\t\t\tthrow Error('Cannot find existing function.');\n\t\t}\n\t\tthis._addFunction(label, func, pos + 1);\n\t}\n\n\t/**\n\t * Removes a function from the queue.\n\t * @param {string|function} labelFunc - an existing label or function\n\t */\n\tremove(labelFunc) {\n\t\tlet pos = this._getPosition(labelFunc);\n\t\tif (pos === -1) {\n\t\t\tthrow Error('Cannot find existing function.');\n\t\t}\n\t\tthis._queue.splice(pos, 1);\n\t}\n\n\t/**\n\t * Resets the splitter and tokenize queue to default.\n\t */\n\treset() {\n\t\tthis._splitter = defaultSplitter;\n\t\tthis._queue = [];\n\t}\n\n\t/**\n\t * Tokenizes a string into tokens.\n\t * @param {string} str - the string\n\t * @return {string[]} the tokens\n\t * @protected\n\t */\n\ttokenize(str) {\n\t\tlet tokens = this._splitter(str);\n\t\tfor (let i = 0; i < this._queue.length; i++) {\n\t\t\ttokens = this._queue[i](tokens);\n\t\t}\n\t\treturn tokens;\n\t}\n\n\t/**\n\t * Serializes the tokenizer by returning the labels of the used functions.\n\t * @returns {{splitter: string?, tokenizers: string[]}} - the serialization\n\t * @protected\n\t */\n\ttoJSON() {\n\t\tlet serialized = {tokenizers: []};\n\t\tif (this._splitter !== defaultSplitter) {\n\t\t\tserialized.splitter = this._splitter[this._symbol];\n\t\t}\n\t\tfor (let i = 0; i < this._queue.length; i++) {\n\t\t\tserialized.tokenizers.push(this._queue[i][this._symbol]);\n\t\t}\n\t\treturn serialized;\n\t}\n\n\t/**\n\t * Deserializes the tokenizer by reassign the correct function to each label.\n\t * @param {{splitter: string, tokenizers: string[]}} serialized - the serialized labels\n\t * @param {Object.<string, function>|Tokenizer} funcTok - the depending functions with labels\n\t * \tor an equivalent tokenizer\n\t */\n\tstatic fromJSON(serialized, funcTok) {\n\t\tlet tokenizer = new Tokenizer();\n\n\t\tif (funcTok !== undefined && funcTok instanceof Tokenizer) {\n\t\t\tif (serialized.hasOwnProperty(\"splitter\")) {\n\t\t\t\tlet splitter = funcTok.getSplitter();\n\t\t\t\tif (serialized.splitter !== splitter[0]) {\n\t\t\t\t\tthrow Error(\"Splitter function not found.\");\n\t\t\t\t}\n\t\t\t\ttokenizer.setSplitter(splitter[0], splitter[1]);\n\t\t\t}\n\n\t\t\tfor (let i = 0; i < serialized.tokenizers.length; i++) {\n\t\t\t\tif (!funcTok.has(serialized.tokenizers[i])) {\n\t\t\t\t\tthrow Error(\"Tokenizer function not found.\");\n\t\t\t\t}\n\t\t\t\tlet labelFunc = funcTok.get(serialized.tokenizers[i]);\n\t\t\t\ttokenizer.add(labelFunc[0], labelFunc[1]);\n\t\t\t}\n\t\t} else {\n\t\t\tif (serialized.hasOwnProperty(\"splitter\")) {\n\t\t\t\tif (!funcTok.splitters.hasOwnProperty(serialized.splitter)) {\n\t\t\t\t\tthrow Error(\"Splitter function not found.\");\n\t\t\t\t}\n\t\t\t\ttokenizer.setSplitter(serialized.splitter, funcTok.splitters[serialized.splitter]);\n\t\t\t}\n\t\t\tfor (let i = 0; i < serialized.tokenizers.length; i++) {\n\t\t\t\tif (!funcTok.tokenizers.hasOwnProperty(serialized.tokenizers[i])) {\n\t\t\t\t\tthrow Error(\"Tokenizer function not found.\");\n\t\t\t\t}\n\t\t\t\ttokenizer.add(serialized.tokenizers[i], funcTok.tokenizers[serialized.tokenizers[i]]);\n\t\t\t}\n\t\t}\n\t\treturn tokenizer;\n\t}\n\n\t/**\n\t * Returns the position of a function inside the queue.\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @return {number} the position\n\t * @private\n\t */\n\t_getPosition(labelFunc) {\n\t\tif (__WEBPACK_IMPORTED_MODULE_0__utils_js__[\"e\" /* isFunction */](labelFunc)) {\n\t\t\treturn this._queue.indexOf(labelFunc);\n\t\t} else if (__WEBPACK_IMPORTED_MODULE_0__utils_js__[\"f\" /* isConvertibleToString */](labelFunc)) {\n\t\t\tlabelFunc = String(labelFunc);\n\t\t\tfor (let i = 0; i < this._queue.length; i++) {\n\t\t\t\tif (this._queue[i][this._symbol] === labelFunc) {\n\t\t\t\t\treturn i;\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tthrow TypeError(\"Type of labelFunc must be string or function.\");\n\t\t}\n\t\treturn -1;\n\t}\n\n\t/**\n\t * Adds a function with defined label at a specific position to the queue.\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t * @param {number} pos - the position\n\t * @private\n\t */\n\t_addFunction(label, func, pos) {\n\t\tlabel = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"a\" /* asString */](label);\n\t\tif (!__WEBPACK_IMPORTED_MODULE_0__utils_js__[\"e\" /* isFunction */](func)) {\n\t\t\tthrow TypeError(\"Type of func must be function.\");\n\t\t}\n\t\tif (label === \"\") {\n\t\t\tthrow Error(\"Label cannot be empty.\");\n\t\t}\n\t\tfunc[this._symbol] = label;\n\t\tthis._queue.splice(pos, 0, func);\n\t}\n}\n/* harmony export (immutable) */ __webpack_exports__[\"a\"] = Tokenizer;\n\n\n\n/***/ }),\n/* 1 */\n/***/ (function(module, __webpack_exports__, __webpack_require__) {\n\n\"use strict\";\n/* harmony export (immutable) */ __webpack_exports__[\"e\"] = isFunction;\n/* unused harmony export isObject */\n/* harmony export (immutable) */ __webpack_exports__[\"b\"] = isNumber;\n/* unused harmony export isBoolean */\n/* unused harmony export isString */\n/* harmony export (immutable) */ __webpack_exports__[\"f\"] = isConvertibleToString;\n/* harmony export (immutable) */ __webpack_exports__[\"d\"] = asBoolean;\n/* harmony export (immutable) */ __webpack_exports__[\"a\"] = asString;\n/* harmony export (immutable) */ __webpack_exports__[\"c\"] = asArrayOfString;\n/**\n * Checks if the variable is a function.\n * @param {*} x - the variable\n * @return {boolean} true if function, otherwise false\n * @protected\n */\nfunction isFunction(x) {\n\treturn Object.prototype.toString.call(x) === \"[object Function]\";\n}\n\n/**\n * Checks if the variable is an object.\n * @param {*} x - the variable\n * @return {boolean} true if object, otherwise false\n * @protected\n */\nfunction isObject(x) {\n\treturn Object.prototype.toString.call(x) === \"[object Object]\";\n}\n\n/**\n * Checks if the variable is a number.\n * @param {*} x - the variable\n * @return {boolean} true if number, otherwise false\n * @protected\n */\nfunction isNumber(x) {\n\treturn Object.prototype.toString.call(x) === \"[object Number]\";\n}\n\n/**\n * Checks if the variable is a boolean.\n * @param {*} x - the variable\n * @return {boolean} true if boolean, otherwise false\n * @protected\n */\nfunction isBoolean(x) {\n\treturn Object.prototype.toString.call(x) === \"[object Boolean]\";\n}\n\n/**\n * Checks if the variable is a string.\n * @param {*} x - the variable\n * @return {boolean} true if string, otherwise false\n * @protected\n */\nfunction isString(x) {\n\treturn Object.prototype.toString.call(x) === \"[object String]\";\n}\n\n/**\n * Checks if the variable is convertible to a string.\n * @param {*} x - the variable\n * @return {boolean} true if convertible, otherwise false\n */\nfunction isConvertibleToString(x) {\n\treturn isString(x) || isNumber(x) || isObject(x) && Object.prototype.toString !== x.toString && isString(x.toString());\n}\n\n/**\n * Converts a variable to a boolean (from boolean or number).\n * Throws an error if not possible.\n * @param {*} x - the variable\n * @param {error} [error=TypeError] - the error to throw\n * @return {boolean} the converted boolean\n * @protected\n */\nfunction asBoolean(x, error = TypeError(\"Value is not convertible to boolean\")) {\n\tif (isBoolean(x) || isNumber(x)) {\n\t\treturn Boolean(x);\n\t}\n\tthrow error;\n}\n\n/**\n * Converts a variable to a string (from string, number or obj.toString).\n * Throws an error if not possible.\n * @param {*} x - the variable\n * @param {error} [error=TypeError] - the error to throw\n * @return {string} the converted string\n * @protected\n */\nfunction asString(x, error = TypeError(\"Value is not convertible to string.\")) {\n\tif (isConvertibleToString(x)) {\n\t\treturn String(x);\n\t}\n\tthrow error;\n}\n\n/**\n * Converts a variable to a array of string (from an array of string, number or obj.toString).\n * Throws an error if not possible.\n * @param {*} x - the variable\n * @param {error} [error=TypeError] - the error to throw\n * @return {string[]} the converted array of string\n * @protected\n */\nfunction asArrayOfString(x, error = TypeError(\"Value is not convertible to an array of strings.\")) {\n\tif (!Array.isArray(x)) {\n\t\tthrow error;\n\t}\n\tlet array = [];\n\tfor (let i = 0; i < x.length; i++) {\n\t\tif (!isConvertibleToString(x[i])) {\n\t\t\tthrow error;\n\t\t}\n\t\tarray.push(String(x[i]));\n\t}\n\treturn array;\n}\n\n\n/***/ }),\n/* 2 */\n/***/ (function(module, __webpack_exports__, __webpack_require__) {\n\n\"use strict\";\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_0__utils_js__ = __webpack_require__(1);\n/**\n * Query builder\n * todo: Document scoring.\n * todo: Align description.\n */\n\n\n/**\n * The base query class to enable boost to a query type.\n *\n * @param {string} type - the type name of the query\n */\nclass BaseQuery {\n\tconstructor(type, data = {}) {\n\t\tthis._data = data;\n\t\tthis._data.type = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"a\" /* asString */](type);\n\t}\n\n\t/**\n\t * Boosts the query result.\n\t *\n\t * See also [Lucene#BoostQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/BoostQuery.html}\n\t * and [Elasticsearch#boost]{@link https://www.elastic.co/guide/en/elasticsearch/reference/5.2/mapping-boost.html}.\n\t *\n\t * @param {number} value - the positive boost\n\t * @return {BaseQuery} object itself for cascading\n\t */\n\tboost(value) {\n\t\tif (!__WEBPACK_IMPORTED_MODULE_0__utils_js__[\"b\" /* isNumber */](value) || value < 0) {\n\t\t\tthrow TypeError(\"Boost must be a positive number.\");\n\t\t}\n\t\tthis._data.boost = value;\n\t\treturn this;\n\t}\n\n\t/**\n\t * Build the final query.\n\t * @return {Object} - the final query\n\t */\n\tbuild() {\n\t\treturn this._data;\n\t}\n}\n/* unused harmony export BaseQuery */\n\n\n/**\n * A query which finds documents where a document field contains a term.\n *\n * See also [Lucene#TermQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/TermQuery.html}\n * and [Elasticsearch#TermQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-term-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .term(\"name\", \"infinity\"])\n * .build();\n * // The resulting documents:\n * // contains the term infinity\n *\n * @param {string} field - the field name of the document\n * @param {string} term - the term\n * @extends BaseQuery\n */\nclass TermQuery extends BaseQuery {\n\tconstructor(field, term, data = {}) {\n\t\tsuper(\"term\", data);\n\t\tthis._data.field = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"a\" /* asString */](field);\n\t\tthis._data.value = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"a\" /* asString */](term);\n\t}\n}\n/* unused harmony export TermQuery */\n\n\n/**\n * A query which finds documents where a document field contains any of the terms.\n *\n * See also [Lucene#TermRangeQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/TermRangeQuery.html}\n * and [Elasticsearch#TermsQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-terms-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .terms(\"quotes\", [\"infinity\", \"atom\", \"energy\"])\n * .build();\n * // The resulting documents:\n * // contains the terms infinity, atom or energy\n *\n * @param {string} field - the field name of the document\n * @param {string[]} terms - the terms\n * @extends BaseQuery\n */\nclass TermsQuery extends BaseQuery {\n\tconstructor(field, terms, data = {}) {\n\t\tsuper(\"terms\", data);\n\t\tthis._data.field = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"a\" /* asString */](field);\n\t\tthis._data.value = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"c\" /* asArrayOfString */](terms);\n\t}\n}\n/* unused harmony export TermsQuery */\n\n\n/**\n * A query which finds documents where the wildcard term can be applied at an existing document field term.\n *\n * Wildcard | Description\n * -------- | ------------\n * ? (question mark) | Skips a single character.\n *\n * To escape a wildcard character, use _\\_ (backslash), e.g. \\?.\n *\n * See also [Lucene#WildcardQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/WildcardQuery.html}\n * and [Elasticsearch#WildcardQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-wildcard-query.html}.\n *\n * _TODO: Implement wildcard * (asterisk) to skip zero or more characters._\n * @todo Implement wildcard * (asterisk) to skip zero or more characters.\n *\n * @example\n * new QueryBuilder()\n *   .wildcard(\"question\", \"e?nste?n\\?\")\n * .build();\n * // The resulting documents:\n * // contains the wildcard surname e?nste?n\\? (like Einstein? or Eynsteyn? but not Einsteine or Ensten?)\n *\n * @param {string} field - the field name of the document\n * @param {string} wildcard - the wildcard term\n * @extends BaseQuery\n */\nclass WildcardQuery extends BaseQuery {\n\tconstructor(field, wildcard, data = {}) {\n\t\tsuper(\"wildcard\", data);\n\t\tthis._data.field = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"a\" /* asString */](field);\n\t\tthis._data.value = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"a\" /* asString */](wildcard);\n\t}\n}\n/* unused harmony export WildcardQuery */\n\n\n/**\n * A query which finds documents where the fuzzy term can be transformed into an existing document field term within a\n * given edit distance\n * ([Damerauâ€“Levenshtein distance]{@link https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance}).\n *\n * The edit distance is the minimum number of an insertion, deletion or substitution of a single character\n * or a transposition of two adjacent characters.\n *\n * * To set the maximal allowed edit distance, use {@link FuzzyQuery#fuzziness} (default is 2).\n * * To set the initial word length, which should ignored for fuzziness, use {@link FuzzyQuery#prefixLength}.\n *\n * See also [Lucene#FuzzyQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/FuzzyQuery.html}\n * and [Elasticsearch#FuzzyQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-fuzzy-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .fuzzy(\"surname\", \"einsten\")\n *     .fuzziness(3)\n *     .prefixLength(3)\n * .build();\n * // The resulting documents:\n * // contains the fuzzy surname einstn (like Einstein or Einst but not Eisstein or Insten)\n *\n * @param {string} field - the field name of the document\n * @param {string} fuzzy - the fuzzy term\n * @extends BaseQuery\n */\nclass FuzzyQuery extends BaseQuery {\n\tconstructor(field, fuzzy, data = {}) {\n\t\tsuper(\"fuzzy\", data);\n\t\tthis._data.field = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"a\" /* asString */](field);\n\t\tthis._data.value = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"a\" /* asString */](fuzzy);\n\t}\n\n\t/**\n\t * Sets the maximal allowed fuzziness.\n\t * @param {number} fuzziness - the fuzziness\n\t * @return {FuzzyQuery} - object itself for cascading\n\t */\n\tfuzziness(fuzziness) {\n\t\tif (!__WEBPACK_IMPORTED_MODULE_0__utils_js__[\"b\" /* isNumber */](fuzziness) || fuzziness < 0) {\n\t\t\tthrow TypeError(\"Fuzziness must be a positive number.\");\n\t\t}\n\t\tthis._data.fuzziness = fuzziness;\n\t\treturn this;\n\t}\n\n\t/**\n\t * Sets the initial word length.\n\t * @param {number} prefixLength - the positive prefix length\n\t * @return {FuzzyQuery}  object itself for cascading\n\t */\n\tprefixLength(prefixLength) {\n\t\tif (!__WEBPACK_IMPORTED_MODULE_0__utils_js__[\"b\" /* isNumber */](prefixLength) || prefixLength < 0) {\n\t\t\tthrow TypeError(\"Prefix length must be a positive number.\");\n\t\t}\n\t\tthis._data.prefix_length = prefixLength;\n\t\treturn this;\n\t}\n}\n/* unused harmony export FuzzyQuery */\n\n\n/**\n * A query which matches documents containing the prefix of a term inside a field.\n *\n * See also [Lucene#PrefixQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/PrefixQuery.html}\n * and [Elasticsearch#MatchQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-prefix-query.html}\n *\n * @example\n * new QueryBuilder()\n *   .prefix(\"surname\", \"alb\")\n * .build()\n * // The resulting documents:\n * // contains the term prefix alb as surname\n *\n * @param {string} field - the field name of the document\n * @param {string} prefix - the prefix of a term\n * @extends BaseQuery\n */\nclass PrefixQuery extends BaseQuery {\n\tconstructor(field, prefix, data = {}) {\n\t\tsuper(\"prefix\", data);\n\t\tthis._data.field = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"a\" /* asString */](field);\n\t\tthis._data.value = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"a\" /* asString */](prefix);\n\t}\n}\n/* unused harmony export PrefixQuery */\n\n\n/**\n * A query which matches all documents with a given field.\n *\n * See also [Elasticsearch#ExistsQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-exists-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .exists(\"name\")\n * .build()\n * // The resulting documents:\n * // has the field \"name\"\n *\n * @param {string} field - the field name of the document\n * @extends BaseQuery\n */\nclass ExistsQuery extends BaseQuery {\n\tconstructor(field, data = {}) {\n\t\tsuper(\"exists\", data);\n\t\tthis._data.field = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"a\" /* asString */](field);\n\t}\n}\n/* unused harmony export ExistsQuery */\n\n\n/**\n * A query which tokenizes the given query text, performs a query foreach token and combines the results using a boolean\n * operator.\n *\n * Operator      | Description\n * ------------- | -------------\n * or (default) | Finds documents which matches some tokens. The minimum amount of matches can be controlled with [minimumShouldMatch]{@link MatchQuery#minimumShouldMatch} (default is 1).\n * and | Finds documents which matches all tokens.\n *\n * To enable a [fuzzy query]{@link FuzzyQuery} for the tokens, use {@link MatchQuery#fuzziness} and {@link MatchQuery#prefixLength}.\n *\n * See also [Lucene#?]{@link ?}\n * and [Elasticsearch#MatchQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .match(\"name\", \"albrt einsten\")\n *     .boost(2.5)\n *     .operator(\"and\")\n *     .fuzziness(2)\n *     .prefixLength(3)\n * .build();\n * // The resulting documents:\n * // contains the fuzzy name albrt einsten (like Albert Einstein) with a boost of 2.5\n *\n * @param {string} field - the field name of the document\n * @param {string} query - the query text\n * @extends BaseQuery\n */\nclass MatchQuery extends BaseQuery {\n\tconstructor(field, query, data = {}) {\n\t\tsuper(\"match\", data);\n\t\tthis._data.field = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"a\" /* asString */](field);\n\t\tthis._data.value = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"a\" /* asString */](query);\n\t}\n\n\t/**\n\t * Controls the amount of minimum matching token queries before a document will be considered.\n\t * @param {number} minShouldMatch - number of minimum matching sub queries\n\t * @return {MatchQuery} object itself for cascading\n\t */\n\tminimumShouldMatch(minShouldMatch) {\n\t\tif (!__WEBPACK_IMPORTED_MODULE_0__utils_js__[\"b\" /* isNumber */](minShouldMatch) || minShouldMatch < 0) {\n\t\t\tthrow TypeError(\"Value for minimum should match must be a positive number.\");\n\t\t}\n\t\tif (this._data.hasOwnProperty(\"operator\") && this._data.operator == \"and\") {\n\t\t\tthrow SyntaxError(\"Match query with \\\"and\\\" operator does not support minimum should match.\");\n\t\t}\n\t\tthis._data.minimum_should_match = minShouldMatch;\n\t\treturn this;\n\t}\n\n\t/**\n\t * Sets the boolean operator.\n\t * @param {string} op - the operator (_or_/_and_)\n\t * @return {MatchQuery} object itself for cascading\n\t */\n\toperator(op) {\n\t\top = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"a\" /* asString */](op);\n\t\tif (op != 'and' && op != 'or') {\n\t\t\tthrow SyntaxError(\"Unknown operator.\");\n\t\t}\n\t\tthis._data.operator = op;\n\t\tif (this._data.hasOwnProperty(\"minimum_should_match\") && this._data.operator == \"and\") {\n\t\t\tthrow SyntaxError(\"Match query with \\\"and\\\" operator does not support minimum should match.\");\n\t\t}\n\t\treturn this;\n\t}\n\n\t/**\n\t * Sets the maximal allowed fuzziness.\n\t * @param {number} fuzziness - the fuzziness\n\t * @return {MatchQuery} object itself for cascading\n\t */\n\tfuzziness(fuzziness) {\n\t\tif (!__WEBPACK_IMPORTED_MODULE_0__utils_js__[\"b\" /* isNumber */](fuzziness) || fuzziness < 0) {\n\t\t\tthrow TypeError(\"Fuzziness must be a positive number.\");\n\t\t}\n\t\tthis._data.fuzziness = fuzziness;\n\t\treturn this;\n\t}\n\n\t/**\n\t * Sets the starting word length which should not be considered for fuzziness.\n\t * @param {number} prefixLength - the positive prefix length\n\t * @return {MatchQuery} - object itself for cascading\n\t */\n\tprefixLength(prefixLength) {\n\t\tif (!__WEBPACK_IMPORTED_MODULE_0__utils_js__[\"b\" /* isNumber */](prefixLength) || prefixLength < 0) {\n\t\t\tthrow TypeError(\"Prefix length must be a positive number.\");\n\t\t}\n\t\tthis._data.prefix_length = prefixLength;\n\t\treturn this;\n\t}\n}\n/* unused harmony export MatchQuery */\n\n\n/**\n * A query that matches all documents and giving them a constant score equal to the query boost.\n *\n * Typically used inside a must clause of a {@link BoolQuery} to subsequently reject non matching documents with the not\n * clause.\n *\n * See also [Lucene#MatchAllDocsQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/MatchAllDocsQuery.html}\n * and [Elasticsearch#MatchAllQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-all-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .matchAll()\n *     .boost(2.5)\n * .build()\n * // The resulting documents:\n * // all documents and giving a score of 2.5\n *\n * @extends BaseQuery\n */\nclass MatchAllQuery extends BaseQuery {\n\tconstructor(data = {}) {\n\t\tsuper(\"match_all\", data);\n\t}\n}\n/* unused harmony export MatchAllQuery */\n\n\n/**\n * A query which holds all sub queries like an array.\n * @private\n */\nclass ArrayQuery extends BaseQuery {\n\tconstructor(callbackName, callback, data = {}) {\n\t\tsuper(\"array\", data);\n\t\tthis._data.values = [];\n\t\tthis._callbackName = callbackName;\n\t\tthis[callbackName] = callback;\n\n\t\tthis._prepare = (queryType, ...args) => {\n\t\t\tlet data = {};\n\t\t\tlet query = new queryType(...args, data);\n\t\t\tthis._data.values.push(data);\n\t\t\tquery.bool = this.bool;\n\t\t\tquery.constantScore = this.constantScore;\n\t\t\tquery.term = this.term;\n\t\t\tquery.terms = this.terms;\n\t\t\tquery.wildcard = this.wildcard;\n\t\t\tquery.fuzzy = this.fuzzy;\n\t\t\tquery.match = this.match;\n\t\t\tquery.matchAll = this.matchAll;\n\t\t\tquery.prefix = this.prefix;\n\t\t\tquery.exists = this.exists;\n\t\t\tquery._prepare = this._prepare;\n\t\t\tquery[this._callbackName] = this[this._callbackName];\n\t\t\treturn query;\n\t\t};\n\t}\n\n\tbool() {\n\t\treturn this._prepare(BoolQuery);\n\t}\n\n\tconstantScore() {\n\t\treturn this._prepare(ConstantScoreQuery);\n\t}\n\n\tterm(field, term) {\n\t\treturn this._prepare(TermQuery, field, term);\n\t}\n\n\tterms(field, terms) {\n\t\treturn this._prepare(TermsQuery, field, terms);\n\t}\n\n\twildcard(field, wildcard) {\n\t\treturn this._prepare(WildcardQuery, field, wildcard);\n\t}\n\n\tfuzzy(field, fuzzy) {\n\t\treturn this._prepare(FuzzyQuery, field, fuzzy);\n\t}\n\n\tmatch(field, query) {\n\t\treturn this._prepare(MatchQuery, field, query);\n\t}\n\n\tmatchAll() {\n\t\treturn this._prepare(MatchAllQuery);\n\t}\n\n\tprefix(field, prefix) {\n\t\treturn this._prepare(PrefixQuery, field, prefix);\n\t}\n\n\texists(field) {\n\t\treturn this._prepare(ExistsQuery, field);\n\t}\n}\n\n/**\n * A query that wraps sub queries and returns a constant score equal to the query boost for every document in the filter.\n *\n * See also [Lucene#BooleanQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/ConstantScoreQuery.html}\n * and [Elasticsearch#ConstantScoreQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-constant-score-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .constantScore()\n *     .boost(1.5)\n *     .startFilter()\n *       .term(\"first_name\", \"albert\")\n *       .term(\"surname\", \"einstein\")\n *     .endFilter()\n * .build()\n * // The resulting documents:\n * // * contains albert as first name, einstein as surname and the document score is 42.\n *\n * @extends BaseQuery\n */\nclass ConstantScoreQuery extends BaseQuery {\n\tconstructor(data = {}) {\n\t\tsuper(\"constant_score\", data);\n\t}\n\n\t/**\n\t * Starts an array of queries. Use endFilter() to finish the array.\n\t * @return {ArrayQuery} array query for holding sub queries\n\t */\n\tstartFilter() {\n\t\tthis._data.filter = {};\n\t\treturn new ArrayQuery(\"endFilter\", () => {\n\t\t\treturn this;\n\t\t}, this._data.filter);\n\t}\n}\n/* unused harmony export ConstantScoreQuery */\n\n\n/**\n * A query that matches documents matching boolean combinations of sub queries.\n *\n * This query consists of one or more boolean clauses with different behavior but interrelated to each other.\n *\n * Occur         | Description\n * ------------- | -------------\n * must  | Finds documents which matches all sub queries.\n * filter  | Finds documents which matches all sub queries but these documents do not contribute to the score.\n * should  | Finds documents which matches some sub queries. The minimum amount of matches can be controlled with [minimumShouldMatch]{@link BoolQuery#minimumShouldMatch} (default is 1).\n * not  | Documents which match any sub query will be ignored.\n *\n * A sub query can be any other query type and also the bool query itself.\n *\n * See also [Lucene#BooleanQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/BooleanQuery.html}\n * and [Elasticsearch#BoolQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .bool()\n *     .startMust().boost(2)\n *       .term(\"first_name\", \"albert\")\n *     .endMust()\n *     .startFilter()\n *       .term(\"birthplace\", \"ulm\")\n *     .endFilter()\n *     .startShould().minimumShouldMatch(2)\n *       .fuzzy(\"surname\", \"einstin\")\n *       .wildcard(\"name\", \"geni?s\")\n *       .term(\"quotes\", \"infinity\")\n *     .endShould()\n *     .startNot()\n *       .terms(\"research_field\", [\"biology\", \"geography\"])\n *     .endNot()\n * .build();\n * // The resulting documents:\n * // contains the name albert (must: contribute to the score with a boost of 2)\n * // contains the birthplace ulm (filter: not contribute to the score)\n * // contains a minimum of two matches from the fuzzy, wildcard and/or term query (should: contribute to the score)\n * // do not contains biology or geography as research field (not: not contribute to the score)\n *\n * @extends BaseQuery\n */\nclass BoolQuery extends BaseQuery {\n\tconstructor(data = {}) {\n\t\tsuper(\"bool\", data);\n\t}\n\n\t/**\n\t * Starts an array of queries for must clause. Use endMust() to finish the array.\n\t * @return {ArrayQuery} array query for holding sub queries\n\t */\n\tstartMust() {\n\t\tthis._data.must = {};\n\t\treturn new ArrayQuery(\"endMust\", () => {\n\t\t\treturn this;\n\t\t}, this._data.must);\n\t}\n\n\t/**\n\t * Starts an array of queries for filter clause. Use endFilter() to finish the array.\n\t * @return {ArrayQuery} array query for holding sub queries\n\t */\n\tstartFilter() {\n\t\tthis._data.filter = {};\n\t\treturn new ArrayQuery(\"endFilter\", () => {\n\t\t\treturn this;\n\t\t}, this._data.filter);\n\t}\n\n\t/**\n\t * Starts an array of queries for should clause. Use endShould() to finish the array.\n\t * @return {ArrayQuery} array query for holding sub queries\n\t */\n\tstartShould() {\n\t\tthis._data.should = {};\n\t\treturn new ArrayQuery(\"endShould\", () => {\n\t\t\treturn this;\n\t\t}, this._data.should);\n\t}\n\n\t/**\n\t * Starts an array of queries for not clause. Use endNot() to finish the array.\n\t * @return {ArrayQuery} array query for holding sub queries\n\t */\n\tstartNot() {\n\t\tthis._data.not = {};\n\t\treturn new ArrayQuery(\"endNot\", () => {\n\t\t\treturn this;\n\t\t}, this._data.not);\n\t}\n\n\t/**\n\t * Controls the amount of minimum matching sub queries before a document will be considered.\n\t * @param {number} minShouldMatch - number of minimum matching sub queries\n\t * @return {BoolQuery} object itself for cascading\n\t */\n\tminimumShouldMatch(minShouldMatch) {\n\t\tif (typeof(minShouldMatch) !== \"number\" || minShouldMatch < 0) {\n\t\t\tthrow TypeError(\"Minimum should match must be a number greater than zero.\");\n\t\t}\n\t\tthis._data.minimum_should_match = minShouldMatch;\n\t\treturn this;\n\t}\n}\n/* unused harmony export BoolQuery */\n\n\n/**\n * This query builder is the root of each query search.\n * The query contains a sub query and parameters for setup scoring and search options.\n *\n * Possible sub query types are:\n * {@link TermQuery}, {@link TermsQuery}, {@link FuzzyQuery}, {@link WildcardQuery},\n * {@link MatchQuery}, {@link MatchAllQuery}, {@link PrefixQuery},  {@link BoolQuery},\n * {@link ConstantScoreQuery}, {@link ExistsQuery}\n *\n * @example\n * new QueryBuilder()\n *   .finalScoring(true)\n *   .useBM25(1.5, 0.5)\n *   .term(\"first_name\", \"albert\")\n * .build();\n * // The resulting documents:\n * // contains the first name albert\n * // are scored and ranked using BM25 with k1=1.5 and b=0.5\n */\nclass QueryBuilder {\n\tconstructor() {\n\t\tthis._data = {query: {}};\n\t\tthis.useBM25();\n\t}\n\n\t/**\n\t * The query performs a final scoring over all scored sub queries and rank documents by there relevant.\n\t * @param {boolean} enabled - flag to enable or disable final scoring\n\t * @return {QueryBuilder}\n\t */\n\tenableFinalScoring(enabled) {\n\t\tthis._data.final_scoring = __WEBPACK_IMPORTED_MODULE_0__utils_js__[\"d\" /* asBoolean */](enabled);\n\t\treturn this;\n\t}\n\n\t/**\n\t * Use [Okapi BM25]{@link https://en.wikipedia.org/wiki/Okapi_BM25} as scoring model (default).\n\t *\n\t * See also [Lucene#MatchAllDocsQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/similarities/BM25Similarity.html}\n\t * and [Elasticsearch#BM25]{@link https://www.elastic.co/guide/en/elasticsearch/guide/current/pluggable-similarites.html#bm25}.\n\t *\n\t * @param {number} [k1=1.2] - controls how quickly an increase in term frequency results in term-frequency saturation.\n\t * \t\t\t\t\t\t\t\t\t\t\t\t\t\tLower values result in quicker saturation, and higher values in slower saturation.\n\t * @param {number} [b=0.75] - controls how much effect field-length normalization should have.\n\t * \t\t\t\t\t\t\t\t\t\t\t\t\t\tA value of 0.0 disables normalization completely, and a value of 1.0 normalizes fully.\n\t * @return {QueryBuilder}\n\t */\n\tuseBM25(k1 = 1.2, b = 0.75) {\n\t\tif (!__WEBPACK_IMPORTED_MODULE_0__utils_js__[\"b\" /* isNumber */](k1) || k1 < 0) {\n\t\t\tthrow TypeError(\"BM25s k1 must be a positive number.\");\n\t\t}\n\t\tif (!__WEBPACK_IMPORTED_MODULE_0__utils_js__[\"b\" /* isNumber */](b) || b < 0 || b > 1) {\n\t\t\tthrow TypeError(\"BM25s b must be a number between 0 and 1 inclusive.\");\n\t\t}\n\n\t\tthis._data.scoring = {\n\t\t\ttype: \"BM25\",\n\t\t\tk1: k1,\n\t\t\tb: b\n\t\t};\n\t\treturn this;\n\t}\n\n\tbool() {\n\t\treturn this._prepare(BoolQuery);\n\t}\n\n\tconstantScore() {\n\t\treturn this._prepare(ConstantScoreQuery);\n\t}\n\n\tterm(field, term) {\n\t\treturn this._prepare(TermQuery, field, term);\n\t}\n\n\tterms(field, terms) {\n\t\treturn this._prepare(TermsQuery, field, terms);\n\t}\n\n\twildcard(field, wildcard) {\n\t\treturn this._prepare(WildcardQuery, field, wildcard);\n\t}\n\n\tfuzzy(field, fuzzy) {\n\t\treturn this._prepare(FuzzyQuery, field, fuzzy);\n\t}\n\n\tmatch(field, query) {\n\t\treturn this._prepare(MatchQuery, field, query);\n\t}\n\n\tmatchAll() {\n\t\treturn this._prepare(MatchAllQuery);\n\t}\n\n\tprefix(field, prefix) {\n\t\treturn this._prepare(PrefixQuery, field, prefix);\n\t}\n\n\texists(field) {\n\t\treturn this._prepare(ExistsQuery, field);\n\t}\n\n\t_prepare(queryType, ...args) {\n\t\tthis._child = new queryType(...args, this._data.query);\n\t\tthis._child.build = () => {\n\t\t\treturn this._data;\n\t\t};\n\t\treturn this._child;\n\t}\n}\n/* harmony export (immutable) */ __webpack_exports__[\"a\"] = QueryBuilder;\n\n\n\n/***/ }),\n/* 3 */\n/***/ (function(module, __webpack_exports__, __webpack_require__) {\n\n\"use strict\";\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_0__tokenizer__ = __webpack_require__(0);\n\n\n/**\n * Inverted index class handles featured text search for specific document fields.\n * @constructor InvertedIndex\n * @param {boolean} [options.store=true] - inverted index will be stored at serialization rather than rebuilt on load.\n */\nclass InvertedIndex {\n\t/**\n\t * @param {boolean} store\n\t * @param {Tokenizer} tokenizer\n\t */\n\tconstructor(store = true, tokenizer = new __WEBPACK_IMPORTED_MODULE_0__tokenizer__[\"a\" /* Tokenizer */]) {\n\t\tthis._store = store;\n\t\tthis._tokenizer = tokenizer;\n\t\tthis._docCount = 0;\n\t\tthis._docStore = {};\n\t\tthis._totalFieldLength = 0;\n\t\tthis._root = {};\n\t}\n\n\tget store() {\n\t\treturn this._store;\n\t}\n\n\tget tokenizer() {\n\t\treturn this._tokenizer;\n\t}\n\n\tget documentCount() {\n\t\treturn this._docCount;\n\t}\n\n\tget documentStore() {\n\t\treturn this._docStore;\n\t}\n\n\tget totalFieldLength() {\n\t\treturn this._totalFieldLength;\n\t}\n\n\tget root() {\n\t\treturn this._root;\n\t}\n\n\t/**\n\t * Adds defined fields of a document to the inverted index.\n\t * @param {object} field - the field to add\n\t * @param {number} docId - the doc id of the field\n\t * @param {number} [boost=1] - object with field (key) specific boost (value)\n\t */\n\tinsert(field, docId, boost = 1) {\n\t\tif (this._docStore.hasOwnProperty(docId)) {\n\t\t\tthrow Error('Field already added.');\n\t\t}\n\n\t\tthis._docCount += 1;\n\t\tthis._docStore[docId] = {};\n\n\t\t// Tokenize document field.\n\t\tlet fieldTokens = this._tokenizer.tokenize(field);\n\t\tthis._totalFieldLength += fieldTokens.length;\n\n\t\tlet termRefs = [];\n\t\tthis._docStore[docId] = {fieldLength: fieldTokens.length, boost: boost};\n\t\tObject.defineProperties(this._docStore[docId], {\n\t\t\ttermRefs: {enumerable: false, configurable: true, writable: true, value: termRefs}\n\t\t});\n\n\t\t// Iterate over all unique field terms.\n\t\tfor (let term of new Set(fieldTokens)) {\n\t\t\tif (term === '') {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t// Calculate term frequency.\n\t\t\tlet tf = 0;\n\t\t\tfor (let j = 0; j < fieldTokens.length; j++) {\n\t\t\t\tif (fieldTokens[j] === term) {\n\t\t\t\t\ttf++;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Add term to index tree.\n\t\t\tlet branch = this._root;\n\t\t\tfor (let i = 0; i < term.length; i++) {\n\t\t\t\tlet c = term[i];\n\t\t\t\tif (!branch.hasOwnProperty(c)) {\n\t\t\t\t\tlet child = {};\n\t\t\t\t\tObject.defineProperties(child, {\n\t\t\t\t\t\tparent: {enumerable: false, configurable: true, writable: true, value: branch}\n\t\t\t\t\t});\n\t\t\t\t\tbranch[c] = child;\n\t\t\t\t}\n\t\t\t\tbranch = branch[c];\n\t\t\t}\n\t\t\t// Add term info to index leaf.\n\t\t\tif (!branch.hasOwnProperty('docs')) {\n\t\t\t\tbranch.docs = {};\n\t\t\t\tbranch.df = 0;\n\t\t\t}\n\t\t\tbranch.docs[docId] = tf;\n\t\t\tbranch.df += 1;\n\n\t\t\t// Store index leaf for deletion.\n\t\t\ttermRefs.push(branch);\n\t\t}\n\t}\n\n\t/**\n\t * Removes all relevant terms of a document from the inverted index.\n\t * @param {number} docId - the document.\n\t */\n\tremove(docId) {\n\t\tif (!this._docStore.hasOwnProperty(String(docId))) {\n\t\t\treturn;\n\t\t}\n\t\tlet docStore = this._docStore[docId];\n\t\t// Remove document.\n\t\tdelete this._docStore[docId];\n\t\tthis._docCount -= 1;\n\n\t\t// Reduce total field length.\n\t\tthis._totalFieldLength -= docStore.fieldLength;\n\n\t\t// Iterate over all term references.\n\t\t// Remove docId from docs and decrement document frequency.\n\t\tlet termRefs = docStore.termRefs;\n\t\tfor (let j = 0; j < termRefs.length; j++) {\n\t\t\tlet index = termRefs[j];\n\t\t\tindex.df -= 1;\n\t\t\tdelete index.docs[docId];\n\n\t\t\t// Delete term branch if not used anymore.\n\t\t\tif (index.df === 0) {\n\t\t\t\tlet keys = [];\n\t\t\t\tdo {\n\t\t\t\t\t// Go tree upwards.\n\t\t\t\t\tlet parent = index.parent;\n\t\t\t\t\t// Delete parent reference for preventing memory leak (cycle reference)\n\t\t\t\t\tdelete index.parent;\n\n\t\t\t\t\t// Iterate over all children.\n\t\t\t\t\tkeys = Object.keys(parent);\n\t\t\t\t\tfor (let k = 0; k < keys.length; k++) {\n\t\t\t\t\t\tlet key = keys[k];\n\t\t\t\t\t\tif (key === 'df' || key === 'docs') {\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Remove previous child form parent.\n\t\t\t\t\t\tif (parent[key] === index) {\n\t\t\t\t\t\t\tdelete parent[key];\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tindex = parent;\n\t\t\t\t} while (index.hasOwnProperty('parent') && keys.length === 1);\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Gets the term index of a term.\n\t * @param {string} term - the term.\n\t * @param {object} root - the term index to start from\n\t * @param {number} start - the position of the term string to start from\n\t * @return {object} - The term index or null if the term is not in the term tree.\n\t */\n\tstatic getTermIndex(term, root, start = 0) {\n\t\tif (start >= term.length) {\n\t\t\treturn null;\n\t\t}\n\t\tfor (let i = start; i < term.length; i++) {\n\t\t\tif (!root.hasOwnProperty(term[i])) {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t\troot = root[term[i]];\n\t\t}\n\t\treturn root;\n\t}\n\n\t/**\n\t * Extends a term index for the one branch.\n\t * @param {object} root - the term index to start from\n\t * @return {Array} - array with term indices and extension\n\t */\n\tstatic getNextTermIndex(root) {\n\t\tlet termIndices = [];\n\t\tlet keys = Object.keys(root);\n\t\tfor (let i = 0; i < keys.length; i++) {\n\t\t\tif (keys[i] !== 'docs' && keys[i] !== 'df') {\n\t\t\t\ttermIndices.push({index: root[keys[i]], term: keys[i]});\n\t\t\t}\n\t\t}\n\t\treturn termIndices;\n\t}\n\n\t/**\n\t * Extends a term index to all available term leafs.\n\t * @param {object} root - the term index to start from\n\t * @returns {Array} - Array with term indices and extension\n\t */\n\tstatic extendTermIndex(root) {\n\t\tlet termIndices = [];\n\t\tlet stack = [root];\n\t\tlet treeStack = [''];\n\t\tdo {\n\t\t\tlet root = stack.pop();\n\t\t\tlet treeTermn = treeStack.pop();\n\n\t\t\tif (root.hasOwnProperty('df')) {\n\t\t\t\ttermIndices.push({index: root, term: treeTermn});\n\t\t\t}\n\n\t\t\tlet keys = Object.keys(root);\n\t\t\tfor (let i = 0; i < keys.length; i++) {\n\t\t\t\tif (keys[i] !== 'docs' && keys[i] !== 'df') {\n\t\t\t\t\tstack.push(root[keys[i]]);\n\t\t\t\t\ttreeStack.push(treeTermn + keys[i]);\n\t\t\t\t}\n\t\t\t}\n\t\t} while (stack.length !== 0);\n\n\t\treturn termIndices;\n\t}\n\n\t/**\n\t * Serialize the inverted index.\n\t * @returns {{docStore: *, _fields: *, index: *}}\n\t */\n\ttoJSON() {\n\t\tif (this._store) {\n\t\t\treturn this;\n\t\t} else {\n\t\t\treturn {\n\t\t\t\t_tokenizer: this._tokenizer,\n\t\t\t};\n\t\t}\n\t}\n\n\t/**\n\t * Deserialize the inverted index.\n\t * @param {{docStore: *, _fields: *, index: *}} serialized - The serialized inverted index.\n\t * @param {Object.<string, function>|Tokenizer} funcTok[undefined] - the depending functions with labels\n\t * \tor an equivalent tokenizer\n\t */\n\tloadJSON(serialized, funcTok = undefined) {\n\t\tlet dbObject = serialized;\n\n\t\tthis._tokenizer = __WEBPACK_IMPORTED_MODULE_0__tokenizer__[\"a\" /* Tokenizer */].fromJSON(dbObject._tokenizer, funcTok);\n\t\tthis._docCount = dbObject._docCount;\n\t\tthis._docStore = dbObject._docStore;\n\t\tthis._totalFieldLength = dbObject._totalFieldLength;\n\t\tthis._root = dbObject._root;\n\n\t\tlet self = this;\n\n\t\tfunction regenerate(index, parent) {\n\t\t\t// Set parent.\n\t\t\tif (parent !== null) {\n\t\t\t\tObject.defineProperties(index, {\n\t\t\t\t\tparent: {enumerable: false, configurable: true, writable: false, value: parent}\n\t\t\t\t});\n\t\t\t}\n\n\t\t\t// Iterate over all keys.\n\t\t\tlet keys = Object.keys(index);\n\t\t\tfor (let i = 0; i < keys.length; i++) {\n\t\t\t\t// Found term, save in document store.\n\t\t\t\tif (keys[i] === 'docs') {\n\t\t\t\t\t// Get documents of term.\n\t\t\t\t\tlet docIds = Object.keys(index.docs);\n\t\t\t\t\tfor (let j = 0; j < docIds.length; j++) {\n\t\t\t\t\t\t// Get document store at specific document/field.\n\t\t\t\t\t\tlet ref = self._docStore[docIds[j]];\n\t\t\t\t\t\tif (!ref.hasOwnProperty('termRefs')) {\n\t\t\t\t\t\t\tObject.defineProperties(ref, {\n\t\t\t\t\t\t\t\ttermRefs: {enumerable: false, configurable: true, writable: true, value: []}\n\t\t\t\t\t\t\t});\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Set reference to term index.\n\t\t\t\t\t\tref.termRefs.push(index);\n\t\t\t\t\t}\n\t\t\t\t} else if (keys[i] !== 'df') {\n\t\t\t\t\t// Iterate over subtree.\n\t\t\t\t\tregenerate(index[keys[i]], index);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tregenerate(this._root, null);\n\t}\n}\n/* harmony export (immutable) */ __webpack_exports__[\"a\"] = InvertedIndex;\n\n\n\n/***/ }),\n/* 4 */\n/***/ (function(module, __webpack_exports__, __webpack_require__) {\n\n\"use strict\";\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_0__inverted_index__ = __webpack_require__(3);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_1__index_searcher__ = __webpack_require__(6);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_2__tokenizer__ = __webpack_require__(0);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_3__utils_js__ = __webpack_require__(1);\n\n\n\n\n\nclass FullTextSearch {\n\t/**\n\t *\n\t * @param options\n\t */\n\tconstructor(fields) {\n\t\tif (fields === undefined) {\n\t\t\tthrow new SyntaxError('Fields needs to be defined!');\n\t\t}\n\n\t\tthis._invIdxs = {};\n\t\t// Get field names and tokenizers.\n\t\tif (Array.isArray(fields)) {\n\t\t\tfor (let i = 0; i < fields.length; i++) {\n\t\t\t\tlet field = fields[i];\n\t\t\t\tlet name = __WEBPACK_IMPORTED_MODULE_3__utils_js__[\"a\" /* asString */](field.name, TypeError('Field name needs to be a string.'));\n\n\t\t\t\tlet store = field.hasOwnProperty(\"store\") ?\n\t\t\t\t\t__WEBPACK_IMPORTED_MODULE_3__utils_js__[\"d\" /* asBoolean */](field.store, TypeError(\"Field store flag needs to be a boolean\")) : true;\n\n\t\t\t\tlet tokenizer = null;\n\t\t\t\tif (field.hasOwnProperty(\"tokenizer\")) {\n\t\t\t\t\tif (!(field.tokenizer instanceof __WEBPACK_IMPORTED_MODULE_2__tokenizer__[\"a\" /* Tokenizer */])) {\n\t\t\t\t\t\tthrow new TypeError(\"Field tokenizer needs to be a instance of tokenizer.\");\n\t\t\t\t\t}\n\t\t\t\t\ttokenizer = field.tokenizer;\n\t\t\t\t} else {\n\t\t\t\t\ttokenizer = new __WEBPACK_IMPORTED_MODULE_2__tokenizer__[\"a\" /* Tokenizer */]();\n\t\t\t\t}\n\t\t\t\tthis._invIdxs[name] = new __WEBPACK_IMPORTED_MODULE_0__inverted_index__[\"a\" /* InvertedIndex */](store, tokenizer);\n\t\t\t}\n\t\t} else {\n\t\t\tthrow new TypeError('fields needs to be an array with field name and a tokenizer (optional).');\n\t\t}\n\n\t\tthis._docs = new Set();\n\t\tthis._idxSearcher = new __WEBPACK_IMPORTED_MODULE_1__index_searcher__[\"a\" /* IndexSearcher */](this._invIdxs, this._docs);\n\t}\n\n\taddDocument(doc, boosts = {}) {\n\t\tif (!doc.hasOwnProperty('$loki')) {\n\t\t\tthrow new Error('Document is not stored in the collection.');\n\t\t}\n\n\t\tlet fieldNames = Object.keys(doc);\n\t\tfor (let i = 0, fieldName; i < fieldNames.length, fieldName = fieldNames[i]; i++) {\n\t\t\tif (this._invIdxs.hasOwnProperty(fieldName)) {\n\t\t\t\tlet boost = boosts.hasOwnProperty(fieldName) ? boosts[fieldName] : 1;\n\t\t\t\tthis._invIdxs[fieldName].insert(doc[fieldName], doc.$loki, boost);\n\t\t\t}\n\t\t}\n\n\t\tthis._docs.add(doc.$loki);\n\t\tthis.setDirty();\n\t}\n\n\tremoveDocument(doc) {\n\t\tif (!doc.hasOwnProperty('$loki')) {\n\t\t\tthrow new Error('Document is not stored in the collection.');\n\t\t}\n\n\t\tlet fieldNames = Object.keys(this._invIdxs);\n\t\tfor (let i = 0; i < fieldNames.length; i++) {\n\t\t\tthis._invIdxs[fieldNames[i]].remove(doc.$loki);\n\t\t}\n\n\t\tthis._docs.delete(doc.$loki);\n\t\tthis.setDirty();\n\t}\n\n\tupdateDocument(doc, boosts = {}) {\n\t\tthis.removeDocument(doc);\n\t\tthis.addDocument(doc, boosts);\n\t}\n\n\tsearch(query) {\n\t\treturn this._idxSearcher.search(query);\n\t}\n\n\ttoJSON() {\n\t\tlet serialized = {};\n\t\tlet fieldNames = Object.keys(this._invIdxs);\n\t\tfor (let i = 0, fieldName; i < fieldNames.length, fieldName = fieldNames[i]; i++) {\n\t\t\tserialized[fieldName] = this._invIdxs[fieldName].toJSON();\n\t\t}\n\t\treturn serialized;\n\t}\n\n\tloadJSON(serialized, tokenizers) {\n\t\tlet db = JSON.parse(serialized);\n\t\tlet fieldNames = Object.keys(db);\n\t\tfor (let i = 0, fieldName; i < fieldNames.length, fieldName = fieldNames[i]; i++) {\n\t\t\tthis._invIdxs[fieldName] = new __WEBPACK_IMPORTED_MODULE_0__inverted_index__[\"a\" /* InvertedIndex */]();\n\t\t\tthis._invIdxs[fieldName].loadJSON(db[fieldName], tokenizers[fieldName]);\n\t\t}\n\t}\n\n\tsetDirty() {\n\t\tthis._idxSearcher.setDirty();\n\t}\n}\n/* harmony export (immutable) */ __webpack_exports__[\"a\"] = FullTextSearch;\n\n\n\n/***/ }),\n/* 5 */\n/***/ (function(module, __webpack_exports__, __webpack_require__) {\n\n\"use strict\";\nObject.defineProperty(__webpack_exports__, \"__esModule\", { value: true });\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_0__full_text_search__ = __webpack_require__(4);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_1__tokenizer__ = __webpack_require__(0);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_2__queries__ = __webpack_require__(2);\n/* harmony reexport (binding) */ __webpack_require__.d(__webpack_exports__, \"FullTextSearch\", function() { return __WEBPACK_IMPORTED_MODULE_0__full_text_search__[\"a\"]; });\n/* harmony reexport (binding) */ __webpack_require__.d(__webpack_exports__, \"Tokenizer\", function() { return __WEBPACK_IMPORTED_MODULE_1__tokenizer__[\"a\"]; });\n/* harmony reexport (binding) */ __webpack_require__.d(__webpack_exports__, \"QueryBuilder\", function() { return __WEBPACK_IMPORTED_MODULE_2__queries__[\"a\"]; });\n\n\n\n\n\n//\n//\n//\n// Loki.Tokenizer = Tokenizer;\n// Loki.QueryBuilder = QueryBuilder;\n// Loki.Plugins.FullTextSearch = FullTextSearch;\n//\n\n\n/***/ }),\n/* 6 */\n/***/ (function(module, __webpack_exports__, __webpack_require__) {\n\n\"use strict\";\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_0__scorer__ = __webpack_require__(7);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_1__inverted_index__ = __webpack_require__(3);\n/* harmony import */ var __WEBPACK_IMPORTED_MODULE_2__queries__ = __webpack_require__(2);\n\n\n\n\nclass IndexSearcher {\n\t/**\n\t *\n\t * @param {object} invIdxs\n\t */\n\tconstructor(invIdxs, docs) {\n\t\tthis._invIdxs = invIdxs;\n\t\tthis._docs = docs;\n\t\tthis._scorer = new __WEBPACK_IMPORTED_MODULE_0__scorer__[\"a\" /* Scorer */](this._invIdxs);\n\t}\n\n\tsearch(query) {\n\t\tlet docResults = this._recursive(query.query, true);\n\n\t\t// Final scoring.\n\t\tlet finalScoring = query.hasOwnProperty(\"final_scoring\") ? query.final_scoring : true;\n\t\tif (finalScoring) {\n\t\t\treturn this._scorer.finalScore(query, docResults);\n\t\t}\n\t\treturn docResults;\n\t}\n\n\tsetDirty() {\n\t\tthis._scorer.setDirty();\n\t}\n\n\t_recursive(query, doScoring) {\n\t\tlet docResults = {};\n\t\tlet boost = query.hasOwnProperty('boost') ? query.boost : 1;\n\t\tlet fieldName = query.hasOwnProperty(\"field\") ? query.field : null;\n\n\t\tlet root = null;\n\t\tlet tokenizer = null;\n\t\tif (this._invIdxs.hasOwnProperty(fieldName)) {\n\t\t\troot = this._invIdxs[fieldName].root;\n\t\t\ttokenizer = this._invIdxs[fieldName].tokenizer;\n\t\t}\n\n\t\tswitch (query.type) {\n\t\t\tcase \"bool\": {\n\t\t\t\tdocResults = null;\n\t\t\t\tif (query.hasOwnProperty(\"must\")) {\n\t\t\t\t\tdocResults = this._getUnique(query.must.values, doScoring, docResults);\n\t\t\t\t}\n\t\t\t\tif (query.hasOwnProperty(\"filter\")) {\n\t\t\t\t\tdocResults = this._getUnique(query.filter.values, false, docResults);\n\t\t\t\t}\n\n\t\t\t\tif (query.hasOwnProperty(\"should\")) {\n\t\t\t\t\tlet shouldDocs = this._getAll(query.should.values, doScoring);\n\n\t\t\t\t\tlet empty = false;\n\t\t\t\t\tif (docResults === null) {\n\t\t\t\t\t\tdocResults = {};\n\t\t\t\t\t\tempty = true;\n\t\t\t\t\t}\n\n\t\t\t\t\tlet msm = query.hasOwnProperty(\"minimum_should_match\") ? query.minimum_should_match : 1;\n\t\t\t\t\t// Remove all docs with fewer matches.\n\t\t\t\t\t// TODO: Enable percent, negative values and ranges.\n\t\t\t\t\tlet docs = Object.keys(shouldDocs);\n\t\t\t\t\tfor (let i = 0, docId; i < docs.length, docId = docs[i]; i++) {\n\t\t\t\t\t\tif (shouldDocs[docId].length >= msm) {\n\t\t\t\t\t\t\tif (docResults.hasOwnProperty(docId)) {\n\t\t\t\t\t\t\t\tdocResults[docId].push(...shouldDocs[docId]);\n\t\t\t\t\t\t\t} else if (empty) {\n\t\t\t\t\t\t\t\tdocResults[docId] = shouldDocs[docId];\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tdelete docResults[docId];\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (query.hasOwnProperty(\"not\")) {\n\t\t\t\t\tlet notDocs = this._getAll(query.not.values, false);\n\t\t\t\t\t// Remove all docs.\n\t\t\t\t\tlet docs = Object.keys(notDocs);\n\t\t\t\t\tfor (let i = 0, docId; i < docs.length, docId = docs[i]; i++) {\n\t\t\t\t\t\tif (docResults.hasOwnProperty(docId)) {\n\t\t\t\t\t\t\tdelete docResults[docId];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"term\": {\n\t\t\t\tlet termIdx = __WEBPACK_IMPORTED_MODULE_1__inverted_index__[\"a\" /* InvertedIndex */].getTermIndex(query.value, root);\n\t\t\t\tthis._scorer.prepare(fieldName, boost, termIdx, doScoring, docResults, query.value);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"terms\": {\n\t\t\t\tfor (let i = 0; i < query.value.length; i++) {\n\t\t\t\t\tlet termIdx = __WEBPACK_IMPORTED_MODULE_1__inverted_index__[\"a\" /* InvertedIndex */].getTermIndex(query.value[i], root);\n\t\t\t\t\tthis._scorer.prepare(fieldName, boost, termIdx, doScoring, docResults, query.value[i]);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"fuzzy\": {\n\t\t\t\tlet f = new FuzzySearch(query);\n\t\t\t\tlet b = f.search(root);\n\t\t\t\tfor (let i = 0; i < b.length; i++) {\n\t\t\t\t\tthis._scorer.prepare(fieldName, boost * b[i].boost, b[i].index, doScoring, docResults, b[i].term);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"wildcard\": {\n\t\t\t\tlet w = new WildcardSearch(query);\n\t\t\t\tlet a = w.search(root);\n\t\t\t\tfor (let i = 0; i < a.length; i++) {\n\t\t\t\t\tthis._scorer.prepare(fieldName, boost, a[i].index, doScoring, docResults, a[i].term);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"match_all\": {\n\t\t\t\tfor (let docId of this._docs) {\n\t\t\t\t\tthis._scorer.scoreConstant(boost, docId, docResults);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"constant_score\": {\n\t\t\t\tdocResults = this._getAll(query.filter.values, false);\n\t\t\t\tlet docs = Object.keys(docResults);\n\t\t\t\t// Add to each document a constant score.\n\t\t\t\tfor (let i = 0; i < docs.length; i++) {\n\t\t\t\t\tthis._scorer.scoreConstant(boost, docs[i], docResults);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"prefix\": {\n\t\t\t\tlet termIdx = __WEBPACK_IMPORTED_MODULE_1__inverted_index__[\"a\" /* InvertedIndex */].getTermIndex(query.value, root);\n\t\t\t\tif (termIdx != null) {\n\t\t\t\t\ttermIdx = __WEBPACK_IMPORTED_MODULE_1__inverted_index__[\"a\" /* InvertedIndex */].extendTermIndex(termIdx);\n\t\t\t\t}\n\t\t\t\tfor (let i = 0; i < termIdx.length; i++) {\n\t\t\t\t\tthis._scorer.prepare(fieldName, boost, termIdx[i].index, doScoring, docResults, query.value + termIdx[i].term);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"exists\": {\n\t\t\t\tif (root != null) {\n\t\t\t\t\tlet docs = Object.keys(this._invIdxs[fieldName].documentStore);\n\t\t\t\t\tfor (let i = 0; i < docs.length; i++) {\n\t\t\t\t\t\tthis._scorer.scoreConstant(boost, docs[i], docResults);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"match\": {\n\t\t\t\tlet terms = tokenizer.tokenize(query.value);\n\t\t\t\tlet operator = query.hasOwnProperty(\"operator\") ? query.operator : \"or\";\n\n\t\t\t\tlet tmpQuery = new __WEBPACK_IMPORTED_MODULE_2__queries__[\"a\" /* QueryBuilder */]().bool();\n\t\t\t\tif (operator === \"or\") {\n\t\t\t\t\tif (query.hasOwnProperty(\"minimum_should_match\")) {\n\t\t\t\t\t\ttmpQuery = tmpQuery.minimumShouldMatch(query.minimum_should_match);\n\t\t\t\t\t}\n\t\t\t\t\t// Build a should query.\n\t\t\t\t\ttmpQuery = tmpQuery.startShould();\n\t\t\t\t} else {\n\t\t\t\t\t// Build a must query.\n\t\t\t\t\ttmpQuery = tmpQuery.startMust();\n\t\t\t\t}\n\t\t\t\ttmpQuery = tmpQuery.boost(boost);\n\n\t\t\t\tif (query.hasOwnProperty(\"fuzziness\")) {\n\t\t\t\t\tlet prefixLength = query.hasOwnProperty(\"prefix_length\") ? query.prefix_length : 2;\n\t\t\t\t\t// Add each fuzzy.\n\t\t\t\t\tfor (let i = 0; i < terms.length; i++) {\n\t\t\t\t\t\ttmpQuery = tmpQuery.fuzzy(fieldName, terms[i]).fuzziness(query.fuzziness).prefixLength(prefixLength);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// Add each term.\n\t\t\t\t\tfor (let i = 0; i < terms.length; i++) {\n\t\t\t\t\t\ttmpQuery = tmpQuery.term(fieldName, terms[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (operator === \"or\") {\n\t\t\t\t\ttmpQuery = tmpQuery.endShould();\n\t\t\t\t} else {\n\t\t\t\t\ttmpQuery = tmpQuery.endMust();\n\t\t\t\t}\n\n\t\t\t\tdocResults = this._recursive(tmpQuery.build(), doScoring);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t}\n\t\treturn docResults;\n\t}\n\n\t_getUnique(values, doScoring, docResults) {\n\t\tif (values.length === 0) {\n\t\t\treturn docResults;\n\t\t}\n\n\t\tfor (let i = 0; i < values.length; i++) {\n\t\t\tlet currDocs = this._recursive(values[i], doScoring);\n\t\t\tif (docResults === null) {\n\t\t\t\tdocResults = this._recursive(values[0], doScoring);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tlet docs = Object.keys(docResults);\n\t\t\tfor (let j = 0, docId; j < docs.length, docId = docs[j]; j++) {\n\t\t\t\tif (!currDocs.hasOwnProperty(docId)) {\n\t\t\t\t\tdelete docResults[docId];\n\t\t\t\t} else {\n\t\t\t\t\tdocResults[docId].push(...currDocs[docId]);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn docResults;\n\t}\n\n\t_getAll(values, doScoring) {\n\t\tlet docResults = {};\n\t\tfor (let i = 0; i < values.length; i++) {\n\t\t\tlet currDocs = this._recursive(values[i], doScoring);\n\t\t\tlet docs = Object.keys(currDocs);\n\t\t\tfor (let j = 0, docId; j < docs.length, docId = docs[j]; j++) {\n\t\t\t\tif (!docResults.hasOwnProperty(docId)) {\n\t\t\t\t\tdocResults[docId] = currDocs[docId];\n\t\t\t\t} else {\n\t\t\t\t\tdocResults[docId].push(...currDocs[docId]);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn docResults;\n\t}\n}\n/* harmony export (immutable) */ __webpack_exports__[\"a\"] = IndexSearcher;\n\n\n\nclass FuzzySearch {\n\tconstructor(query) {\n\t\tthis._fuzzy = query.value;\n\t\tthis._fuzziness = query.hasOwnProperty('fuzziness') ? query.fuzziness : 2;\n\t\tthis._prefixLength = query.hasOwnProperty('prefix_length') ? query.prefix_length : 2;\n\t}\n\n\t/**\n\t * Copyright Kigiri: https://github.com/kigiri\n\t *                     Milot Mirdita: https://github.com/milot-mirdita\n\t *                     Toni Neubert:  https://github.com/Viatorus/\n\t */\n\tlevenshtein_distance(a, b) {\n\t\tif (a.length === 0) return b.length;\n\t\tif (b.length === 0) return a.length;\n\t\tlet tmp, i, j, prev, val;\n\t\t// swap to save some memory O(min(a,b)) instead of O(a)\n\t\tif (a.length > b.length) {\n\t\t\ttmp = a;\n\t\t\ta = b;\n\t\t\tb = tmp;\n\t\t}\n\n\t\tvar row = Array(a.length + 1);\n\t\t// init the row\n\t\tfor (i = 0; i <= a.length; i++) {\n\t\t\trow[i] = i;\n\t\t}\n\n\t\t// fill in the rest\n\t\tfor (i = 1; i <= b.length; i++) {\n\t\t\tprev = i;\n\t\t\tfor (j = 1; j <= a.length; j++) {\n\t\t\t\tif (b[i - 1] === a[j - 1]) {\t// match\n\t\t\t\t\tval = row[j - 1];\n\t\t\t\t} else {\n\t\t\t\t\tval = Math.min(row[j - 1] + 1, // substitution\n\t\t\t\t\t\tMath.min(prev + 1,         // insertion\n\t\t\t\t\t\t\trow[j] + 1));          // deletion\n\n\t\t\t\t\t// transposition.\n\t\t\t\t\tif (i > 1 && j > 1 && b[i - 2] === a[j - 1] && a[j - 2] === b[i - 1]) {\n\t\t\t\t\t\tval = Math.min(val, row[j - 1] - (a[j - 1] === b[i - 1] ? 1 : 0));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trow[j - 1] = prev;\n\t\t\t\tprev = val;\n\t\t\t}\n\t\t\trow[a.length] = prev;\n\t\t}\n\t\treturn row[a.length];\n\t}\n\n\t/**\n\t * Performs a fuzzy search for a given term.\n\t * @param {string} query - a fuzzy term to match.\n\t * @param {number} [maxDistance=2] - maximal edit distance between terms\n\t * @returns {Array} - array with all matching term indices.\n\t */\n\tsearch(root) {\n\t\t// Todo: Include levenshtein to reduce similar iterations.\n\t\t// Tree tokens at same depth share same row until depth (should works if recursive).\n\t\t// Pregenerate tree token ?\n\t\t//var treeToken = Array(token.length + maxDistance);\n\n\t\tlet start = root;\n\t\tlet pre = this._fuzzy.slice(0, this._prefixLength);\n\t\tlet fuzzy = this._fuzzy;\n\t\tif (this._prefixLength != 0) {\n\t\t\tstart = __WEBPACK_IMPORTED_MODULE_1__inverted_index__[\"a\" /* InvertedIndex */].getTermIndex(pre, start);\n\t\t\tfuzzy = fuzzy.slice(this._prefixLength);\n\t\t}\n\t\tif (start === null) {\n\t\t\treturn [];\n\t\t}\n\n\t\tlet similarTokens = [];\n\n\t\tlet stack = [start];\n\t\tlet treeStack = [''];\n\t\tdo {\n\t\t\tlet root = stack.pop();\n\t\t\tlet treeTerms = treeStack.pop();\n\n\t\t\t// Compare tokens if they are in near distance.\n\t\t\tif (root.hasOwnProperty('df') && Math.abs(fuzzy.length - treeTerms.length) <= this._fuzziness) {\n\t\t\t\tconst distance = this.levenshtein_distance(fuzzy, treeTerms);\n\t\t\t\tif (distance <= this._fuzziness) {\n\t\t\t\t\t// Calculate boost.\n\t\t\t\t\tlet boost = 1 - distance / (pre.length + treeTerms.length);\n\t\t\t\t\tsimilarTokens.push({term: pre + treeTerms, index: root, boost: boost});\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Iterate over all subtrees.\n\t\t\t// If token from tree is not longer than maximal distance.\n\t\t\tif (treeTerms.length - fuzzy.length <= this._fuzziness) {\n\t\t\t\t// Iterate over all subtrees.\n\t\t\t\tlet keys = Object.keys(root);\n\t\t\t\tfor (let i = 0; i < keys.length; i++) {\n\t\t\t\t\tif (keys[i] !== 'docs' && keys[i] !== 'df') {\n\t\t\t\t\t\tstack.push(root[keys[i]]);\n\t\t\t\t\t\ttreeStack.push(treeTerms + keys[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t} while (stack.length !== 0);\n\n\t\treturn similarTokens;\n\t}\n}\n\nclass WildcardSearch {\n\n\tconstructor(query) {\n\t\tthis._wildcard = query.value;\n\t\tthis._result = [];\n\t}\n\n\t/**\n\t * Performs a wild card search for a given query term.\n\t * @param {string} query - a wild card query to match.\n\t * @returns {Array} - array with all matching term indices.\n\t */\n\tsearch(root) {\n\t\t// Todo: Need an implementation for star operator in the middle.\n\t\tthis._result = [];\n\t\tthis._recursive(root);\n\t\treturn this._result;\n\t}\n\n\t/**\n\t *\n\t * @param root\n\t * @param idx\n\t * @param term\n\t * @param escaped\n\t * @private\n\t */\n\t_recursive(root, idx = 0, term = '', escaped = false) {\n\t\tif (root === null) {\n\t\t\treturn;\n\t\t}\n\n\t\tif (idx === this._wildcard.length) {\n\t\t\tif (root.hasOwnProperty('df')) {\n\t\t\t\tthis._result.push({index: root, term: term});\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\n\t\tif (!escaped && this._wildcard[idx] === '\\\\') {\n\t\t\tthis._recursive(root, idx + 1, term, true);\n\t\t} else if (!escaped && this._wildcard[idx] === '?') {\n\t\t\tlet others = __WEBPACK_IMPORTED_MODULE_1__inverted_index__[\"a\" /* InvertedIndex */].getNextTermIndex(root);\n\t\t\tfor (let i = 0; i < others.length; i++) {\n\t\t\t\tthis._recursive(others[i].index, idx + 1, term + others[i].term);\n\t\t\t}\n\t\t} else if (!escaped && this._wildcard[idx] === '*') {\n\t\t\tlet all = __WEBPACK_IMPORTED_MODULE_1__inverted_index__[\"a\" /* InvertedIndex */].extendTermIndex(root);\n\t\t\tfor (let i = 0; i < all.length; i++) {\n\t\t\t\tthis._recursive(all[i].index, idx + 1, term + all[i].term);\n\t\t\t}\n\t\t} else {\n\t\t\tthis._recursive(__WEBPACK_IMPORTED_MODULE_1__inverted_index__[\"a\" /* InvertedIndex */].getTermIndex(this._wildcard[idx], root), idx + 1, term + this._wildcard[idx]);\n\t\t}\n\t}\n}\n\n\n/***/ }),\n/* 7 */\n/***/ (function(module, __webpack_exports__, __webpack_require__) {\n\n\"use strict\";\nclass Scorer {\n\tconstructor(invIdxs) {\n\t\tthis._invIdxs = invIdxs;\n\t\tthis._cache = {};\n\t}\n\n\tsetDirty() {\n\t\tthis._cache = {};\n\t}\n\n\tprepare(fieldName, boost, termIdx, doScoring, docResults = {}, term = null) {\n\t\tif (termIdx == null || !termIdx.hasOwnProperty('docs')) {\n\t\t\treturn null;\n\t\t}\n\n\t\tlet idf = this._idf(fieldName, termIdx.df);\n\t\tlet docIds = Object.keys(termIdx.docs);\n\t\tfor (let j = 0; j < docIds.length; j++) {\n\t\t\tlet docId = docIds[j];\n\t\t\tif (!docResults.hasOwnProperty(docId)) {\n\t\t\t\tdocResults[docId] = [];\n\t\t\t}\n\n\t\t\tif (doScoring) {\n\t\t\t\tlet tf = termIdx.docs[docId];\n\t\t\t\tdocResults[docId].push({\n\t\t\t\t\ttype: 'BM25',\n\t\t\t\t\ttf: tf,\n\t\t\t\t\tidf: idf,\n\t\t\t\t\tboost: boost,\n\t\t\t\t\tfieldName: fieldName,\n\t\t\t\t\tterm: term\n\t\t\t\t});\n\t\t\t} else {\n\t\t\t\t// TODO: Maybe only 1 constant store per document\n\t\t\t\tdocResults[docId].push({\n\t\t\t\t\ttype: \"constant\", value: 1, boost: boost, fieldName: fieldName,\n\t\t\t\t\tterm: term\n\t\t\t\t});\n\t\t\t}\n\t\t}\n\n\t\treturn docResults;\n\t}\n\n\tscoreConstant(boost, docId, docResults = {}) {\n\t\tif (!docResults.hasOwnProperty(docId)) {\n\t\t\tdocResults[docId] = [];\n\t\t}\n\t\tdocResults[docId].push({type: \"constant\", value: 1, boost: boost});\n\t\treturn docResults;\n\t}\n\n\tfinalScore(query, docResults = {}) {\n\n\t\tlet result = {};\n\t\tlet k1 = query.scoring.k1;\n\t\tlet b = query.scoring.b;\n\n\t\tlet docs = Object.keys(docResults);\n\t\tfor (let i = 0, docId; i < docs.length, docId = docs[i]; i++) {\n\t\t\tlet docScore = 0;\n\t\t\tfor (let j = 0; j < docResults[docId].length; j++) {\n\t\t\t\tlet docResult = docResults[docId][j];\n\n\t\t\t\tlet res = 0;\n\t\t\t\tswitch (docResult.type) {\n\t\t\t\t\tcase 'BM25': {\n\t\t\t\t\t\tlet fieldLength = this._invIdxs[docResult.fieldName].documentStore[docId].fieldLength /\n\t\t\t\t\t\t\tMath.pow(this._invIdxs[docResult.fieldName].documentStore[docId].boost, 2);\n\t\t\t\t\t\tlet avgFieldLength = this._avgFieldLength(docResult.fieldName);\n\t\t\t\t\t\tlet tfNorm = ((k1 + 1) * docResult.tf) / (k1 * ((1 - b)\n\t\t\t\t\t\t\t+ b * (fieldLength / avgFieldLength)) + docResult.tf);\n\t\t\t\t\t\tres = docResult.idf * tfNorm * docResult.boost;\n\t\t\t\t\t\t/*console.log(\n\t\t\t\t\t\t\tdocId + \":\" + docResult.fieldName + \":\" + docResult.term + \" = \" + res,\n\t\t\t\t\t\t\t\"\\n\\ttype: BM25\",\n\t\t\t\t\t\t\t\"\\n\\tboost: \" + docResult.boost,\n\t\t\t\t\t\t\t\"\\n\\tidf : \" + docResult.idf,\n\t\t\t\t\t\t\t\"\\n\\ttfNorm : \" + tfNorm,\n\t\t\t\t\t\t\t\"\\n\\ttf : \" + docResult.tf,\n\t\t\t\t\t\t\t\"\\n\\tavg : \" + avgFieldLength,\n\t\t\t\t\t\t\t\"\\n\\tfl : \" + fieldLength);*/\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\tcase 'constant':\n\t\t\t\t\t\tres = docResult.value * docResult.boost;\n\t\t\t\t\t\t/*console.log(\n\t\t\t\t\t\t\t\"Constant: \" + res,\n\t\t\t\t\t\t\t\"\\n\\tboost: \" + docResult.boost,\n\t\t\t\t\t\t\t\"\\n\\tvalue : \" + docResult.value);*/\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdocScore += res;\n\t\t\t}\n\t\t\t//console.log(docId, \" === \", docScore);\n\t\t\tresult[docId] = docScore;\n\t\t}\n\t\treturn result;\n\t}\n\n\t_getCache(fieldName) {\n\t\tif (!this._cache.hasOwnProperty(fieldName)) {\n\t\t\tlet avgFieldLength = this._invIdxs[fieldName].totalFieldLength / this._invIdxs[fieldName].documentCount;\n\t\t\tthis._cache[fieldName] = {idfs: {}, avgFieldLength: avgFieldLength};\n\t\t}\n\t\treturn this._cache[fieldName];\n\t}\n\n\t/**\n\t * Returns the idf by either calculate it or use a cached one.\n\t * @param {number} docFreq - the doc frequency of the term\n\t * @returns {number} the idf\n\t * @private\n\t */\n\t_idf(fieldName, docFreq) {\n\t\tlet cache = this._getCache(fieldName);\n\t\tif (cache.idfs.hasOwnProperty(String(docFreq))) {\n\t\t\treturn cache.idfs[docFreq];\n\t\t}\n\t\treturn cache.idfs[docFreq] = Math.log(1 + (this._invIdxs[fieldName].documentCount - docFreq + 0.5) / (docFreq + 0.5));\n\t}\n\n\t_avgFieldLength(fieldName) {\n\t\treturn this._getCache(fieldName).avgFieldLength;\n\t}\n}\n/* harmony export (immutable) */ __webpack_exports__[\"a\"] = Scorer;\n\n\n\n/***/ })\n/******/ ]);\n});\n\n\n// WEBPACK FOOTER //\n// loki.FullTextSearch.min.js"," \t// The module cache\n \tvar installedModules = {};\n\n \t// The require function\n \tfunction __webpack_require__(moduleId) {\n\n \t\t// Check if module is in cache\n \t\tif(installedModules[moduleId]) {\n \t\t\treturn installedModules[moduleId].exports;\n \t\t}\n \t\t// Create a new module (and put it into the cache)\n \t\tvar module = installedModules[moduleId] = {\n \t\t\ti: moduleId,\n \t\t\tl: false,\n \t\t\texports: {}\n \t\t};\n\n \t\t// Execute the module function\n \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n \t\t// Flag the module as loaded\n \t\tmodule.l = true;\n\n \t\t// Return the exports of the module\n \t\treturn module.exports;\n \t}\n\n\n \t// expose the modules object (__webpack_modules__)\n \t__webpack_require__.m = modules;\n\n \t// expose the module cache\n \t__webpack_require__.c = installedModules;\n\n \t// identity function for calling harmony imports with the correct context\n \t__webpack_require__.i = function(value) { return value; };\n\n \t// define getter function for harmony exports\n \t__webpack_require__.d = function(exports, name, getter) {\n \t\tif(!__webpack_require__.o(exports, name)) {\n \t\t\tObject.defineProperty(exports, name, {\n \t\t\t\tconfigurable: false,\n \t\t\t\tenumerable: true,\n \t\t\t\tget: getter\n \t\t\t});\n \t\t}\n \t};\n\n \t// getDefaultExport function for compatibility with non-harmony modules\n \t__webpack_require__.n = function(module) {\n \t\tvar getter = module && module.__esModule ?\n \t\t\tfunction getDefault() { return module['default']; } :\n \t\t\tfunction getModuleExports() { return module; };\n \t\t__webpack_require__.d(getter, 'a', getter);\n \t\treturn getter;\n \t};\n\n \t// Object.prototype.hasOwnProperty.call\n \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n \t// __webpack_public_path__\n \t__webpack_require__.p = \"\";\n\n \t// Load entry module and return exports\n \treturn __webpack_require__(__webpack_require__.s = 5);\n\n\n\n// WEBPACK FOOTER //\n// webpack/bootstrap ddc8f5cab96806dc262d","import * as Utils from './utils.js';\n\n/**\n * Splits a string at non-alphanumeric characters into lower case tokens.\n * @param {string} str - the string\n * @returns {string[]} - the tokens\n * @private\n */\nfunction defaultSplitter(str) {\n\tlet trimmedTokens = [];\n\tlet tokens = str.split(/[^\\w]/);\n\tfor (let i = 0; i < tokens.length; i++) {\n\t\tif (tokens[i] !== '') {\n\t\t\ttrimmedTokens.push(tokens[i].toLowerCase());\n\t\t}\n\t}\n\treturn trimmedTokens;\n}\n\n/**\n * The tokenizer is used to prepare the string content of a document field for the inverted index.\n * Firstly the string gets split into tokens.\n * After that the tokens will be trimmed/stemmed with defined functions from the queue.\n *\n * * To change the splitter function, use {@link Tokenizer#setSplitter}.\n * * To add functions to the queue, use {@link Tokenizer#add}, {@link Tokenizer#addBefore} and\n *   {@link Tokenizer#addAfter}.\n * * To remove a function from the queue, use {@link Tokenizer#remove}.\n * * To reset the tokenizer, use {@link Tokenizer#reset}.\n */\nexport class Tokenizer {\n\t/**\n\t * Initializes the tokenizer with a splitter, which splits a string at non-alphanumeric characters.\n\t * The queue is empty.\n\t */\n\tconstructor() {\n\t\tthis._splitter = null;\n\t\tthis._queue = [];\n\t\tthis._symbol = Symbol('label');\n\t\tthis.reset();\n\t}\n\n\t/**\n\t * Sets a function with defined label as the splitter function.\n\t * The function must take a string as argument and return an array of tokens.\n\t *\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t */\n\tsetSplitter(label, func) {\n\t\tlabel = Utils.asString(label);\n\t\tif (!Utils.isFunction(func)) {\n\t\t\tthrow TypeError(\"Splitter must be a function.\");\n\t\t}\n\t\tif (label === \"\") {\n\t\t\tthrow Error(\"Label cannot be empty.\");\n\t\t}\n\t\tfunc[this._symbol] = label;\n\t\tthis._splitter = func;\n\t}\n\n\t/**\n\t * Gets the splitter.\n\t * @return {Array.<string, function>} - tuple with label and function\n\t */\n\tgetSplitter() {\n\t\treturn [this._splitter[this._symbol], this._splitter];\n\t}\n\n\t/**\n\t * Resets the splitter to default.\n\t */\n\tresetSplitter() {\n\t\tthis._splitter = defaultSplitter;\n\t}\n\n\t/**\n\t * Checks if a function is inside the queue.\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @returns {boolean} true if exists, otherwise false\n\t */\n\thas(labelFunc) {\n\t\treturn this._getPosition(labelFunc) !== -1;\n\t}\n\n\t/**\n\t * Gets a function from the queue.\n\t * Only the first found function gets returned if a label or a function is multiple used.\n\t *\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @return {Array.<string, function>} - tuple with label and function\n\t */\n\tget(labelFunc) {\n\t\tlet pos = this._getPosition(labelFunc);\n\t\tif (pos === -1) {\n\t\t\tthrow Error('Cannot find existing function.');\n\t\t}\n\t\treturn [this._queue[pos][this._symbol], this._queue[pos]];\n\t}\n\n\t/**\n\t * Adds a function with defined label to the end of the queue.\n\t * The function must take an array of tokens as argument and return an array of tokens.\n\t *\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t */\n\tadd(label, func) {\n\t\tthis._addFunction(label, func, this._queue.length);\n\t}\n\n\t/**\n\t * Adds a function with defined label before an existing function to the queue.\n\t * The function must take an array of tokens as argument and return an array of tokens.\n\t *\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t */\n\taddBefore(labelFunc, label, func) {\n\t\tlet pos = this._getPosition(labelFunc);\n\t\tif (pos === -1) {\n\t\t\tthrow Error('Cannot find existing function.');\n\t\t}\n\t\tthis._addFunction(label, func, pos);\n\t}\n\n\t/**\n\t * Adds a function with defined label after an existing function to the queue.\n\t * The function must take an array of tokens as argument and return an array of tokens.\n\t *\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t */\n\taddAfter(labelFunc, label, func) {\n\t\tlet pos = this._getPosition(labelFunc);\n\t\tif (pos === -1) {\n\t\t\tthrow Error('Cannot find existing function.');\n\t\t}\n\t\tthis._addFunction(label, func, pos + 1);\n\t}\n\n\t/**\n\t * Removes a function from the queue.\n\t * @param {string|function} labelFunc - an existing label or function\n\t */\n\tremove(labelFunc) {\n\t\tlet pos = this._getPosition(labelFunc);\n\t\tif (pos === -1) {\n\t\t\tthrow Error('Cannot find existing function.');\n\t\t}\n\t\tthis._queue.splice(pos, 1);\n\t}\n\n\t/**\n\t * Resets the splitter and tokenize queue to default.\n\t */\n\treset() {\n\t\tthis._splitter = defaultSplitter;\n\t\tthis._queue = [];\n\t}\n\n\t/**\n\t * Tokenizes a string into tokens.\n\t * @param {string} str - the string\n\t * @return {string[]} the tokens\n\t * @protected\n\t */\n\ttokenize(str) {\n\t\tlet tokens = this._splitter(str);\n\t\tfor (let i = 0; i < this._queue.length; i++) {\n\t\t\ttokens = this._queue[i](tokens);\n\t\t}\n\t\treturn tokens;\n\t}\n\n\t/**\n\t * Serializes the tokenizer by returning the labels of the used functions.\n\t * @returns {{splitter: string?, tokenizers: string[]}} - the serialization\n\t * @protected\n\t */\n\ttoJSON() {\n\t\tlet serialized = {tokenizers: []};\n\t\tif (this._splitter !== defaultSplitter) {\n\t\t\tserialized.splitter = this._splitter[this._symbol];\n\t\t}\n\t\tfor (let i = 0; i < this._queue.length; i++) {\n\t\t\tserialized.tokenizers.push(this._queue[i][this._symbol]);\n\t\t}\n\t\treturn serialized;\n\t}\n\n\t/**\n\t * Deserializes the tokenizer by reassign the correct function to each label.\n\t * @param {{splitter: string, tokenizers: string[]}} serialized - the serialized labels\n\t * @param {Object.<string, function>|Tokenizer} funcTok - the depending functions with labels\n\t * \tor an equivalent tokenizer\n\t */\n\tstatic fromJSON(serialized, funcTok) {\n\t\tlet tokenizer = new Tokenizer();\n\n\t\tif (funcTok !== undefined && funcTok instanceof Tokenizer) {\n\t\t\tif (serialized.hasOwnProperty(\"splitter\")) {\n\t\t\t\tlet splitter = funcTok.getSplitter();\n\t\t\t\tif (serialized.splitter !== splitter[0]) {\n\t\t\t\t\tthrow Error(\"Splitter function not found.\");\n\t\t\t\t}\n\t\t\t\ttokenizer.setSplitter(splitter[0], splitter[1]);\n\t\t\t}\n\n\t\t\tfor (let i = 0; i < serialized.tokenizers.length; i++) {\n\t\t\t\tif (!funcTok.has(serialized.tokenizers[i])) {\n\t\t\t\t\tthrow Error(\"Tokenizer function not found.\");\n\t\t\t\t}\n\t\t\t\tlet labelFunc = funcTok.get(serialized.tokenizers[i]);\n\t\t\t\ttokenizer.add(labelFunc[0], labelFunc[1]);\n\t\t\t}\n\t\t} else {\n\t\t\tif (serialized.hasOwnProperty(\"splitter\")) {\n\t\t\t\tif (!funcTok.splitters.hasOwnProperty(serialized.splitter)) {\n\t\t\t\t\tthrow Error(\"Splitter function not found.\");\n\t\t\t\t}\n\t\t\t\ttokenizer.setSplitter(serialized.splitter, funcTok.splitters[serialized.splitter]);\n\t\t\t}\n\t\t\tfor (let i = 0; i < serialized.tokenizers.length; i++) {\n\t\t\t\tif (!funcTok.tokenizers.hasOwnProperty(serialized.tokenizers[i])) {\n\t\t\t\t\tthrow Error(\"Tokenizer function not found.\");\n\t\t\t\t}\n\t\t\t\ttokenizer.add(serialized.tokenizers[i], funcTok.tokenizers[serialized.tokenizers[i]]);\n\t\t\t}\n\t\t}\n\t\treturn tokenizer;\n\t}\n\n\t/**\n\t * Returns the position of a function inside the queue.\n\t * @param {string|function} labelFunc - an existing label or function\n\t * @return {number} the position\n\t * @private\n\t */\n\t_getPosition(labelFunc) {\n\t\tif (Utils.isFunction(labelFunc)) {\n\t\t\treturn this._queue.indexOf(labelFunc);\n\t\t} else if (Utils.isConvertibleToString(labelFunc)) {\n\t\t\tlabelFunc = String(labelFunc);\n\t\t\tfor (let i = 0; i < this._queue.length; i++) {\n\t\t\t\tif (this._queue[i][this._symbol] === labelFunc) {\n\t\t\t\t\treturn i;\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tthrow TypeError(\"Type of labelFunc must be string or function.\");\n\t\t}\n\t\treturn -1;\n\t}\n\n\t/**\n\t * Adds a function with defined label at a specific position to the queue.\n\t * @param {string} label - the label\n\t * @param {function} func - the function\n\t * @param {number} pos - the position\n\t * @private\n\t */\n\t_addFunction(label, func, pos) {\n\t\tlabel = Utils.asString(label);\n\t\tif (!Utils.isFunction(func)) {\n\t\t\tthrow TypeError(\"Type of func must be function.\");\n\t\t}\n\t\tif (label === \"\") {\n\t\t\tthrow Error(\"Label cannot be empty.\");\n\t\t}\n\t\tfunc[this._symbol] = label;\n\t\tthis._queue.splice(pos, 0, func);\n\t}\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/inverted_index/tokenizer.js\n// module id = 0\n// module chunks = 0","/**\n * Checks if the variable is a function.\n * @param {*} x - the variable\n * @return {boolean} true if function, otherwise false\n * @protected\n */\nexport function isFunction(x) {\n\treturn Object.prototype.toString.call(x) === \"[object Function]\";\n}\n\n/**\n * Checks if the variable is an object.\n * @param {*} x - the variable\n * @return {boolean} true if object, otherwise false\n * @protected\n */\nexport function isObject(x) {\n\treturn Object.prototype.toString.call(x) === \"[object Object]\";\n}\n\n/**\n * Checks if the variable is a number.\n * @param {*} x - the variable\n * @return {boolean} true if number, otherwise false\n * @protected\n */\nexport function isNumber(x) {\n\treturn Object.prototype.toString.call(x) === \"[object Number]\";\n}\n\n/**\n * Checks if the variable is a boolean.\n * @param {*} x - the variable\n * @return {boolean} true if boolean, otherwise false\n * @protected\n */\nexport function isBoolean(x) {\n\treturn Object.prototype.toString.call(x) === \"[object Boolean]\";\n}\n\n/**\n * Checks if the variable is a string.\n * @param {*} x - the variable\n * @return {boolean} true if string, otherwise false\n * @protected\n */\nexport function isString(x) {\n\treturn Object.prototype.toString.call(x) === \"[object String]\";\n}\n\n/**\n * Checks if the variable is convertible to a string.\n * @param {*} x - the variable\n * @return {boolean} true if convertible, otherwise false\n */\nexport function isConvertibleToString(x) {\n\treturn isString(x) || isNumber(x) || isObject(x) && Object.prototype.toString !== x.toString && isString(x.toString());\n}\n\n/**\n * Converts a variable to a boolean (from boolean or number).\n * Throws an error if not possible.\n * @param {*} x - the variable\n * @param {error} [error=TypeError] - the error to throw\n * @return {boolean} the converted boolean\n * @protected\n */\nexport function asBoolean(x, error = TypeError(\"Value is not convertible to boolean\")) {\n\tif (isBoolean(x) || isNumber(x)) {\n\t\treturn Boolean(x);\n\t}\n\tthrow error;\n}\n\n/**\n * Converts a variable to a string (from string, number or obj.toString).\n * Throws an error if not possible.\n * @param {*} x - the variable\n * @param {error} [error=TypeError] - the error to throw\n * @return {string} the converted string\n * @protected\n */\nexport function asString(x, error = TypeError(\"Value is not convertible to string.\")) {\n\tif (isConvertibleToString(x)) {\n\t\treturn String(x);\n\t}\n\tthrow error;\n}\n\n/**\n * Converts a variable to a array of string (from an array of string, number or obj.toString).\n * Throws an error if not possible.\n * @param {*} x - the variable\n * @param {error} [error=TypeError] - the error to throw\n * @return {string[]} the converted array of string\n * @protected\n */\nexport function asArrayOfString(x, error = TypeError(\"Value is not convertible to an array of strings.\")) {\n\tif (!Array.isArray(x)) {\n\t\tthrow error;\n\t}\n\tlet array = [];\n\tfor (let i = 0; i < x.length; i++) {\n\t\tif (!isConvertibleToString(x[i])) {\n\t\t\tthrow error;\n\t\t}\n\t\tarray.push(String(x[i]));\n\t}\n\treturn array;\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/inverted_index/utils.js\n// module id = 1\n// module chunks = 0","/**\n * Query builder\n * todo: Document scoring.\n * todo: Align description.\n */\nimport * as Utils from './utils.js';\n\n/**\n * The base query class to enable boost to a query type.\n *\n * @param {string} type - the type name of the query\n */\nexport class BaseQuery {\n\tconstructor(type, data = {}) {\n\t\tthis._data = data;\n\t\tthis._data.type = Utils.asString(type);\n\t}\n\n\t/**\n\t * Boosts the query result.\n\t *\n\t * See also [Lucene#BoostQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/BoostQuery.html}\n\t * and [Elasticsearch#boost]{@link https://www.elastic.co/guide/en/elasticsearch/reference/5.2/mapping-boost.html}.\n\t *\n\t * @param {number} value - the positive boost\n\t * @return {BaseQuery} object itself for cascading\n\t */\n\tboost(value) {\n\t\tif (!Utils.isNumber(value) || value < 0) {\n\t\t\tthrow TypeError(\"Boost must be a positive number.\");\n\t\t}\n\t\tthis._data.boost = value;\n\t\treturn this;\n\t}\n\n\t/**\n\t * Build the final query.\n\t * @return {Object} - the final query\n\t */\n\tbuild() {\n\t\treturn this._data;\n\t}\n}\n\n/**\n * A query which finds documents where a document field contains a term.\n *\n * See also [Lucene#TermQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/TermQuery.html}\n * and [Elasticsearch#TermQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-term-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .term(\"name\", \"infinity\"])\n * .build();\n * // The resulting documents:\n * // contains the term infinity\n *\n * @param {string} field - the field name of the document\n * @param {string} term - the term\n * @extends BaseQuery\n */\nexport class TermQuery extends BaseQuery {\n\tconstructor(field, term, data = {}) {\n\t\tsuper(\"term\", data);\n\t\tthis._data.field = Utils.asString(field);\n\t\tthis._data.value = Utils.asString(term);\n\t}\n}\n\n/**\n * A query which finds documents where a document field contains any of the terms.\n *\n * See also [Lucene#TermRangeQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/TermRangeQuery.html}\n * and [Elasticsearch#TermsQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-terms-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .terms(\"quotes\", [\"infinity\", \"atom\", \"energy\"])\n * .build();\n * // The resulting documents:\n * // contains the terms infinity, atom or energy\n *\n * @param {string} field - the field name of the document\n * @param {string[]} terms - the terms\n * @extends BaseQuery\n */\nexport class TermsQuery extends BaseQuery {\n\tconstructor(field, terms, data = {}) {\n\t\tsuper(\"terms\", data);\n\t\tthis._data.field = Utils.asString(field);\n\t\tthis._data.value = Utils.asArrayOfString(terms);\n\t}\n}\n\n/**\n * A query which finds documents where the wildcard term can be applied at an existing document field term.\n *\n * Wildcard | Description\n * -------- | ------------\n * ? (question mark) | Skips a single character.\n *\n * To escape a wildcard character, use _\\_ (backslash), e.g. \\?.\n *\n * See also [Lucene#WildcardQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/WildcardQuery.html}\n * and [Elasticsearch#WildcardQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-wildcard-query.html}.\n *\n * _TODO: Implement wildcard * (asterisk) to skip zero or more characters._\n * @todo Implement wildcard * (asterisk) to skip zero or more characters.\n *\n * @example\n * new QueryBuilder()\n *   .wildcard(\"question\", \"e?nste?n\\?\")\n * .build();\n * // The resulting documents:\n * // contains the wildcard surname e?nste?n\\? (like Einstein? or Eynsteyn? but not Einsteine or Ensten?)\n *\n * @param {string} field - the field name of the document\n * @param {string} wildcard - the wildcard term\n * @extends BaseQuery\n */\nexport class WildcardQuery extends BaseQuery {\n\tconstructor(field, wildcard, data = {}) {\n\t\tsuper(\"wildcard\", data);\n\t\tthis._data.field = Utils.asString(field);\n\t\tthis._data.value = Utils.asString(wildcard);\n\t}\n}\n\n/**\n * A query which finds documents where the fuzzy term can be transformed into an existing document field term within a\n * given edit distance\n * ([Damerauâ€“Levenshtein distance]{@link https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance}).\n *\n * The edit distance is the minimum number of an insertion, deletion or substitution of a single character\n * or a transposition of two adjacent characters.\n *\n * * To set the maximal allowed edit distance, use {@link FuzzyQuery#fuzziness} (default is 2).\n * * To set the initial word length, which should ignored for fuzziness, use {@link FuzzyQuery#prefixLength}.\n *\n * See also [Lucene#FuzzyQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/FuzzyQuery.html}\n * and [Elasticsearch#FuzzyQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-fuzzy-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .fuzzy(\"surname\", \"einsten\")\n *     .fuzziness(3)\n *     .prefixLength(3)\n * .build();\n * // The resulting documents:\n * // contains the fuzzy surname einstn (like Einstein or Einst but not Eisstein or Insten)\n *\n * @param {string} field - the field name of the document\n * @param {string} fuzzy - the fuzzy term\n * @extends BaseQuery\n */\nexport class FuzzyQuery extends BaseQuery {\n\tconstructor(field, fuzzy, data = {}) {\n\t\tsuper(\"fuzzy\", data);\n\t\tthis._data.field = Utils.asString(field);\n\t\tthis._data.value = Utils.asString(fuzzy);\n\t}\n\n\t/**\n\t * Sets the maximal allowed fuzziness.\n\t * @param {number} fuzziness - the fuzziness\n\t * @return {FuzzyQuery} - object itself for cascading\n\t */\n\tfuzziness(fuzziness) {\n\t\tif (!Utils.isNumber(fuzziness) || fuzziness < 0) {\n\t\t\tthrow TypeError(\"Fuzziness must be a positive number.\");\n\t\t}\n\t\tthis._data.fuzziness = fuzziness;\n\t\treturn this;\n\t}\n\n\t/**\n\t * Sets the initial word length.\n\t * @param {number} prefixLength - the positive prefix length\n\t * @return {FuzzyQuery}  object itself for cascading\n\t */\n\tprefixLength(prefixLength) {\n\t\tif (!Utils.isNumber(prefixLength) || prefixLength < 0) {\n\t\t\tthrow TypeError(\"Prefix length must be a positive number.\");\n\t\t}\n\t\tthis._data.prefix_length = prefixLength;\n\t\treturn this;\n\t}\n}\n\n/**\n * A query which matches documents containing the prefix of a term inside a field.\n *\n * See also [Lucene#PrefixQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/PrefixQuery.html}\n * and [Elasticsearch#MatchQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-prefix-query.html}\n *\n * @example\n * new QueryBuilder()\n *   .prefix(\"surname\", \"alb\")\n * .build()\n * // The resulting documents:\n * // contains the term prefix alb as surname\n *\n * @param {string} field - the field name of the document\n * @param {string} prefix - the prefix of a term\n * @extends BaseQuery\n */\nexport class PrefixQuery extends BaseQuery {\n\tconstructor(field, prefix, data = {}) {\n\t\tsuper(\"prefix\", data);\n\t\tthis._data.field = Utils.asString(field);\n\t\tthis._data.value = Utils.asString(prefix);\n\t}\n}\n\n/**\n * A query which matches all documents with a given field.\n *\n * See also [Elasticsearch#ExistsQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-exists-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .exists(\"name\")\n * .build()\n * // The resulting documents:\n * // has the field \"name\"\n *\n * @param {string} field - the field name of the document\n * @extends BaseQuery\n */\nexport class ExistsQuery extends BaseQuery {\n\tconstructor(field, data = {}) {\n\t\tsuper(\"exists\", data);\n\t\tthis._data.field = Utils.asString(field);\n\t}\n}\n\n/**\n * A query which tokenizes the given query text, performs a query foreach token and combines the results using a boolean\n * operator.\n *\n * Operator      | Description\n * ------------- | -------------\n * or (default) | Finds documents which matches some tokens. The minimum amount of matches can be controlled with [minimumShouldMatch]{@link MatchQuery#minimumShouldMatch} (default is 1).\n * and | Finds documents which matches all tokens.\n *\n * To enable a [fuzzy query]{@link FuzzyQuery} for the tokens, use {@link MatchQuery#fuzziness} and {@link MatchQuery#prefixLength}.\n *\n * See also [Lucene#?]{@link ?}\n * and [Elasticsearch#MatchQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .match(\"name\", \"albrt einsten\")\n *     .boost(2.5)\n *     .operator(\"and\")\n *     .fuzziness(2)\n *     .prefixLength(3)\n * .build();\n * // The resulting documents:\n * // contains the fuzzy name albrt einsten (like Albert Einstein) with a boost of 2.5\n *\n * @param {string} field - the field name of the document\n * @param {string} query - the query text\n * @extends BaseQuery\n */\nexport class MatchQuery extends BaseQuery {\n\tconstructor(field, query, data = {}) {\n\t\tsuper(\"match\", data);\n\t\tthis._data.field = Utils.asString(field);\n\t\tthis._data.value = Utils.asString(query);\n\t}\n\n\t/**\n\t * Controls the amount of minimum matching token queries before a document will be considered.\n\t * @param {number} minShouldMatch - number of minimum matching sub queries\n\t * @return {MatchQuery} object itself for cascading\n\t */\n\tminimumShouldMatch(minShouldMatch) {\n\t\tif (!Utils.isNumber(minShouldMatch) || minShouldMatch < 0) {\n\t\t\tthrow TypeError(\"Value for minimum should match must be a positive number.\");\n\t\t}\n\t\tif (this._data.hasOwnProperty(\"operator\") && this._data.operator == \"and\") {\n\t\t\tthrow SyntaxError(\"Match query with \\\"and\\\" operator does not support minimum should match.\");\n\t\t}\n\t\tthis._data.minimum_should_match = minShouldMatch;\n\t\treturn this;\n\t}\n\n\t/**\n\t * Sets the boolean operator.\n\t * @param {string} op - the operator (_or_/_and_)\n\t * @return {MatchQuery} object itself for cascading\n\t */\n\toperator(op) {\n\t\top = Utils.asString(op);\n\t\tif (op != 'and' && op != 'or') {\n\t\t\tthrow SyntaxError(\"Unknown operator.\");\n\t\t}\n\t\tthis._data.operator = op;\n\t\tif (this._data.hasOwnProperty(\"minimum_should_match\") && this._data.operator == \"and\") {\n\t\t\tthrow SyntaxError(\"Match query with \\\"and\\\" operator does not support minimum should match.\");\n\t\t}\n\t\treturn this;\n\t}\n\n\t/**\n\t * Sets the maximal allowed fuzziness.\n\t * @param {number} fuzziness - the fuzziness\n\t * @return {MatchQuery} object itself for cascading\n\t */\n\tfuzziness(fuzziness) {\n\t\tif (!Utils.isNumber(fuzziness) || fuzziness < 0) {\n\t\t\tthrow TypeError(\"Fuzziness must be a positive number.\");\n\t\t}\n\t\tthis._data.fuzziness = fuzziness;\n\t\treturn this;\n\t}\n\n\t/**\n\t * Sets the starting word length which should not be considered for fuzziness.\n\t * @param {number} prefixLength - the positive prefix length\n\t * @return {MatchQuery} - object itself for cascading\n\t */\n\tprefixLength(prefixLength) {\n\t\tif (!Utils.isNumber(prefixLength) || prefixLength < 0) {\n\t\t\tthrow TypeError(\"Prefix length must be a positive number.\");\n\t\t}\n\t\tthis._data.prefix_length = prefixLength;\n\t\treturn this;\n\t}\n}\n\n/**\n * A query that matches all documents and giving them a constant score equal to the query boost.\n *\n * Typically used inside a must clause of a {@link BoolQuery} to subsequently reject non matching documents with the not\n * clause.\n *\n * See also [Lucene#MatchAllDocsQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/MatchAllDocsQuery.html}\n * and [Elasticsearch#MatchAllQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-all-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .matchAll()\n *     .boost(2.5)\n * .build()\n * // The resulting documents:\n * // all documents and giving a score of 2.5\n *\n * @extends BaseQuery\n */\nexport class MatchAllQuery extends BaseQuery {\n\tconstructor(data = {}) {\n\t\tsuper(\"match_all\", data);\n\t}\n}\n\n/**\n * A query which holds all sub queries like an array.\n * @private\n */\nclass ArrayQuery extends BaseQuery {\n\tconstructor(callbackName, callback, data = {}) {\n\t\tsuper(\"array\", data);\n\t\tthis._data.values = [];\n\t\tthis._callbackName = callbackName;\n\t\tthis[callbackName] = callback;\n\n\t\tthis._prepare = (queryType, ...args) => {\n\t\t\tlet data = {};\n\t\t\tlet query = new queryType(...args, data);\n\t\t\tthis._data.values.push(data);\n\t\t\tquery.bool = this.bool;\n\t\t\tquery.constantScore = this.constantScore;\n\t\t\tquery.term = this.term;\n\t\t\tquery.terms = this.terms;\n\t\t\tquery.wildcard = this.wildcard;\n\t\t\tquery.fuzzy = this.fuzzy;\n\t\t\tquery.match = this.match;\n\t\t\tquery.matchAll = this.matchAll;\n\t\t\tquery.prefix = this.prefix;\n\t\t\tquery.exists = this.exists;\n\t\t\tquery._prepare = this._prepare;\n\t\t\tquery[this._callbackName] = this[this._callbackName];\n\t\t\treturn query;\n\t\t};\n\t}\n\n\tbool() {\n\t\treturn this._prepare(BoolQuery);\n\t}\n\n\tconstantScore() {\n\t\treturn this._prepare(ConstantScoreQuery);\n\t}\n\n\tterm(field, term) {\n\t\treturn this._prepare(TermQuery, field, term);\n\t}\n\n\tterms(field, terms) {\n\t\treturn this._prepare(TermsQuery, field, terms);\n\t}\n\n\twildcard(field, wildcard) {\n\t\treturn this._prepare(WildcardQuery, field, wildcard);\n\t}\n\n\tfuzzy(field, fuzzy) {\n\t\treturn this._prepare(FuzzyQuery, field, fuzzy);\n\t}\n\n\tmatch(field, query) {\n\t\treturn this._prepare(MatchQuery, field, query);\n\t}\n\n\tmatchAll() {\n\t\treturn this._prepare(MatchAllQuery);\n\t}\n\n\tprefix(field, prefix) {\n\t\treturn this._prepare(PrefixQuery, field, prefix);\n\t}\n\n\texists(field) {\n\t\treturn this._prepare(ExistsQuery, field);\n\t}\n}\n\n/**\n * A query that wraps sub queries and returns a constant score equal to the query boost for every document in the filter.\n *\n * See also [Lucene#BooleanQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/ConstantScoreQuery.html}\n * and [Elasticsearch#ConstantScoreQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-constant-score-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .constantScore()\n *     .boost(1.5)\n *     .startFilter()\n *       .term(\"first_name\", \"albert\")\n *       .term(\"surname\", \"einstein\")\n *     .endFilter()\n * .build()\n * // The resulting documents:\n * // * contains albert as first name, einstein as surname and the document score is 42.\n *\n * @extends BaseQuery\n */\nexport class ConstantScoreQuery extends BaseQuery {\n\tconstructor(data = {}) {\n\t\tsuper(\"constant_score\", data);\n\t}\n\n\t/**\n\t * Starts an array of queries. Use endFilter() to finish the array.\n\t * @return {ArrayQuery} array query for holding sub queries\n\t */\n\tstartFilter() {\n\t\tthis._data.filter = {};\n\t\treturn new ArrayQuery(\"endFilter\", () => {\n\t\t\treturn this;\n\t\t}, this._data.filter);\n\t}\n}\n\n/**\n * A query that matches documents matching boolean combinations of sub queries.\n *\n * This query consists of one or more boolean clauses with different behavior but interrelated to each other.\n *\n * Occur         | Description\n * ------------- | -------------\n * must  | Finds documents which matches all sub queries.\n * filter  | Finds documents which matches all sub queries but these documents do not contribute to the score.\n * should  | Finds documents which matches some sub queries. The minimum amount of matches can be controlled with [minimumShouldMatch]{@link BoolQuery#minimumShouldMatch} (default is 1).\n * not  | Documents which match any sub query will be ignored.\n *\n * A sub query can be any other query type and also the bool query itself.\n *\n * See also [Lucene#BooleanQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/BooleanQuery.html}\n * and [Elasticsearch#BoolQuery]{@link https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html}.\n *\n * @example\n * new QueryBuilder()\n *   .bool()\n *     .startMust().boost(2)\n *       .term(\"first_name\", \"albert\")\n *     .endMust()\n *     .startFilter()\n *       .term(\"birthplace\", \"ulm\")\n *     .endFilter()\n *     .startShould().minimumShouldMatch(2)\n *       .fuzzy(\"surname\", \"einstin\")\n *       .wildcard(\"name\", \"geni?s\")\n *       .term(\"quotes\", \"infinity\")\n *     .endShould()\n *     .startNot()\n *       .terms(\"research_field\", [\"biology\", \"geography\"])\n *     .endNot()\n * .build();\n * // The resulting documents:\n * // contains the name albert (must: contribute to the score with a boost of 2)\n * // contains the birthplace ulm (filter: not contribute to the score)\n * // contains a minimum of two matches from the fuzzy, wildcard and/or term query (should: contribute to the score)\n * // do not contains biology or geography as research field (not: not contribute to the score)\n *\n * @extends BaseQuery\n */\nexport class BoolQuery extends BaseQuery {\n\tconstructor(data = {}) {\n\t\tsuper(\"bool\", data);\n\t}\n\n\t/**\n\t * Starts an array of queries for must clause. Use endMust() to finish the array.\n\t * @return {ArrayQuery} array query for holding sub queries\n\t */\n\tstartMust() {\n\t\tthis._data.must = {};\n\t\treturn new ArrayQuery(\"endMust\", () => {\n\t\t\treturn this;\n\t\t}, this._data.must);\n\t}\n\n\t/**\n\t * Starts an array of queries for filter clause. Use endFilter() to finish the array.\n\t * @return {ArrayQuery} array query for holding sub queries\n\t */\n\tstartFilter() {\n\t\tthis._data.filter = {};\n\t\treturn new ArrayQuery(\"endFilter\", () => {\n\t\t\treturn this;\n\t\t}, this._data.filter);\n\t}\n\n\t/**\n\t * Starts an array of queries for should clause. Use endShould() to finish the array.\n\t * @return {ArrayQuery} array query for holding sub queries\n\t */\n\tstartShould() {\n\t\tthis._data.should = {};\n\t\treturn new ArrayQuery(\"endShould\", () => {\n\t\t\treturn this;\n\t\t}, this._data.should);\n\t}\n\n\t/**\n\t * Starts an array of queries for not clause. Use endNot() to finish the array.\n\t * @return {ArrayQuery} array query for holding sub queries\n\t */\n\tstartNot() {\n\t\tthis._data.not = {};\n\t\treturn new ArrayQuery(\"endNot\", () => {\n\t\t\treturn this;\n\t\t}, this._data.not);\n\t}\n\n\t/**\n\t * Controls the amount of minimum matching sub queries before a document will be considered.\n\t * @param {number} minShouldMatch - number of minimum matching sub queries\n\t * @return {BoolQuery} object itself for cascading\n\t */\n\tminimumShouldMatch(minShouldMatch) {\n\t\tif (typeof(minShouldMatch) !== \"number\" || minShouldMatch < 0) {\n\t\t\tthrow TypeError(\"Minimum should match must be a number greater than zero.\");\n\t\t}\n\t\tthis._data.minimum_should_match = minShouldMatch;\n\t\treturn this;\n\t}\n}\n\n/**\n * This query builder is the root of each query search.\n * The query contains a sub query and parameters for setup scoring and search options.\n *\n * Possible sub query types are:\n * {@link TermQuery}, {@link TermsQuery}, {@link FuzzyQuery}, {@link WildcardQuery},\n * {@link MatchQuery}, {@link MatchAllQuery}, {@link PrefixQuery},  {@link BoolQuery},\n * {@link ConstantScoreQuery}, {@link ExistsQuery}\n *\n * @example\n * new QueryBuilder()\n *   .finalScoring(true)\n *   .useBM25(1.5, 0.5)\n *   .term(\"first_name\", \"albert\")\n * .build();\n * // The resulting documents:\n * // contains the first name albert\n * // are scored and ranked using BM25 with k1=1.5 and b=0.5\n */\nexport class QueryBuilder {\n\tconstructor() {\n\t\tthis._data = {query: {}};\n\t\tthis.useBM25();\n\t}\n\n\t/**\n\t * The query performs a final scoring over all scored sub queries and rank documents by there relevant.\n\t * @param {boolean} enabled - flag to enable or disable final scoring\n\t * @return {QueryBuilder}\n\t */\n\tenableFinalScoring(enabled) {\n\t\tthis._data.final_scoring = Utils.asBoolean(enabled);\n\t\treturn this;\n\t}\n\n\t/**\n\t * Use [Okapi BM25]{@link https://en.wikipedia.org/wiki/Okapi_BM25} as scoring model (default).\n\t *\n\t * See also [Lucene#MatchAllDocsQuery]{@link https://lucene.apache.org/core/6_4_0/core/org/apache/lucene/search/similarities/BM25Similarity.html}\n\t * and [Elasticsearch#BM25]{@link https://www.elastic.co/guide/en/elasticsearch/guide/current/pluggable-similarites.html#bm25}.\n\t *\n\t * @param {number} [k1=1.2] - controls how quickly an increase in term frequency results in term-frequency saturation.\n\t * \t\t\t\t\t\t\t\t\t\t\t\t\t\tLower values result in quicker saturation, and higher values in slower saturation.\n\t * @param {number} [b=0.75] - controls how much effect field-length normalization should have.\n\t * \t\t\t\t\t\t\t\t\t\t\t\t\t\tA value of 0.0 disables normalization completely, and a value of 1.0 normalizes fully.\n\t * @return {QueryBuilder}\n\t */\n\tuseBM25(k1 = 1.2, b = 0.75) {\n\t\tif (!Utils.isNumber(k1) || k1 < 0) {\n\t\t\tthrow TypeError(\"BM25s k1 must be a positive number.\");\n\t\t}\n\t\tif (!Utils.isNumber(b) || b < 0 || b > 1) {\n\t\t\tthrow TypeError(\"BM25s b must be a number between 0 and 1 inclusive.\");\n\t\t}\n\n\t\tthis._data.scoring = {\n\t\t\ttype: \"BM25\",\n\t\t\tk1: k1,\n\t\t\tb: b\n\t\t};\n\t\treturn this;\n\t}\n\n\tbool() {\n\t\treturn this._prepare(BoolQuery);\n\t}\n\n\tconstantScore() {\n\t\treturn this._prepare(ConstantScoreQuery);\n\t}\n\n\tterm(field, term) {\n\t\treturn this._prepare(TermQuery, field, term);\n\t}\n\n\tterms(field, terms) {\n\t\treturn this._prepare(TermsQuery, field, terms);\n\t}\n\n\twildcard(field, wildcard) {\n\t\treturn this._prepare(WildcardQuery, field, wildcard);\n\t}\n\n\tfuzzy(field, fuzzy) {\n\t\treturn this._prepare(FuzzyQuery, field, fuzzy);\n\t}\n\n\tmatch(field, query) {\n\t\treturn this._prepare(MatchQuery, field, query);\n\t}\n\n\tmatchAll() {\n\t\treturn this._prepare(MatchAllQuery);\n\t}\n\n\tprefix(field, prefix) {\n\t\treturn this._prepare(PrefixQuery, field, prefix);\n\t}\n\n\texists(field) {\n\t\treturn this._prepare(ExistsQuery, field);\n\t}\n\n\t_prepare(queryType, ...args) {\n\t\tthis._child = new queryType(...args, this._data.query);\n\t\tthis._child.build = () => {\n\t\t\treturn this._data;\n\t\t};\n\t\treturn this._child;\n\t}\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/inverted_index/queries.js\n// module id = 2\n// module chunks = 0","import {Tokenizer} from './tokenizer';\n\n/**\n * Inverted index class handles featured text search for specific document fields.\n * @constructor InvertedIndex\n * @param {boolean} [options.store=true] - inverted index will be stored at serialization rather than rebuilt on load.\n */\nexport class InvertedIndex {\n\t/**\n\t * @param {boolean} store\n\t * @param {Tokenizer} tokenizer\n\t */\n\tconstructor(store = true, tokenizer = new Tokenizer) {\n\t\tthis._store = store;\n\t\tthis._tokenizer = tokenizer;\n\t\tthis._docCount = 0;\n\t\tthis._docStore = {};\n\t\tthis._totalFieldLength = 0;\n\t\tthis._root = {};\n\t}\n\n\tget store() {\n\t\treturn this._store;\n\t}\n\n\tget tokenizer() {\n\t\treturn this._tokenizer;\n\t}\n\n\tget documentCount() {\n\t\treturn this._docCount;\n\t}\n\n\tget documentStore() {\n\t\treturn this._docStore;\n\t}\n\n\tget totalFieldLength() {\n\t\treturn this._totalFieldLength;\n\t}\n\n\tget root() {\n\t\treturn this._root;\n\t}\n\n\t/**\n\t * Adds defined fields of a document to the inverted index.\n\t * @param {object} field - the field to add\n\t * @param {number} docId - the doc id of the field\n\t * @param {number} [boost=1] - object with field (key) specific boost (value)\n\t */\n\tinsert(field, docId, boost = 1) {\n\t\tif (this._docStore.hasOwnProperty(docId)) {\n\t\t\tthrow Error('Field already added.');\n\t\t}\n\n\t\tthis._docCount += 1;\n\t\tthis._docStore[docId] = {};\n\n\t\t// Tokenize document field.\n\t\tlet fieldTokens = this._tokenizer.tokenize(field);\n\t\tthis._totalFieldLength += fieldTokens.length;\n\n\t\tlet termRefs = [];\n\t\tthis._docStore[docId] = {fieldLength: fieldTokens.length, boost: boost};\n\t\tObject.defineProperties(this._docStore[docId], {\n\t\t\ttermRefs: {enumerable: false, configurable: true, writable: true, value: termRefs}\n\t\t});\n\n\t\t// Iterate over all unique field terms.\n\t\tfor (let term of new Set(fieldTokens)) {\n\t\t\tif (term === '') {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t// Calculate term frequency.\n\t\t\tlet tf = 0;\n\t\t\tfor (let j = 0; j < fieldTokens.length; j++) {\n\t\t\t\tif (fieldTokens[j] === term) {\n\t\t\t\t\ttf++;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Add term to index tree.\n\t\t\tlet branch = this._root;\n\t\t\tfor (let i = 0; i < term.length; i++) {\n\t\t\t\tlet c = term[i];\n\t\t\t\tif (!branch.hasOwnProperty(c)) {\n\t\t\t\t\tlet child = {};\n\t\t\t\t\tObject.defineProperties(child, {\n\t\t\t\t\t\tparent: {enumerable: false, configurable: true, writable: true, value: branch}\n\t\t\t\t\t});\n\t\t\t\t\tbranch[c] = child;\n\t\t\t\t}\n\t\t\t\tbranch = branch[c];\n\t\t\t}\n\t\t\t// Add term info to index leaf.\n\t\t\tif (!branch.hasOwnProperty('docs')) {\n\t\t\t\tbranch.docs = {};\n\t\t\t\tbranch.df = 0;\n\t\t\t}\n\t\t\tbranch.docs[docId] = tf;\n\t\t\tbranch.df += 1;\n\n\t\t\t// Store index leaf for deletion.\n\t\t\ttermRefs.push(branch);\n\t\t}\n\t}\n\n\t/**\n\t * Removes all relevant terms of a document from the inverted index.\n\t * @param {number} docId - the document.\n\t */\n\tremove(docId) {\n\t\tif (!this._docStore.hasOwnProperty(String(docId))) {\n\t\t\treturn;\n\t\t}\n\t\tlet docStore = this._docStore[docId];\n\t\t// Remove document.\n\t\tdelete this._docStore[docId];\n\t\tthis._docCount -= 1;\n\n\t\t// Reduce total field length.\n\t\tthis._totalFieldLength -= docStore.fieldLength;\n\n\t\t// Iterate over all term references.\n\t\t// Remove docId from docs and decrement document frequency.\n\t\tlet termRefs = docStore.termRefs;\n\t\tfor (let j = 0; j < termRefs.length; j++) {\n\t\t\tlet index = termRefs[j];\n\t\t\tindex.df -= 1;\n\t\t\tdelete index.docs[docId];\n\n\t\t\t// Delete term branch if not used anymore.\n\t\t\tif (index.df === 0) {\n\t\t\t\tlet keys = [];\n\t\t\t\tdo {\n\t\t\t\t\t// Go tree upwards.\n\t\t\t\t\tlet parent = index.parent;\n\t\t\t\t\t// Delete parent reference for preventing memory leak (cycle reference)\n\t\t\t\t\tdelete index.parent;\n\n\t\t\t\t\t// Iterate over all children.\n\t\t\t\t\tkeys = Object.keys(parent);\n\t\t\t\t\tfor (let k = 0; k < keys.length; k++) {\n\t\t\t\t\t\tlet key = keys[k];\n\t\t\t\t\t\tif (key === 'df' || key === 'docs') {\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Remove previous child form parent.\n\t\t\t\t\t\tif (parent[key] === index) {\n\t\t\t\t\t\t\tdelete parent[key];\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tindex = parent;\n\t\t\t\t} while (index.hasOwnProperty('parent') && keys.length === 1);\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Gets the term index of a term.\n\t * @param {string} term - the term.\n\t * @param {object} root - the term index to start from\n\t * @param {number} start - the position of the term string to start from\n\t * @return {object} - The term index or null if the term is not in the term tree.\n\t */\n\tstatic getTermIndex(term, root, start = 0) {\n\t\tif (start >= term.length) {\n\t\t\treturn null;\n\t\t}\n\t\tfor (let i = start; i < term.length; i++) {\n\t\t\tif (!root.hasOwnProperty(term[i])) {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t\troot = root[term[i]];\n\t\t}\n\t\treturn root;\n\t}\n\n\t/**\n\t * Extends a term index for the one branch.\n\t * @param {object} root - the term index to start from\n\t * @return {Array} - array with term indices and extension\n\t */\n\tstatic getNextTermIndex(root) {\n\t\tlet termIndices = [];\n\t\tlet keys = Object.keys(root);\n\t\tfor (let i = 0; i < keys.length; i++) {\n\t\t\tif (keys[i] !== 'docs' && keys[i] !== 'df') {\n\t\t\t\ttermIndices.push({index: root[keys[i]], term: keys[i]});\n\t\t\t}\n\t\t}\n\t\treturn termIndices;\n\t}\n\n\t/**\n\t * Extends a term index to all available term leafs.\n\t * @param {object} root - the term index to start from\n\t * @returns {Array} - Array with term indices and extension\n\t */\n\tstatic extendTermIndex(root) {\n\t\tlet termIndices = [];\n\t\tlet stack = [root];\n\t\tlet treeStack = [''];\n\t\tdo {\n\t\t\tlet root = stack.pop();\n\t\t\tlet treeTermn = treeStack.pop();\n\n\t\t\tif (root.hasOwnProperty('df')) {\n\t\t\t\ttermIndices.push({index: root, term: treeTermn});\n\t\t\t}\n\n\t\t\tlet keys = Object.keys(root);\n\t\t\tfor (let i = 0; i < keys.length; i++) {\n\t\t\t\tif (keys[i] !== 'docs' && keys[i] !== 'df') {\n\t\t\t\t\tstack.push(root[keys[i]]);\n\t\t\t\t\ttreeStack.push(treeTermn + keys[i]);\n\t\t\t\t}\n\t\t\t}\n\t\t} while (stack.length !== 0);\n\n\t\treturn termIndices;\n\t}\n\n\t/**\n\t * Serialize the inverted index.\n\t * @returns {{docStore: *, _fields: *, index: *}}\n\t */\n\ttoJSON() {\n\t\tif (this._store) {\n\t\t\treturn this;\n\t\t} else {\n\t\t\treturn {\n\t\t\t\t_tokenizer: this._tokenizer,\n\t\t\t};\n\t\t}\n\t}\n\n\t/**\n\t * Deserialize the inverted index.\n\t * @param {{docStore: *, _fields: *, index: *}} serialized - The serialized inverted index.\n\t * @param {Object.<string, function>|Tokenizer} funcTok[undefined] - the depending functions with labels\n\t * \tor an equivalent tokenizer\n\t */\n\tloadJSON(serialized, funcTok = undefined) {\n\t\tlet dbObject = serialized;\n\n\t\tthis._tokenizer = Tokenizer.fromJSON(dbObject._tokenizer, funcTok);\n\t\tthis._docCount = dbObject._docCount;\n\t\tthis._docStore = dbObject._docStore;\n\t\tthis._totalFieldLength = dbObject._totalFieldLength;\n\t\tthis._root = dbObject._root;\n\n\t\tlet self = this;\n\n\t\tfunction regenerate(index, parent) {\n\t\t\t// Set parent.\n\t\t\tif (parent !== null) {\n\t\t\t\tObject.defineProperties(index, {\n\t\t\t\t\tparent: {enumerable: false, configurable: true, writable: false, value: parent}\n\t\t\t\t});\n\t\t\t}\n\n\t\t\t// Iterate over all keys.\n\t\t\tlet keys = Object.keys(index);\n\t\t\tfor (let i = 0; i < keys.length; i++) {\n\t\t\t\t// Found term, save in document store.\n\t\t\t\tif (keys[i] === 'docs') {\n\t\t\t\t\t// Get documents of term.\n\t\t\t\t\tlet docIds = Object.keys(index.docs);\n\t\t\t\t\tfor (let j = 0; j < docIds.length; j++) {\n\t\t\t\t\t\t// Get document store at specific document/field.\n\t\t\t\t\t\tlet ref = self._docStore[docIds[j]];\n\t\t\t\t\t\tif (!ref.hasOwnProperty('termRefs')) {\n\t\t\t\t\t\t\tObject.defineProperties(ref, {\n\t\t\t\t\t\t\t\ttermRefs: {enumerable: false, configurable: true, writable: true, value: []}\n\t\t\t\t\t\t\t});\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Set reference to term index.\n\t\t\t\t\t\tref.termRefs.push(index);\n\t\t\t\t\t}\n\t\t\t\t} else if (keys[i] !== 'df') {\n\t\t\t\t\t// Iterate over subtree.\n\t\t\t\t\tregenerate(index[keys[i]], index);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tregenerate(this._root, null);\n\t}\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/inverted_index/inverted_index.js\n// module id = 3\n// module chunks = 0","import {InvertedIndex} from './inverted_index';\nimport {IndexSearcher} from './index_searcher';\nimport {Tokenizer} from './tokenizer';\nimport * as Utils from './utils.js';\n\nexport class FullTextSearch {\n\t/**\n\t *\n\t * @param options\n\t */\n\tconstructor(fields) {\n\t\tif (fields === undefined) {\n\t\t\tthrow new SyntaxError('Fields needs to be defined!');\n\t\t}\n\n\t\tthis._invIdxs = {};\n\t\t// Get field names and tokenizers.\n\t\tif (Array.isArray(fields)) {\n\t\t\tfor (let i = 0; i < fields.length; i++) {\n\t\t\t\tlet field = fields[i];\n\t\t\t\tlet name = Utils.asString(field.name, TypeError('Field name needs to be a string.'));\n\n\t\t\t\tlet store = field.hasOwnProperty(\"store\") ?\n\t\t\t\t\tUtils.asBoolean(field.store, TypeError(\"Field store flag needs to be a boolean\")) : true;\n\n\t\t\t\tlet tokenizer = null;\n\t\t\t\tif (field.hasOwnProperty(\"tokenizer\")) {\n\t\t\t\t\tif (!(field.tokenizer instanceof Tokenizer)) {\n\t\t\t\t\t\tthrow new TypeError(\"Field tokenizer needs to be a instance of tokenizer.\");\n\t\t\t\t\t}\n\t\t\t\t\ttokenizer = field.tokenizer;\n\t\t\t\t} else {\n\t\t\t\t\ttokenizer = new Tokenizer();\n\t\t\t\t}\n\t\t\t\tthis._invIdxs[name] = new InvertedIndex(store, tokenizer);\n\t\t\t}\n\t\t} else {\n\t\t\tthrow new TypeError('fields needs to be an array with field name and a tokenizer (optional).');\n\t\t}\n\n\t\tthis._docs = new Set();\n\t\tthis._idxSearcher = new IndexSearcher(this._invIdxs, this._docs);\n\t}\n\n\taddDocument(doc, boosts = {}) {\n\t\tif (!doc.hasOwnProperty('$loki')) {\n\t\t\tthrow new Error('Document is not stored in the collection.');\n\t\t}\n\n\t\tlet fieldNames = Object.keys(doc);\n\t\tfor (let i = 0, fieldName; i < fieldNames.length, fieldName = fieldNames[i]; i++) {\n\t\t\tif (this._invIdxs.hasOwnProperty(fieldName)) {\n\t\t\t\tlet boost = boosts.hasOwnProperty(fieldName) ? boosts[fieldName] : 1;\n\t\t\t\tthis._invIdxs[fieldName].insert(doc[fieldName], doc.$loki, boost);\n\t\t\t}\n\t\t}\n\n\t\tthis._docs.add(doc.$loki);\n\t\tthis.setDirty();\n\t}\n\n\tremoveDocument(doc) {\n\t\tif (!doc.hasOwnProperty('$loki')) {\n\t\t\tthrow new Error('Document is not stored in the collection.');\n\t\t}\n\n\t\tlet fieldNames = Object.keys(this._invIdxs);\n\t\tfor (let i = 0; i < fieldNames.length; i++) {\n\t\t\tthis._invIdxs[fieldNames[i]].remove(doc.$loki);\n\t\t}\n\n\t\tthis._docs.delete(doc.$loki);\n\t\tthis.setDirty();\n\t}\n\n\tupdateDocument(doc, boosts = {}) {\n\t\tthis.removeDocument(doc);\n\t\tthis.addDocument(doc, boosts);\n\t}\n\n\tsearch(query) {\n\t\treturn this._idxSearcher.search(query);\n\t}\n\n\ttoJSON() {\n\t\tlet serialized = {};\n\t\tlet fieldNames = Object.keys(this._invIdxs);\n\t\tfor (let i = 0, fieldName; i < fieldNames.length, fieldName = fieldNames[i]; i++) {\n\t\t\tserialized[fieldName] = this._invIdxs[fieldName].toJSON();\n\t\t}\n\t\treturn serialized;\n\t}\n\n\tloadJSON(serialized, tokenizers) {\n\t\tlet db = JSON.parse(serialized);\n\t\tlet fieldNames = Object.keys(db);\n\t\tfor (let i = 0, fieldName; i < fieldNames.length, fieldName = fieldNames[i]; i++) {\n\t\t\tthis._invIdxs[fieldName] = new InvertedIndex();\n\t\t\tthis._invIdxs[fieldName].loadJSON(db[fieldName], tokenizers[fieldName]);\n\t\t}\n\t}\n\n\tsetDirty() {\n\t\tthis._idxSearcher.setDirty();\n\t}\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/inverted_index/full_text_search.js\n// module id = 4\n// module chunks = 0","import {Scorer} from './scorer';\nimport {InvertedIndex} from './inverted_index';\nimport {QueryBuilder} from './queries';\n\nexport class IndexSearcher {\n\t/**\n\t *\n\t * @param {object} invIdxs\n\t */\n\tconstructor(invIdxs, docs) {\n\t\tthis._invIdxs = invIdxs;\n\t\tthis._docs = docs;\n\t\tthis._scorer = new Scorer(this._invIdxs);\n\t}\n\n\tsearch(query) {\n\t\tlet docResults = this._recursive(query.query, true);\n\n\t\t// Final scoring.\n\t\tlet finalScoring = query.hasOwnProperty(\"final_scoring\") ? query.final_scoring : true;\n\t\tif (finalScoring) {\n\t\t\treturn this._scorer.finalScore(query, docResults);\n\t\t}\n\t\treturn docResults;\n\t}\n\n\tsetDirty() {\n\t\tthis._scorer.setDirty();\n\t}\n\n\t_recursive(query, doScoring) {\n\t\tlet docResults = {};\n\t\tlet boost = query.hasOwnProperty('boost') ? query.boost : 1;\n\t\tlet fieldName = query.hasOwnProperty(\"field\") ? query.field : null;\n\n\t\tlet root = null;\n\t\tlet tokenizer = null;\n\t\tif (this._invIdxs.hasOwnProperty(fieldName)) {\n\t\t\troot = this._invIdxs[fieldName].root;\n\t\t\ttokenizer = this._invIdxs[fieldName].tokenizer;\n\t\t}\n\n\t\tswitch (query.type) {\n\t\t\tcase \"bool\": {\n\t\t\t\tdocResults = null;\n\t\t\t\tif (query.hasOwnProperty(\"must\")) {\n\t\t\t\t\tdocResults = this._getUnique(query.must.values, doScoring, docResults);\n\t\t\t\t}\n\t\t\t\tif (query.hasOwnProperty(\"filter\")) {\n\t\t\t\t\tdocResults = this._getUnique(query.filter.values, false, docResults);\n\t\t\t\t}\n\n\t\t\t\tif (query.hasOwnProperty(\"should\")) {\n\t\t\t\t\tlet shouldDocs = this._getAll(query.should.values, doScoring);\n\n\t\t\t\t\tlet empty = false;\n\t\t\t\t\tif (docResults === null) {\n\t\t\t\t\t\tdocResults = {};\n\t\t\t\t\t\tempty = true;\n\t\t\t\t\t}\n\n\t\t\t\t\tlet msm = query.hasOwnProperty(\"minimum_should_match\") ? query.minimum_should_match : 1;\n\t\t\t\t\t// Remove all docs with fewer matches.\n\t\t\t\t\t// TODO: Enable percent, negative values and ranges.\n\t\t\t\t\tlet docs = Object.keys(shouldDocs);\n\t\t\t\t\tfor (let i = 0, docId; i < docs.length, docId = docs[i]; i++) {\n\t\t\t\t\t\tif (shouldDocs[docId].length >= msm) {\n\t\t\t\t\t\t\tif (docResults.hasOwnProperty(docId)) {\n\t\t\t\t\t\t\t\tdocResults[docId].push(...shouldDocs[docId]);\n\t\t\t\t\t\t\t} else if (empty) {\n\t\t\t\t\t\t\t\tdocResults[docId] = shouldDocs[docId];\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tdelete docResults[docId];\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (query.hasOwnProperty(\"not\")) {\n\t\t\t\t\tlet notDocs = this._getAll(query.not.values, false);\n\t\t\t\t\t// Remove all docs.\n\t\t\t\t\tlet docs = Object.keys(notDocs);\n\t\t\t\t\tfor (let i = 0, docId; i < docs.length, docId = docs[i]; i++) {\n\t\t\t\t\t\tif (docResults.hasOwnProperty(docId)) {\n\t\t\t\t\t\t\tdelete docResults[docId];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"term\": {\n\t\t\t\tlet termIdx = InvertedIndex.getTermIndex(query.value, root);\n\t\t\t\tthis._scorer.prepare(fieldName, boost, termIdx, doScoring, docResults, query.value);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"terms\": {\n\t\t\t\tfor (let i = 0; i < query.value.length; i++) {\n\t\t\t\t\tlet termIdx = InvertedIndex.getTermIndex(query.value[i], root);\n\t\t\t\t\tthis._scorer.prepare(fieldName, boost, termIdx, doScoring, docResults, query.value[i]);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"fuzzy\": {\n\t\t\t\tlet f = new FuzzySearch(query);\n\t\t\t\tlet b = f.search(root);\n\t\t\t\tfor (let i = 0; i < b.length; i++) {\n\t\t\t\t\tthis._scorer.prepare(fieldName, boost * b[i].boost, b[i].index, doScoring, docResults, b[i].term);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"wildcard\": {\n\t\t\t\tlet w = new WildcardSearch(query);\n\t\t\t\tlet a = w.search(root);\n\t\t\t\tfor (let i = 0; i < a.length; i++) {\n\t\t\t\t\tthis._scorer.prepare(fieldName, boost, a[i].index, doScoring, docResults, a[i].term);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"match_all\": {\n\t\t\t\tfor (let docId of this._docs) {\n\t\t\t\t\tthis._scorer.scoreConstant(boost, docId, docResults);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"constant_score\": {\n\t\t\t\tdocResults = this._getAll(query.filter.values, false);\n\t\t\t\tlet docs = Object.keys(docResults);\n\t\t\t\t// Add to each document a constant score.\n\t\t\t\tfor (let i = 0; i < docs.length; i++) {\n\t\t\t\t\tthis._scorer.scoreConstant(boost, docs[i], docResults);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"prefix\": {\n\t\t\t\tlet termIdx = InvertedIndex.getTermIndex(query.value, root);\n\t\t\t\tif (termIdx != null) {\n\t\t\t\t\ttermIdx = InvertedIndex.extendTermIndex(termIdx);\n\t\t\t\t}\n\t\t\t\tfor (let i = 0; i < termIdx.length; i++) {\n\t\t\t\t\tthis._scorer.prepare(fieldName, boost, termIdx[i].index, doScoring, docResults, query.value + termIdx[i].term);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"exists\": {\n\t\t\t\tif (root != null) {\n\t\t\t\t\tlet docs = Object.keys(this._invIdxs[fieldName].documentStore);\n\t\t\t\t\tfor (let i = 0; i < docs.length; i++) {\n\t\t\t\t\t\tthis._scorer.scoreConstant(boost, docs[i], docResults);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcase \"match\": {\n\t\t\t\tlet terms = tokenizer.tokenize(query.value);\n\t\t\t\tlet operator = query.hasOwnProperty(\"operator\") ? query.operator : \"or\";\n\n\t\t\t\tlet tmpQuery = new QueryBuilder().bool();\n\t\t\t\tif (operator === \"or\") {\n\t\t\t\t\tif (query.hasOwnProperty(\"minimum_should_match\")) {\n\t\t\t\t\t\ttmpQuery = tmpQuery.minimumShouldMatch(query.minimum_should_match);\n\t\t\t\t\t}\n\t\t\t\t\t// Build a should query.\n\t\t\t\t\ttmpQuery = tmpQuery.startShould();\n\t\t\t\t} else {\n\t\t\t\t\t// Build a must query.\n\t\t\t\t\ttmpQuery = tmpQuery.startMust();\n\t\t\t\t}\n\t\t\t\ttmpQuery = tmpQuery.boost(boost);\n\n\t\t\t\tif (query.hasOwnProperty(\"fuzziness\")) {\n\t\t\t\t\tlet prefixLength = query.hasOwnProperty(\"prefix_length\") ? query.prefix_length : 2;\n\t\t\t\t\t// Add each fuzzy.\n\t\t\t\t\tfor (let i = 0; i < terms.length; i++) {\n\t\t\t\t\t\ttmpQuery = tmpQuery.fuzzy(fieldName, terms[i]).fuzziness(query.fuzziness).prefixLength(prefixLength);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// Add each term.\n\t\t\t\t\tfor (let i = 0; i < terms.length; i++) {\n\t\t\t\t\t\ttmpQuery = tmpQuery.term(fieldName, terms[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (operator === \"or\") {\n\t\t\t\t\ttmpQuery = tmpQuery.endShould();\n\t\t\t\t} else {\n\t\t\t\t\ttmpQuery = tmpQuery.endMust();\n\t\t\t\t}\n\n\t\t\t\tdocResults = this._recursive(tmpQuery.build(), doScoring);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t}\n\t\treturn docResults;\n\t}\n\n\t_getUnique(values, doScoring, docResults) {\n\t\tif (values.length === 0) {\n\t\t\treturn docResults;\n\t\t}\n\n\t\tfor (let i = 0; i < values.length; i++) {\n\t\t\tlet currDocs = this._recursive(values[i], doScoring);\n\t\t\tif (docResults === null) {\n\t\t\t\tdocResults = this._recursive(values[0], doScoring);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tlet docs = Object.keys(docResults);\n\t\t\tfor (let j = 0, docId; j < docs.length, docId = docs[j]; j++) {\n\t\t\t\tif (!currDocs.hasOwnProperty(docId)) {\n\t\t\t\t\tdelete docResults[docId];\n\t\t\t\t} else {\n\t\t\t\t\tdocResults[docId].push(...currDocs[docId]);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn docResults;\n\t}\n\n\t_getAll(values, doScoring) {\n\t\tlet docResults = {};\n\t\tfor (let i = 0; i < values.length; i++) {\n\t\t\tlet currDocs = this._recursive(values[i], doScoring);\n\t\t\tlet docs = Object.keys(currDocs);\n\t\t\tfor (let j = 0, docId; j < docs.length, docId = docs[j]; j++) {\n\t\t\t\tif (!docResults.hasOwnProperty(docId)) {\n\t\t\t\t\tdocResults[docId] = currDocs[docId];\n\t\t\t\t} else {\n\t\t\t\t\tdocResults[docId].push(...currDocs[docId]);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn docResults;\n\t}\n}\n\n\nclass FuzzySearch {\n\tconstructor(query) {\n\t\tthis._fuzzy = query.value;\n\t\tthis._fuzziness = query.hasOwnProperty('fuzziness') ? query.fuzziness : 2;\n\t\tthis._prefixLength = query.hasOwnProperty('prefix_length') ? query.prefix_length : 2;\n\t}\n\n\t/**\n\t * Copyright Kigiri: https://github.com/kigiri\n\t *                     Milot Mirdita: https://github.com/milot-mirdita\n\t *                     Toni Neubert:  https://github.com/Viatorus/\n\t */\n\tlevenshtein_distance(a, b) {\n\t\tif (a.length === 0) return b.length;\n\t\tif (b.length === 0) return a.length;\n\t\tlet tmp, i, j, prev, val;\n\t\t// swap to save some memory O(min(a,b)) instead of O(a)\n\t\tif (a.length > b.length) {\n\t\t\ttmp = a;\n\t\t\ta = b;\n\t\t\tb = tmp;\n\t\t}\n\n\t\tvar row = Array(a.length + 1);\n\t\t// init the row\n\t\tfor (i = 0; i <= a.length; i++) {\n\t\t\trow[i] = i;\n\t\t}\n\n\t\t// fill in the rest\n\t\tfor (i = 1; i <= b.length; i++) {\n\t\t\tprev = i;\n\t\t\tfor (j = 1; j <= a.length; j++) {\n\t\t\t\tif (b[i - 1] === a[j - 1]) {\t// match\n\t\t\t\t\tval = row[j - 1];\n\t\t\t\t} else {\n\t\t\t\t\tval = Math.min(row[j - 1] + 1, // substitution\n\t\t\t\t\t\tMath.min(prev + 1,         // insertion\n\t\t\t\t\t\t\trow[j] + 1));          // deletion\n\n\t\t\t\t\t// transposition.\n\t\t\t\t\tif (i > 1 && j > 1 && b[i - 2] === a[j - 1] && a[j - 2] === b[i - 1]) {\n\t\t\t\t\t\tval = Math.min(val, row[j - 1] - (a[j - 1] === b[i - 1] ? 1 : 0));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trow[j - 1] = prev;\n\t\t\t\tprev = val;\n\t\t\t}\n\t\t\trow[a.length] = prev;\n\t\t}\n\t\treturn row[a.length];\n\t}\n\n\t/**\n\t * Performs a fuzzy search for a given term.\n\t * @param {string} query - a fuzzy term to match.\n\t * @param {number} [maxDistance=2] - maximal edit distance between terms\n\t * @returns {Array} - array with all matching term indices.\n\t */\n\tsearch(root) {\n\t\t// Todo: Include levenshtein to reduce similar iterations.\n\t\t// Tree tokens at same depth share same row until depth (should works if recursive).\n\t\t// Pregenerate tree token ?\n\t\t//var treeToken = Array(token.length + maxDistance);\n\n\t\tlet start = root;\n\t\tlet pre = this._fuzzy.slice(0, this._prefixLength);\n\t\tlet fuzzy = this._fuzzy;\n\t\tif (this._prefixLength != 0) {\n\t\t\tstart = InvertedIndex.getTermIndex(pre, start);\n\t\t\tfuzzy = fuzzy.slice(this._prefixLength);\n\t\t}\n\t\tif (start === null) {\n\t\t\treturn [];\n\t\t}\n\n\t\tlet similarTokens = [];\n\n\t\tlet stack = [start];\n\t\tlet treeStack = [''];\n\t\tdo {\n\t\t\tlet root = stack.pop();\n\t\t\tlet treeTerms = treeStack.pop();\n\n\t\t\t// Compare tokens if they are in near distance.\n\t\t\tif (root.hasOwnProperty('df') && Math.abs(fuzzy.length - treeTerms.length) <= this._fuzziness) {\n\t\t\t\tconst distance = this.levenshtein_distance(fuzzy, treeTerms);\n\t\t\t\tif (distance <= this._fuzziness) {\n\t\t\t\t\t// Calculate boost.\n\t\t\t\t\tlet boost = 1 - distance / (pre.length + treeTerms.length);\n\t\t\t\t\tsimilarTokens.push({term: pre + treeTerms, index: root, boost: boost});\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Iterate over all subtrees.\n\t\t\t// If token from tree is not longer than maximal distance.\n\t\t\tif (treeTerms.length - fuzzy.length <= this._fuzziness) {\n\t\t\t\t// Iterate over all subtrees.\n\t\t\t\tlet keys = Object.keys(root);\n\t\t\t\tfor (let i = 0; i < keys.length; i++) {\n\t\t\t\t\tif (keys[i] !== 'docs' && keys[i] !== 'df') {\n\t\t\t\t\t\tstack.push(root[keys[i]]);\n\t\t\t\t\t\ttreeStack.push(treeTerms + keys[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t} while (stack.length !== 0);\n\n\t\treturn similarTokens;\n\t}\n}\n\nclass WildcardSearch {\n\n\tconstructor(query) {\n\t\tthis._wildcard = query.value;\n\t\tthis._result = [];\n\t}\n\n\t/**\n\t * Performs a wild card search for a given query term.\n\t * @param {string} query - a wild card query to match.\n\t * @returns {Array} - array with all matching term indices.\n\t */\n\tsearch(root) {\n\t\t// Todo: Need an implementation for star operator in the middle.\n\t\tthis._result = [];\n\t\tthis._recursive(root);\n\t\treturn this._result;\n\t}\n\n\t/**\n\t *\n\t * @param root\n\t * @param idx\n\t * @param term\n\t * @param escaped\n\t * @private\n\t */\n\t_recursive(root, idx = 0, term = '', escaped = false) {\n\t\tif (root === null) {\n\t\t\treturn;\n\t\t}\n\n\t\tif (idx === this._wildcard.length) {\n\t\t\tif (root.hasOwnProperty('df')) {\n\t\t\t\tthis._result.push({index: root, term: term});\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\n\t\tif (!escaped && this._wildcard[idx] === '\\\\') {\n\t\t\tthis._recursive(root, idx + 1, term, true);\n\t\t} else if (!escaped && this._wildcard[idx] === '?') {\n\t\t\tlet others = InvertedIndex.getNextTermIndex(root);\n\t\t\tfor (let i = 0; i < others.length; i++) {\n\t\t\t\tthis._recursive(others[i].index, idx + 1, term + others[i].term);\n\t\t\t}\n\t\t} else if (!escaped && this._wildcard[idx] === '*') {\n\t\t\tlet all = InvertedIndex.extendTermIndex(root);\n\t\t\tfor (let i = 0; i < all.length; i++) {\n\t\t\t\tthis._recursive(all[i].index, idx + 1, term + all[i].term);\n\t\t\t}\n\t\t} else {\n\t\t\tthis._recursive(InvertedIndex.getTermIndex(this._wildcard[idx], root), idx + 1, term + this._wildcard[idx]);\n\t\t}\n\t}\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/inverted_index/index_searcher.js\n// module id = 6\n// module chunks = 0","export class Scorer {\n\tconstructor(invIdxs) {\n\t\tthis._invIdxs = invIdxs;\n\t\tthis._cache = {};\n\t}\n\n\tsetDirty() {\n\t\tthis._cache = {};\n\t}\n\n\tprepare(fieldName, boost, termIdx, doScoring, docResults = {}, term = null) {\n\t\tif (termIdx == null || !termIdx.hasOwnProperty('docs')) {\n\t\t\treturn null;\n\t\t}\n\n\t\tlet idf = this._idf(fieldName, termIdx.df);\n\t\tlet docIds = Object.keys(termIdx.docs);\n\t\tfor (let j = 0; j < docIds.length; j++) {\n\t\t\tlet docId = docIds[j];\n\t\t\tif (!docResults.hasOwnProperty(docId)) {\n\t\t\t\tdocResults[docId] = [];\n\t\t\t}\n\n\t\t\tif (doScoring) {\n\t\t\t\tlet tf = termIdx.docs[docId];\n\t\t\t\tdocResults[docId].push({\n\t\t\t\t\ttype: 'BM25',\n\t\t\t\t\ttf: tf,\n\t\t\t\t\tidf: idf,\n\t\t\t\t\tboost: boost,\n\t\t\t\t\tfieldName: fieldName,\n\t\t\t\t\tterm: term\n\t\t\t\t});\n\t\t\t} else {\n\t\t\t\t// TODO: Maybe only 1 constant store per document\n\t\t\t\tdocResults[docId].push({\n\t\t\t\t\ttype: \"constant\", value: 1, boost: boost, fieldName: fieldName,\n\t\t\t\t\tterm: term\n\t\t\t\t});\n\t\t\t}\n\t\t}\n\n\t\treturn docResults;\n\t}\n\n\tscoreConstant(boost, docId, docResults = {}) {\n\t\tif (!docResults.hasOwnProperty(docId)) {\n\t\t\tdocResults[docId] = [];\n\t\t}\n\t\tdocResults[docId].push({type: \"constant\", value: 1, boost: boost});\n\t\treturn docResults;\n\t}\n\n\tfinalScore(query, docResults = {}) {\n\n\t\tlet result = {};\n\t\tlet k1 = query.scoring.k1;\n\t\tlet b = query.scoring.b;\n\n\t\tlet docs = Object.keys(docResults);\n\t\tfor (let i = 0, docId; i < docs.length, docId = docs[i]; i++) {\n\t\t\tlet docScore = 0;\n\t\t\tfor (let j = 0; j < docResults[docId].length; j++) {\n\t\t\t\tlet docResult = docResults[docId][j];\n\n\t\t\t\tlet res = 0;\n\t\t\t\tswitch (docResult.type) {\n\t\t\t\t\tcase 'BM25': {\n\t\t\t\t\t\tlet fieldLength = this._invIdxs[docResult.fieldName].documentStore[docId].fieldLength /\n\t\t\t\t\t\t\tMath.pow(this._invIdxs[docResult.fieldName].documentStore[docId].boost, 2);\n\t\t\t\t\t\tlet avgFieldLength = this._avgFieldLength(docResult.fieldName);\n\t\t\t\t\t\tlet tfNorm = ((k1 + 1) * docResult.tf) / (k1 * ((1 - b)\n\t\t\t\t\t\t\t+ b * (fieldLength / avgFieldLength)) + docResult.tf);\n\t\t\t\t\t\tres = docResult.idf * tfNorm * docResult.boost;\n\t\t\t\t\t\t/*console.log(\n\t\t\t\t\t\t\tdocId + \":\" + docResult.fieldName + \":\" + docResult.term + \" = \" + res,\n\t\t\t\t\t\t\t\"\\n\\ttype: BM25\",\n\t\t\t\t\t\t\t\"\\n\\tboost: \" + docResult.boost,\n\t\t\t\t\t\t\t\"\\n\\tidf : \" + docResult.idf,\n\t\t\t\t\t\t\t\"\\n\\ttfNorm : \" + tfNorm,\n\t\t\t\t\t\t\t\"\\n\\ttf : \" + docResult.tf,\n\t\t\t\t\t\t\t\"\\n\\tavg : \" + avgFieldLength,\n\t\t\t\t\t\t\t\"\\n\\tfl : \" + fieldLength);*/\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\tcase 'constant':\n\t\t\t\t\t\tres = docResult.value * docResult.boost;\n\t\t\t\t\t\t/*console.log(\n\t\t\t\t\t\t\t\"Constant: \" + res,\n\t\t\t\t\t\t\t\"\\n\\tboost: \" + docResult.boost,\n\t\t\t\t\t\t\t\"\\n\\tvalue : \" + docResult.value);*/\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdocScore += res;\n\t\t\t}\n\t\t\t//console.log(docId, \" === \", docScore);\n\t\t\tresult[docId] = docScore;\n\t\t}\n\t\treturn result;\n\t}\n\n\t_getCache(fieldName) {\n\t\tif (!this._cache.hasOwnProperty(fieldName)) {\n\t\t\tlet avgFieldLength = this._invIdxs[fieldName].totalFieldLength / this._invIdxs[fieldName].documentCount;\n\t\t\tthis._cache[fieldName] = {idfs: {}, avgFieldLength: avgFieldLength};\n\t\t}\n\t\treturn this._cache[fieldName];\n\t}\n\n\t/**\n\t * Returns the idf by either calculate it or use a cached one.\n\t * @param {number} docFreq - the doc frequency of the term\n\t * @returns {number} the idf\n\t * @private\n\t */\n\t_idf(fieldName, docFreq) {\n\t\tlet cache = this._getCache(fieldName);\n\t\tif (cache.idfs.hasOwnProperty(String(docFreq))) {\n\t\t\treturn cache.idfs[docFreq];\n\t\t}\n\t\treturn cache.idfs[docFreq] = Math.log(1 + (this._invIdxs[fieldName].documentCount - docFreq + 0.5) / (docFreq + 0.5));\n\t}\n\n\t_avgFieldLength(fieldName) {\n\t\treturn this._getCache(fieldName).avgFieldLength;\n\t}\n}\n\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./src/inverted_index/scorer.js\n// module id = 7\n// module chunks = 0"],"sourceRoot":""}